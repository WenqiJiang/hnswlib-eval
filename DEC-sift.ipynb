{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from tensorboardX import SummaryWriter\n",
    "import uuid\n",
    "\n",
    "from ptdec.dec import DEC\n",
    "# from ptdec.model import train, predict\n",
    "from ptsdae.sdae import StackedDenoisingAutoEncoder\n",
    "import ptsdae.model as ae\n",
    "from ptdec.utils import cluster_accuracy\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f2015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmap_fvecs(fname):\n",
    "    x = np.memmap(fname, dtype='int32', mode='r')\n",
    "    d = x[0]\n",
    "    return x.view('float32').reshape(-1, d + 1)[:, 1:]\n",
    "\n",
    "def mmap_bvecs(fname):\n",
    "    x = np.memmap(fname, dtype='uint8', mode='r')\n",
    "    d = x[:4].view('int32')[0]\n",
    "    return x.reshape(-1, d + 4)[:, 4:]\n",
    "\n",
    "def ivecs_read(fname):\n",
    "    a = np.fromfile(fname, dtype='int32')\n",
    "    d = a[0]\n",
    "    # Wenqi: Format of ground truth (for 10000 query vectors):\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #        ...     ...\n",
    "    #   1000(topK), [1000 ids]\n",
    "    # 10000 rows in total, 10000 * 1001 elements, 10000 * 1001 * 4 bytes\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy()\n",
    "\n",
    "def fvecs_read(fname):\n",
    "    return ivecs_read(fname).view('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd0c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = 'SIFT1M'\n",
    "num_vec_learn = int(1e4)\n",
    "\n",
    "if dbname.startswith('SIFT'):\n",
    "    # SIFT1M to SIFT1000M\n",
    "    dbsize = int(dbname[4:-1])\n",
    "    xb = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_base.bvecs')\n",
    "    xq = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_query.bvecs')\n",
    "    gt = ivecs_read('/mnt/scratch/wenqi/Faiss_experiments/bigann/gnd/idx_%dM.ivecs' % dbsize)\n",
    "\n",
    "    N_VEC = int(dbsize * 1000 * 1000)\n",
    "\n",
    "    # trim xb to correct size\n",
    "    xb = xb[:dbsize * 1000 * 1000]\n",
    "\n",
    "    # Wenqi: load xq to main memory and reshape\n",
    "    xq = xq.astype('float32').copy()\n",
    "#     xq = np.array(xq, dtype=np.float32)\n",
    "    xb = xb.astype('float32').copy()\n",
    "    gt = np.array(gt, dtype=np.int32)\n",
    "\n",
    "    print(\"Vector shapes:\")\n",
    "    print(\"Base vector xb: \", xb.shape)\n",
    "    print(\"Query vector xq: \", xq.shape)\n",
    "    print(\"Ground truth gt: \", gt.shape)\n",
    "else:\n",
    "    print('unknown dataset', dbname, file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "dim = xb.shape[1] # should be 128\n",
    "nq = xq.shape[0]\n",
    "\n",
    "# Normalize all to 0~1\n",
    "xb = xb / 256\n",
    "xq = xq / 256\n",
    "xt = xb[:num_vec_learn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ac099",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()  # create the TensorBoard object\n",
    "# callback function to call during training, uses writer from the scope\n",
    "\n",
    "def training_callback(epoch, lr, loss, validation_loss):\n",
    "    writer.add_scalars(\n",
    "        \"data/autoencoder\",\n",
    "        {\"lr\": lr, \"loss\": loss, \"validation_loss\": validation_loss,},\n",
    "        epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda=False\n",
    "batch_size=256\n",
    "pretrain_epochs=100\n",
    "finetune_epochs=100\n",
    "testing_mode=False # whether to run in testing mode (default False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ddc788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN_arch = [128, 100, 100, 64, 32]\n",
    "DNN_arch = [128, 500, 500, 2000, 32]\n",
    "\n",
    "autoencoder = StackedDenoisingAutoEncoder(\n",
    "    DNN_arch, final_activation=None\n",
    ")\n",
    "if cuda:\n",
    "    autoencoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset\n",
    "# A blog about torch dataset: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "\n",
    "class SIFTDataset(Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    def __init__(self, vectors):\n",
    "        'Initialization'\n",
    "        # vectors: 2D vectors dim0: num_vec dim1: vec_dim\n",
    "        self.vectors = vectors \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.vectors.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        return self.vectors[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c686c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.Tensor((1000, 128))\n",
    "ds_train = SIFTDataset(\n",
    "    torch.Tensor(xt))\n",
    "ds_val = SIFTDataset(\n",
    "    torch.Tensor(xb[num_vec_learn:2*num_vec_learn]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec629790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE: takes a torch dataset as input\n",
    "#   SDAE model: https://github.com/vlukiyanov/pt-sdae/blob/master/ptsdae/model.py\n",
    "#   Torch dataset manual: https://pytorch.org/docs/stable/data.html\n",
    "print(\"Pretraining stage.\")\n",
    "ae.pretrain(\n",
    "    ds_train,\n",
    "    autoencoder,\n",
    "    cuda=cuda,\n",
    "    validation=ds_val,\n",
    "    epochs=pretrain_epochs,\n",
    "    batch_size=batch_size,\n",
    "    optimizer=lambda model: SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
    "    scheduler=lambda x: StepLR(x, 100, gamma=0.1),\n",
    "    corruption=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24eade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training stage.\")\n",
    "ae_optimizer = SGD(params=autoencoder.parameters(), lr=0.1, momentum=0.9)\n",
    "ae.train(\n",
    "    ds_train,\n",
    "    autoencoder,\n",
    "    cuda=cuda,\n",
    "    validation=ds_val,\n",
    "    epochs=finetune_epochs,\n",
    "    batch_size=batch_size,\n",
    "    optimizer=ae_optimizer,\n",
    "    scheduler=StepLR(ae_optimizer, 100, gamma=0.1),\n",
    "    corruption=0.2,\n",
    "    update_callback=training_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9945bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the SAE model\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def save_obj(obj, dirc, name):\n",
    "    # note use \"dir/\" in dirc\n",
    "    with open(os.path.join(dirc, name + '.pkl'), 'wb') as f:\n",
    "        pickle.dump(obj, f, protocol=4) # for py37,pickle.HIGHEST_PROTOCOL=4\n",
    "\n",
    "def load_obj(dirc, name):\n",
    "    with open(os.path.join(dirc, name + '.pkl'), 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = './models/'\n",
    "if not os.path.exists(fdir): os.mkdir(fdir)\n",
    "DNN_arch_name = ''\n",
    "for i in DNN_arch: DNN_arch_name += '{}_'.format(i)\n",
    "file_name = 'SAE_' + DNN_arch_name + 'epoch_{}_{}'.format(pretrain_epochs, finetune_epochs)\n",
    "print('file_name', file_name)\n",
    "\n",
    "save_obj(model, fdir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728cf321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader, default_collate\n",
    "from typing import Tuple, Callable, Optional, Union\n",
    "from tqdm import tqdm\n",
    "from ptdec.utils import target_distribution, cluster_accuracy\n",
    "\n",
    "def train(\n",
    "    dataset: torch.utils.data.Dataset,\n",
    "    model: torch.nn.Module,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    stopping_delta: Optional[float] = None,\n",
    "    collate_fn=default_collate,\n",
    "    cuda: bool = True,\n",
    "    sampler: Optional[torch.utils.data.sampler.Sampler] = None,\n",
    "    silent: bool = False,\n",
    "    update_freq: int = 10,\n",
    "    evaluate_batch_size: int = 1024,\n",
    "    update_callback: Optional[Callable[[float, float], None]] = None,\n",
    "    epoch_callback: Optional[Callable[[int, torch.nn.Module], None]] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train the DEC model given a dataset, a model instance and various configuration parameters.\n",
    "    :param dataset: instance of Dataset to use for training\n",
    "    :param model: instance of DEC model to train\n",
    "    :param epochs: number of training epochs\n",
    "    :param batch_size: size of the batch to train with\n",
    "    :param optimizer: instance of optimizer to use\n",
    "    :param stopping_delta: label delta as a proportion to use for stopping, None to disable, default None\n",
    "    :param collate_fn: function to merge a list of samples into mini-batch\n",
    "    :param cuda: whether to use CUDA, defaults to True\n",
    "    :param sampler: optional sampler to use in the DataLoader, defaults to None\n",
    "    :param silent: set to True to prevent printing out summary statistics, defaults to False\n",
    "    :param update_freq: frequency of batches with which to update counter, None disables, default 10\n",
    "    :param evaluate_batch_size: batch size for evaluation stage, default 1024\n",
    "    :param update_callback: optional function of accuracy and loss to update, default None\n",
    "    :param epoch_callback: optional function of epoch and model, default None\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    static_dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=False,\n",
    "        sampler=sampler,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        sampler=sampler,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    data_iterator = tqdm(\n",
    "        static_dataloader,\n",
    "        leave=True,\n",
    "        unit=\"batch\",\n",
    "        postfix={\n",
    "            \"epo\": -1,\n",
    "#             \"acc\": \"%.4f\" % 0.0,\n",
    "            \"lss\": \"%.8f\" % 0.0,\n",
    "            \"dlb\": \"%.4f\" % -1,\n",
    "        },\n",
    "        disable=silent,\n",
    "    )\n",
    "    kmeans = KMeans(n_clusters=model.cluster_number, n_init=20)\n",
    "    model.train()\n",
    "    features = []\n",
    "    actual = []\n",
    "    # form initial cluster centres\n",
    "    for index, batch in enumerate(data_iterator):\n",
    "        if (isinstance(batch, tuple) or isinstance(batch, list)) and len(batch) == 2:\n",
    "            batch, value = batch  # if we have a prediction label, separate it to actual\n",
    "            actual.append(value)\n",
    "        if cuda:\n",
    "            batch = batch.cuda(non_blocking=True)\n",
    "        features.append(model.encoder(batch).detach().cpu())\n",
    "    predicted = kmeans.fit_predict(torch.cat(features).numpy())\n",
    "    predicted_previous = torch.tensor(np.copy(predicted), dtype=torch.long)\n",
    "#     if actual: \n",
    "#         actual = torch.cat(actual).long()\n",
    "#         _, accuracy = cluster_accuracy(predicted, actual.cpu().numpy())\n",
    "    cluster_centers = torch.tensor(\n",
    "        kmeans.cluster_centers_, dtype=torch.float, requires_grad=True\n",
    "    )\n",
    "    if cuda:\n",
    "        cluster_centers = cluster_centers.cuda(non_blocking=True)\n",
    "    with torch.no_grad():\n",
    "        # initialise the cluster centers\n",
    "        model.state_dict()[\"assignment.cluster_centers\"].copy_(cluster_centers)\n",
    "    loss_function = nn.KLDivLoss(size_average=False)\n",
    "    delta_label = None\n",
    "    for epoch in range(epochs):\n",
    "        features = []\n",
    "        data_iterator = tqdm(\n",
    "            train_dataloader,\n",
    "            leave=True,\n",
    "            unit=\"batch\",\n",
    "            postfix={\n",
    "                \"epo\": epoch,\n",
    "#                 \"acc\": \"%.4f\" % (accuracy or 0.0),\n",
    "                \"lss\": \"%.8f\" % 0.0,\n",
    "                \"dlb\": \"%.4f\" % (delta_label or 0.0),\n",
    "            },\n",
    "            disable=silent,\n",
    "        )\n",
    "        model.train()\n",
    "        for index, batch in enumerate(data_iterator):\n",
    "            if (isinstance(batch, tuple) or isinstance(batch, list)) and len(\n",
    "                batch\n",
    "            ) == 2:\n",
    "                batch, _ = batch  # if we have a prediction label, strip it away\n",
    "            if cuda:\n",
    "                batch = batch.cuda(non_blocking=True)\n",
    "            output = model(batch)\n",
    "            target = target_distribution(output).detach()\n",
    "            loss = loss_function(output.log(), target) / output.shape[0]\n",
    "#             print('output.log()', output.log())\n",
    "#             print('target', target)\n",
    "#             print('loss_function(output.log(), target)', loss_function(output.log(), target))\n",
    "#             print('loss', loss)\n",
    "            data_iterator.set_postfix(\n",
    "                epo=epoch,\n",
    "#                 acc=\"%.4f\" % (accuracy or 0.0),\n",
    "                lss=\"%.8f\" % float(loss.item()),\n",
    "                dlb=\"%.4f\" % (delta_label or 0.0),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(closure=None)\n",
    "            features.append(model.encoder(batch).detach().cpu())\n",
    "            if update_freq is not None and index % update_freq == 0:\n",
    "                loss_value = float(loss.item())\n",
    "                data_iterator.set_postfix(\n",
    "                    epo=epoch,\n",
    "#                     acc=\"%.4f\" % (accuracy or 0.0),\n",
    "                    lss=\"%.8f\" % loss_value,\n",
    "                    dlb=\"%.4f\" % (delta_label or 0.0),\n",
    "                )\n",
    "#                 if update_callback is not None:\n",
    "#                     update_callback(accuracy, loss_value, delta_label)\n",
    "        predicted = predict(\n",
    "            dataset,\n",
    "            model,\n",
    "            batch_size=evaluate_batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            silent=True,\n",
    "            return_actual=False,\n",
    "            cuda=cuda,\n",
    "        )\n",
    "#         print('predicted', predicted)\n",
    "\n",
    "#         predicted, actual = predict(\n",
    "#             dataset,\n",
    "#             model,\n",
    "#             batch_size=evaluate_batch_size,\n",
    "#             collate_fn=collate_fn,\n",
    "#             silent=True,\n",
    "#             return_actual=True,\n",
    "#             cuda=cuda,\n",
    "#         )\n",
    "        delta_label = (\n",
    "            float((predicted != predicted_previous).float().sum().item())\n",
    "            / predicted_previous.shape[0]\n",
    "        )\n",
    "        if stopping_delta is not None and delta_label < stopping_delta:\n",
    "            print(\n",
    "                'Early stopping as label delta \"%1.5f\" less than \"%1.5f\".'\n",
    "                % (delta_label, stopping_delta)\n",
    "            )\n",
    "            break\n",
    "        predicted_previous = predicted\n",
    "#         _, accuracy = cluster_accuracy(predicted.cpu().numpy(), actual.cpu().numpy())\n",
    "        data_iterator.set_postfix(\n",
    "            epo=epoch,\n",
    "#             acc=\"%.4f\" % (accuracy or 0.0),\n",
    "            lss=\"%.8f\" % 0.0,\n",
    "            dlb=\"%.4f\" % (delta_label or 0.0),\n",
    "        )\n",
    "        if epoch_callback is not None:\n",
    "            epoch_callback(epoch, model)\n",
    "\n",
    "\n",
    "def predict(\n",
    "    dataset: torch.utils.data.Dataset,\n",
    "    model: torch.nn.Module,\n",
    "    batch_size: int = 1024,\n",
    "    collate_fn=default_collate,\n",
    "    cuda: bool = True,\n",
    "    silent: bool = False,\n",
    "    return_actual: bool = False,\n",
    ") -> Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Predict clusters for a dataset given a DEC model instance and various configuration parameters.\n",
    "    :param dataset: instance of Dataset to use for training\n",
    "    :param model: instance of DEC model to predict\n",
    "    :param batch_size: size of the batch to predict with, default 1024\n",
    "    :param collate_fn: function to merge a list of samples into mini-batch\n",
    "    :param cuda: whether CUDA is used, defaults to True\n",
    "    :param silent: set to True to prevent printing out summary statistics, defaults to False\n",
    "    :param return_actual: return actual values, if present in the Dataset\n",
    "    :return: tuple of prediction and actual if return_actual is True otherwise prediction\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False\n",
    "    )\n",
    "    data_iterator = tqdm(dataloader, leave=True, unit=\"batch\", disable=silent,)\n",
    "    features = []\n",
    "    actual = []\n",
    "    model.eval()\n",
    "    for batch in data_iterator:\n",
    "        if (isinstance(batch, tuple) or isinstance(batch, list)) and len(batch) == 2:\n",
    "            batch, value = batch  # unpack if we have a prediction label\n",
    "            if return_actual:\n",
    "                actual.append(value)\n",
    "        elif return_actual:\n",
    "            raise ValueError(\n",
    "                \"Dataset has no actual value to unpack, but return_actual is set.\"\n",
    "            )\n",
    "        if cuda:\n",
    "            batch = batch.cuda(non_blocking=True)\n",
    "        features.append(\n",
    "            model(batch).detach().cpu()\n",
    "        )  # move to the CPU to prevent out of memory on the GPU\n",
    "    if return_actual:\n",
    "        return torch.cat(features).max(1)[1], torch.cat(actual).long()\n",
    "    else:\n",
    "        return torch.cat(features).max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4df00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Note: should pretrain & train the models for enough epochs (e.g., 100), \" \\\n",
    "      \"otherwise this model can predict all vectors to a same centroid which lead to zero gradients\")\n",
    "\n",
    "print(\"DEC stage.\")\n",
    "model = DEC(cluster_number=32, hidden_dimension=32, encoder=autoencoder.encoder)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "    \n",
    "# Learning rate: somehow the MNIST dataset has values from 0~5.x; we only have 0~1\n",
    "# MNIST learning rate 0.01\n",
    "# Our, set to 0.001 and try? \n",
    "# dec_optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "dec_optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8427304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f33f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    dataset=ds_train,\n",
    "    model=model,\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    optimizer=dec_optimizer,\n",
    "    stopping_delta=0.000001,\n",
    "    cuda=cuda,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de75b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all = ds_train = SIFTDataset(\n",
    "    torch.Tensor(xb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe unbalance factor on the training set\n",
    "\n",
    "partition_IDs_train = predict(\n",
    "    ds_train, model, batch_size=1024, silent=True, return_actual=False, cuda=cuda\n",
    ").cpu().numpy() \n",
    "\n",
    "# Create a mapping: partition ID -> {list of vector IDs}\n",
    "\n",
    "num_partition = 32\n",
    "\n",
    "partition_id_vec_id_list_train = dict()\n",
    "for i in range(num_partition):\n",
    "    partition_id_vec_id_list_train[i] = []\n",
    "\n",
    "\n",
    "for i in range(num_vec_learn):\n",
    "    partition_ID = int(partition_IDs[i])\n",
    "    partition_id_vec_id_list_train[partition_ID].append(i)\n",
    "    \n",
    "for i in range(num_partition):\n",
    "    print('items in partition ', i, len(partition_id_vec_id_list_train[i]), 'average =', int(num_vec_learn/num_partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = predict(\n",
    "    ds_all, model, batch_size=1024, silent=True, return_actual=False, cuda=cuda\n",
    ")\n",
    "partition_IDs = predicted.cpu().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea26c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping: partition ID -> {list of vector IDs}\n",
    "\n",
    "num_partition = 32\n",
    "\n",
    "partition_id_vec_id_list_1M = dict()\n",
    "for i in range(num_partition):\n",
    "    partition_id_vec_id_list_1M[i] = []\n",
    "\n",
    "\n",
    "for i in range(int(1e6)):\n",
    "    partition_ID = int(partition_IDs[i])\n",
    "    partition_id_vec_id_list_1M[partition_ID].append(i)\n",
    "    \n",
    "for i in range(num_partition):\n",
    "    print('items in partition ', i, len(partition_id_vec_id_list_1M[i]), 'average =', int(1e6/num_partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56277f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def scan_partition(query_vec, partition_id_list, vector_set):\n",
    "    \"\"\"\n",
    "    query_vec = (128, )\n",
    "    partition_id_list = (N_num_vec, )\n",
    "    vector_set = 1M dataset (1M, 128)\n",
    "    \"\"\"\n",
    "    min_dist = 1e10\n",
    "    min_dist_ID = None\n",
    "    for vec_id in partition_id_list:\n",
    "        dataset_vec = vector_set[vec_id]\n",
    "        dist = np.linalg.norm(query_vec - dataset_vec)\n",
    "        if dist <= min_dist:\n",
    "            min_dist = dist\n",
    "            min_dist_ID = vec_id\n",
    "            \n",
    "    return min_dist_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e52056",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors = []\n",
    "\n",
    "# N = 1000\n",
    "N = 100\n",
    "#### Wenqi: here had a bug: previously xb, now xq\n",
    "ds_xq = SIFTDataset(torch.Tensor(xq))\n",
    "\n",
    "query_partition = predict(\n",
    "    ds_xq, model, batch_size=1024, silent=True, return_actual=False, cuda=cuda\n",
    ").cpu().numpy()\n",
    "\n",
    "for i in range(N):\n",
    "    partition_id = int(query_partition[i])\n",
    "    nearest_neighbor_ID = scan_partition(xq[i], partition_id_vec_id_list_1M[partition_id], xb)\n",
    "    nearest_neighbors.append(nearest_neighbor_ID)\n",
    "    print(i, nearest_neighbor_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837579b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = 0\n",
    "for i in range(N):\n",
    "    if nearest_neighbors[i] == gt[i][0]:\n",
    "        correct_count += 1\n",
    "        \n",
    "print(correct_count, 'recall@1 = ', correct_count / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b254d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
