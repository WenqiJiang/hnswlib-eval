{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598ff25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 15, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25bcbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmap_fvecs(fname):\n",
    "    x = np.memmap(fname, dtype='int32', mode='r')\n",
    "    d = x[0]\n",
    "    return x.view('float32').reshape(-1, d + 1)[:, 1:]\n",
    "\n",
    "def mmap_bvecs(fname):\n",
    "    x = np.memmap(fname, dtype='uint8', mode='r')\n",
    "    d = x[:4].view('int32')[0]\n",
    "    return x.reshape(-1, d + 4)[:, 4:]\n",
    "\n",
    "def ivecs_read(fname):\n",
    "    a = np.fromfile(fname, dtype='int32')\n",
    "    d = a[0]\n",
    "    # Wenqi: Format of ground truth (for 10000 query vectors):\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #        ...     ...\n",
    "    #   1000(topK), [1000 ids]\n",
    "    # 10000 rows in total, 10000 * 1001 elements, 10000 * 1001 * 4 bytes\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy()\n",
    "\n",
    "def fvecs_read(fname):\n",
    "    return ivecs_read(fname).view('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8cf3987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shapes:\n",
      "Base vector xb:  (1000000, 128)\n",
      "Query vector xq:  (10000, 128)\n",
      "Ground truth gt:  (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "dbname = 'SIFT1M'\n",
    "num_vec_train = int(1e4)\n",
    "num_query_learn = int(1e0)\n",
    "\n",
    "if dbname.startswith('SIFT'):\n",
    "    # SIFT1M to SIFT1000M\n",
    "    dbsize = int(dbname[4:-1])\n",
    "    xb = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_base.bvecs')\n",
    "    xq = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_query.bvecs')\n",
    "    gt = ivecs_read('/mnt/scratch/wenqi/Faiss_experiments/bigann/gnd/idx_%dM.ivecs' % dbsize)\n",
    "\n",
    "    N_VEC = int(dbsize * 1000 * 1000)\n",
    "\n",
    "    # trim xb to correct size\n",
    "    xb = xb[:dbsize * 1000 * 1000]\n",
    "\n",
    "    # Wenqi: load xq to main memory and reshape\n",
    "    xq = xq.astype('float32').copy()\n",
    "#     xq = np.array(xq, dtype=np.float32)\n",
    "    xb = xb.astype('float32').copy()\n",
    "    gt = np.array(gt, dtype=np.int32)\n",
    "\n",
    "    print(\"Vector shapes:\")\n",
    "    print(\"Base vector xb: \", xb.shape)\n",
    "    print(\"Query vector xq: \", xq.shape)\n",
    "    print(\"Ground truth gt: \", gt.shape)\n",
    "else:\n",
    "    print('unknown dataset', dbname, file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "dim = xb.shape[1] # should be 128\n",
    "nq = xq.shape[0]\n",
    "\n",
    "# Normalize all to 0~1\n",
    "xb = xb / 256\n",
    "xq = xq / 256\n",
    "xq_learn = xq[:num_query_learn]\n",
    "xt = xb[:num_vec_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d43bf9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DeepAutoencoder class\n",
    "\n",
    "# We partition the data to 32 shards (bottleneck dimension)\n",
    "# The input data is 128-dimension\n",
    "class DeepAutoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "#             torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Instantiating the model and hyperparameters\n",
    "model = DeepAutoencoder()\n",
    "# loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72693ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(reconstructed_feat, input_feat, query, hidden_feat, query_hidden_feat):\n",
    "    \"\"\"\n",
    "    centroid vectors: same shape as hidden_feat\n",
    "    \"\"\"\n",
    "    \n",
    "    num_vec = reconstructed_feat.shape[0]\n",
    "    full_dim = reconstructed_feat.shape[1]\n",
    "    hidden_dim = hidden_feat.shape[1]\n",
    "    query_num = query.shape[0]\n",
    "    \n",
    "    hidden_feat_factor = full_dim / hidden_dim\n",
    "    \n",
    "    loss = 0\n",
    "    # For each query, each db vector has a weight\n",
    "    for i in range(query_num):\n",
    "        # the closer the distance, the larger the weights\n",
    "#         weights = torch.ones(num_vec)\n",
    "        weights = 1 / torch.sum((input_feat - query[i].repeat(num_vec, 1)) ** 2, dim=1)\n",
    "        print('weights: ', weights, 'min: ', torch.min(weights), 'max: ', torch.max(weights))\n",
    "        dist = torch.sum((reconstructed_feat - input_feat) ** 2, dim=1)\n",
    "        part_loss = torch.mean(weights * dist)\n",
    "        \n",
    "#         dist_hidden = torch.sum((hidden_feat - query_hidden_feat[i].repeat(num_vec, 1)) ** 2, dim=1)\n",
    "#         part_loss_hidden = torch.mean(weights * dist_hidden) * hidden_feat_factor\n",
    "        \n",
    "#         print(\"L2 dist loss: \", part_loss, \"Hidden dist loss: \", part_loss_hidden)\n",
    "        \n",
    "#         loss += part_loss + part_loss_hidden\n",
    "        \n",
    "        loss += part_loss \n",
    "    \n",
    "    loss /= query_num \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea70f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sum((out[0] * xq_learn_tensor[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a505bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sum(out[0].repeat(5000, 1) * xq_learn_tensor, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "272159d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000 # Wenqi: original iteration = 100\n",
    "num_partitions = 32\n",
    "\n",
    "# List that will store the training loss\n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Dictionary that will store the\n",
    "# different images and outputs for\n",
    "# various epochs\n",
    "outputs = {}\n",
    "\n",
    "xt_tensor = torch.FloatTensor(xt)\n",
    "xq_learn_tensor = torch.FloatTensor(xq_learn)\n",
    "hidden_feature = model.encoder(xt_tensor)\n",
    "out = model(xt_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7217f4c3",
   "metadata": {},
   "source": [
    "Loss (Overfit the first 10 queries)\n",
    "\n",
    "First 1000 iter: 43 -> 1.6 (Recall first 10=0.7)\n",
    "\n",
    "Second 1000 iter: 1.6 -> 1.25\n",
    "\n",
    "Third 1000 iter: 1.25 -> 1.11\n",
    "\n",
    "Fourth 1000 iter: 1.11 -> 1.05 (Recall first 10=0.7)\n",
    "\n",
    "Maybe the weights should be L2 distance rather than dot product?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a23b3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out torch.Size([10000, 128]) tensor([[0.5246, 0.5142, 0.4820,  ..., 0.5216, 0.5200, 0.4907],\n",
      "        [0.5244, 0.5137, 0.4818,  ..., 0.5209, 0.5191, 0.4908],\n",
      "        [0.5247, 0.5141, 0.4822,  ..., 0.5215, 0.5196, 0.4908],\n",
      "        ...,\n",
      "        [0.5241, 0.5135, 0.4818,  ..., 0.5211, 0.5188, 0.4911],\n",
      "        [0.5249, 0.5137, 0.4816,  ..., 0.5212, 0.5195, 0.4907],\n",
      "        [0.5244, 0.5141, 0.4820,  ..., 0.5213, 0.5196, 0.4909]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "xt_tensor torch.Size([10000, 128]) tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0547, 0.0391, 0.0234],\n",
      "        [0.2539, 0.1367, 0.0312,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0273, 0.0195, 0.0273,  ..., 0.1758, 0.0625, 0.0898],\n",
      "        [0.1484, 0.0000, 0.0000,  ..., 0.1055, 0.0156, 0.0312],\n",
      "        [0.0000, 0.0000, 0.0078,  ..., 0.1172, 0.0117, 0.0000]])\n",
      "hidden_feature torch.Size([10000, 32]) tensor([[ 0.2066, -0.0411,  0.0467,  ...,  0.1053, -0.0183,  0.1370],\n",
      "        [ 0.1892, -0.0064,  0.0792,  ...,  0.1262, -0.0402,  0.1418],\n",
      "        [ 0.1978, -0.0367,  0.0536,  ...,  0.1156, -0.0142,  0.1455],\n",
      "        ...,\n",
      "        [ 0.2025, -0.0069,  0.0588,  ...,  0.1173, -0.0344,  0.1537],\n",
      "        [ 0.1861, -0.0151,  0.0477,  ...,  0.0971, -0.0417,  0.1227],\n",
      "        [ 0.2016, -0.0125,  0.0575,  ...,  0.1116, -0.0285,  0.1431]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 0 train loss: 6.340961456298828\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 1 train loss: 6.311497211456299\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 2 train loss: 6.281898021697998\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 3 train loss: 6.252299785614014\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 4 train loss: 6.2228264808654785\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 5 train loss: 6.193068981170654\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 6 train loss: 6.162665367126465\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 7 train loss: 6.131711006164551\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 8 train loss: 6.10030460357666\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 9 train loss: 6.067845344543457\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 10 train loss: 6.033960819244385\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 11 train loss: 5.9985575675964355\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 12 train loss: 5.961208820343018\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 13 train loss: 5.921653747558594\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 14 train loss: 5.8797712326049805\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 15 train loss: 5.835481643676758\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 16 train loss: 5.788283348083496\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 17 train loss: 5.73744535446167\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 18 train loss: 5.682262897491455\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 19 train loss: 5.622008323669434\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 20 train loss: 5.555871963500977\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 21 train loss: 5.482865810394287\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 22 train loss: 5.4018473625183105\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 23 train loss: 5.311531066894531\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 24 train loss: 5.210210800170898\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 25 train loss: 5.095855236053467\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 26 train loss: 4.966586112976074\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 27 train loss: 4.820739269256592\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 28 train loss: 4.656738758087158\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 29 train loss: 4.4725236892700195\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 30 train loss: 4.266611099243164\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 31 train loss: 4.038072109222412\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 32 train loss: 3.786625385284424\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 33 train loss: 3.513190269470215\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 34 train loss: 3.2203209400177\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 35 train loss: 2.9123826026916504\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 36 train loss: 2.595607280731201\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 37 train loss: 2.278350591659546\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 38 train loss: 1.970655918121338\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 39 train loss: 1.6835534572601318\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 40 train loss: 1.4279836416244507\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 41 train loss: 1.2130852937698364\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 42 train loss: 1.0455336570739746\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 43 train loss: 0.927649974822998\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 44 train loss: 0.8562656044960022\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 45 train loss: 0.8226226568222046\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 46 train loss: 0.8136168122291565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 47 train loss: 0.8160706758499146\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 48 train loss: 0.8209813237190247\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 49 train loss: 0.823842465877533\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 50 train loss: 0.8230041861534119\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 51 train loss: 0.8182008266448975\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 52 train loss: 0.8097785115242004\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 53 train loss: 0.7983524799346924\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 54 train loss: 0.784609317779541\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 55 train loss: 0.7691603899002075\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 56 train loss: 0.7524999976158142\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 57 train loss: 0.7350570559501648\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 58 train loss: 0.7173279523849487\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 59 train loss: 0.699982225894928\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 60 train loss: 0.6838704347610474\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 61 train loss: 0.6699330806732178\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 62 train loss: 0.6590756773948669\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 63 train loss: 0.6518680453300476\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 64 train loss: 0.648453950881958\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 65 train loss: 0.6483147144317627\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 66 train loss: 0.6502416729927063\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 67 train loss: 0.6525747179985046\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 68 train loss: 0.6536710262298584\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 69 train loss: 0.6524145007133484\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 70 train loss: 0.6485232710838318\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 71 train loss: 0.6425319314002991\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 72 train loss: 0.6355109810829163\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 73 train loss: 0.6286693811416626\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 74 train loss: 0.6229977011680603\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 75 train loss: 0.6190516948699951\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 76 train loss: 0.6169047355651855\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 77 train loss: 0.6162405610084534\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 78 train loss: 0.6165181994438171\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 79 train loss: 0.6171473264694214\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 80 train loss: 0.6176223158836365\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 81 train loss: 0.6176032423973083\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 82 train loss: 0.6169422268867493\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 83 train loss: 0.6156700849533081\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 84 train loss: 0.6139543056488037\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 85 train loss: 0.6120433807373047\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 86 train loss: 0.6102018356323242\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 87 train loss: 0.608648955821991\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 88 train loss: 0.6075124740600586\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 89 train loss: 0.6068024039268494\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 90 train loss: 0.6064203977584839\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 91 train loss: 0.6061964631080627\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 92 train loss: 0.6059466600418091\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 93 train loss: 0.6055293083190918\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 94 train loss: 0.6048829555511475\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 95 train loss: 0.6040334105491638\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 96 train loss: 0.6030722260475159\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 97 train loss: 0.602118730545044\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 98 train loss: 0.6012781858444214\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 99 train loss: 0.6006140112876892\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 100 train loss: 0.6001378297805786\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 101 train loss: 0.5998165607452393\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 102 train loss: 0.5995901823043823\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 103 train loss: 0.5993937253952026\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 104 train loss: 0.599175751209259\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 105 train loss: 0.5989087820053101\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 106 train loss: 0.5985901951789856\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 107 train loss: 0.5982354879379272\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 108 train loss: 0.597871720790863\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 109 train loss: 0.597527027130127\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 110 train loss: 0.5972217917442322\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 111 train loss: 0.5969624519348145\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 112 train loss: 0.5967408418655396\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 113 train loss: 0.5965391397476196\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 114 train loss: 0.596337080001831\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 115 train loss: 0.5961188673973083\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 116 train loss: 0.5958784818649292\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 117 train loss: 0.595620334148407\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 118 train loss: 0.5953568816184998\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 119 train loss: 0.5951026082038879\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 120 train loss: 0.5948692560195923\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 121 train loss: 0.5946609973907471\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 122 train loss: 0.5944756269454956\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 123 train loss: 0.5943054556846619\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 124 train loss: 0.5941418409347534\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 125 train loss: 0.5939784049987793\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 126 train loss: 0.5938126444816589\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 127 train loss: 0.5936456918716431\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 128 train loss: 0.5934811234474182\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 129 train loss: 0.5933220982551575\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 130 train loss: 0.5931712985038757\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 131 train loss: 0.5930284857749939\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 132 train loss: 0.5928913950920105\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 133 train loss: 0.5927578806877136\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 134 train loss: 0.5926259756088257\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 135 train loss: 0.5924950838088989\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 136 train loss: 0.5923664569854736\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 137 train loss: 0.5922411680221558\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 138 train loss: 0.5921211838722229\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 139 train loss: 0.5920072197914124\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 140 train loss: 0.5918993353843689\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 141 train loss: 0.5917964577674866\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 142 train loss: 0.591697096824646\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 143 train loss: 0.5916005969047546\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 144 train loss: 0.5915064215660095\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 145 train loss: 0.5914148688316345\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 146 train loss: 0.5913257598876953\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 147 train loss: 0.591239333152771\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 148 train loss: 0.5911545157432556\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 149 train loss: 0.5910710692405701\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 150 train loss: 0.5909885764122009\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 151 train loss: 0.5909064412117004\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 152 train loss: 0.5908246636390686\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 153 train loss: 0.5907435417175293\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 154 train loss: 0.5906631946563721\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 155 train loss: 0.5905832648277283\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 156 train loss: 0.590503990650177\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 157 train loss: 0.5904254913330078\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 158 train loss: 0.5903467535972595\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 159 train loss: 0.5902681350708008\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 160 train loss: 0.5901896953582764\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 161 train loss: 0.5901117324829102\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 162 train loss: 0.5900344252586365\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 163 train loss: 0.5899580121040344\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 164 train loss: 0.5898822546005249\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 165 train loss: 0.5898070335388184\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 166 train loss: 0.5897324681282043\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 167 train loss: 0.5896585583686829\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 168 train loss: 0.5895854830741882\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 169 train loss: 0.5895134210586548\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 170 train loss: 0.5894421339035034\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 171 train loss: 0.5893718004226685\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 172 train loss: 0.5893023014068604\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 173 train loss: 0.5892336964607239\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 174 train loss: 0.5891660451889038\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 175 train loss: 0.5890994071960449\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 176 train loss: 0.5890339016914368\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 177 train loss: 0.5889694094657898\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 178 train loss: 0.5889061093330383\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 179 train loss: 0.588843584060669\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 180 train loss: 0.5887821912765503\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 181 train loss: 0.5887218117713928\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 182 train loss: 0.5886627435684204\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 183 train loss: 0.5886046886444092\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 184 train loss: 0.5885476469993591\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 185 train loss: 0.5884920954704285\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 186 train loss: 0.5884373784065247\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 187 train loss: 0.5883838534355164\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 188 train loss: 0.5883312225341797\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 189 train loss: 0.5882794260978699\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 190 train loss: 0.588228702545166\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 191 train loss: 0.5881786346435547\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 192 train loss: 0.5881293416023254\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 193 train loss: 0.588080644607544\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 194 train loss: 0.5880324244499207\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 195 train loss: 0.5879848599433899\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 196 train loss: 0.5879372358322144\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 197 train loss: 0.5878897309303284\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 198 train loss: 0.5878421664237976\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 199 train loss: 0.5877948999404907\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 200 train loss: 0.5877485871315002\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 201 train loss: 0.5877038240432739\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 202 train loss: 0.5876604318618774\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 203 train loss: 0.5876176953315735\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 204 train loss: 0.5875749588012695\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 205 train loss: 0.5875323414802551\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 206 train loss: 0.587489902973175\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 207 train loss: 0.5874474048614502\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 208 train loss: 0.5874049663543701\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 209 train loss: 0.58736252784729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 210 train loss: 0.5873203873634338\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 211 train loss: 0.5872785449028015\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 212 train loss: 0.5872365236282349\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 213 train loss: 0.5871950387954712\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 214 train loss: 0.5871537327766418\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 215 train loss: 0.5871129631996155\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 216 train loss: 0.5870723724365234\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 217 train loss: 0.587031900882721\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 218 train loss: 0.5869911313056946\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 219 train loss: 0.5869501233100891\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 220 train loss: 0.5869090557098389\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 221 train loss: 0.5868682265281677\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 222 train loss: 0.5868276953697205\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 223 train loss: 0.5867873430252075\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 224 train loss: 0.5867472887039185\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 225 train loss: 0.5867074131965637\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 226 train loss: 0.5866674780845642\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 227 train loss: 0.5866275429725647\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 228 train loss: 0.5865877270698547\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 229 train loss: 0.5865479111671448\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 230 train loss: 0.5865082740783691\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 231 train loss: 0.5864689946174622\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 232 train loss: 0.5864297151565552\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 233 train loss: 0.5863904356956482\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 234 train loss: 0.586351215839386\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 235 train loss: 0.5863119959831238\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 236 train loss: 0.5862725377082825\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 237 train loss: 0.5862331986427307\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 238 train loss: 0.5861939787864685\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 239 train loss: 0.5861546993255615\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 240 train loss: 0.5861155390739441\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 241 train loss: 0.5860763192176819\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 242 train loss: 0.5860373377799988\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 243 train loss: 0.585997998714447\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 244 train loss: 0.58595871925354\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 245 train loss: 0.5859194993972778\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 246 train loss: 0.5858802795410156\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 247 train loss: 0.5858407616615295\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 248 train loss: 0.5858015418052673\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 249 train loss: 0.585762083530426\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 250 train loss: 0.5857226848602295\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 251 train loss: 0.5856832265853882\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 252 train loss: 0.585643470287323\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 253 train loss: 0.5856035947799683\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 254 train loss: 0.5855638384819031\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 255 train loss: 0.5855237245559692\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 256 train loss: 0.5854836702346802\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 257 train loss: 0.5854433178901672\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 258 train loss: 0.5854030847549438\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 259 train loss: 0.5853624939918518\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 260 train loss: 0.585321843624115\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 261 train loss: 0.5852810740470886\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 262 train loss: 0.5852399468421936\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 263 train loss: 0.5851987600326538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 264 train loss: 0.5851574540138245\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 265 train loss: 0.5851159691810608\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 266 train loss: 0.5850743651390076\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 267 train loss: 0.5850324034690857\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 268 train loss: 0.584990382194519\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 269 train loss: 0.5849480032920837\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 270 train loss: 0.5849056243896484\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 271 train loss: 0.5848627686500549\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 272 train loss: 0.5848199725151062\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 273 train loss: 0.5847765207290649\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 274 train loss: 0.5847330093383789\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 275 train loss: 0.5846889615058899\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 276 train loss: 0.5846447944641113\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 277 train loss: 0.5846002101898193\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 278 train loss: 0.5845551490783691\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 279 train loss: 0.5845100283622742\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 280 train loss: 0.5844643115997314\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 281 train loss: 0.5844182968139648\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 282 train loss: 0.58437180519104\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 283 train loss: 0.5843247771263123\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 284 train loss: 0.5842770338058472\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 285 train loss: 0.584229052066803\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 286 train loss: 0.5841804146766663\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 287 train loss: 0.5841314196586609\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 288 train loss: 0.5840816497802734\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 289 train loss: 0.5840314030647278\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 290 train loss: 0.5839803814888\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 291 train loss: 0.583928644657135\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 292 train loss: 0.5838762521743774\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 293 train loss: 0.5838231444358826\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 294 train loss: 0.5837692618370056\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 295 train loss: 0.5837143659591675\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 296 train loss: 0.5836586952209473\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 297 train loss: 0.5836018919944763\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 298 train loss: 0.5835440158843994\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 299 train loss: 0.5834848284721375\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 300 train loss: 0.5834245681762695\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 301 train loss: 0.5833629965782166\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 302 train loss: 0.5833001732826233\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 303 train loss: 0.5832357406616211\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 304 train loss: 0.5831698775291443\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 305 train loss: 0.5831024646759033\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 306 train loss: 0.5830334424972534\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 307 train loss: 0.5829626321792603\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 308 train loss: 0.5828900933265686\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 309 train loss: 0.5828155875205994\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 310 train loss: 0.582738995552063\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 311 train loss: 0.5826603770256042\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 312 train loss: 0.5825794339179993\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 313 train loss: 0.5824961066246033\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 314 train loss: 0.5824101567268372\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 315 train loss: 0.5823213458061218\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 316 train loss: 0.5822299122810364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 317 train loss: 0.5821353793144226\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 318 train loss: 0.5820373296737671\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 319 train loss: 0.5819360613822937\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 320 train loss: 0.5818314552307129\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 321 train loss: 0.5817227959632874\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 322 train loss: 0.5816102623939514\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 323 train loss: 0.5814933180809021\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 324 train loss: 0.5813722610473633\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 325 train loss: 0.5812462568283081\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 326 train loss: 0.5811153054237366\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 327 train loss: 0.5809788703918457\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 328 train loss: 0.5808368921279907\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 329 train loss: 0.5806888341903687\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 330 train loss: 0.5805345177650452\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 331 train loss: 0.5803734064102173\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 332 train loss: 0.5802043676376343\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 333 train loss: 0.5800277590751648\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 334 train loss: 0.579842209815979\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 335 train loss: 0.5796473622322083\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 336 train loss: 0.5794414281845093\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 337 train loss: 0.5792219638824463\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 338 train loss: 0.5789840221405029\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 339 train loss: 0.5787158608436584\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 340 train loss: 0.5784051418304443\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 341 train loss: 0.578057050704956\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 342 train loss: 0.5777166485786438\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 343 train loss: 0.5773875713348389\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 344 train loss: 0.5770285725593567\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 345 train loss: 0.5766317844390869\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 346 train loss: 0.5761933922767639\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 347 train loss: 0.5757131576538086\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 348 train loss: 0.575184166431427\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 349 train loss: 0.5745905637741089\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 350 train loss: 0.5739234685897827\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 351 train loss: 0.5732214450836182\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 352 train loss: 0.5724829435348511\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 353 train loss: 0.5716655254364014\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 354 train loss: 0.5707823038101196\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 355 train loss: 0.569818913936615\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 356 train loss: 0.5687787532806396\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 357 train loss: 0.5676701068878174\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 358 train loss: 0.5664749145507812\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 359 train loss: 0.5651896595954895\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 360 train loss: 0.5638149976730347\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 361 train loss: 0.5623518824577332\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 362 train loss: 0.5608124136924744\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 363 train loss: 0.559218168258667\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 364 train loss: 0.5575881600379944\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 365 train loss: 0.5559564828872681\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 366 train loss: 0.5543676018714905\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 367 train loss: 0.5528510808944702\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 368 train loss: 0.5514093041419983\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 369 train loss: 0.550052285194397\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 370 train loss: 0.5488224625587463\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 371 train loss: 0.5477413535118103\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 372 train loss: 0.5467997789382935\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 373 train loss: 0.5459840893745422\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 374 train loss: 0.5452753305435181\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 375 train loss: 0.5446243286132812\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 376 train loss: 0.5439863801002502\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 377 train loss: 0.5433404445648193\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 378 train loss: 0.5426811575889587\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 379 train loss: 0.5420138835906982\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 380 train loss: 0.541343092918396\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 381 train loss: 0.5406779050827026\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 382 train loss: 0.5400279760360718\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 383 train loss: 0.539398193359375\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 384 train loss: 0.5387915968894958\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 385 train loss: 0.5382173657417297\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 386 train loss: 0.5376764535903931\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 387 train loss: 0.5371587872505188\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 388 train loss: 0.5366610288619995\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 389 train loss: 0.536173403263092\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 390 train loss: 0.5356922745704651\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 391 train loss: 0.5352112054824829\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 392 train loss: 0.5347318649291992\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 393 train loss: 0.5342482924461365\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 394 train loss: 0.5337643027305603\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 395 train loss: 0.5332874655723572\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 396 train loss: 0.5328071713447571\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 397 train loss: 0.5323260426521301\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 398 train loss: 0.531838595867157\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 399 train loss: 0.5313430428504944\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 400 train loss: 0.5308365225791931\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 401 train loss: 0.5303150415420532\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 402 train loss: 0.5297777652740479\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 403 train loss: 0.529223620891571\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 404 train loss: 0.528654932975769\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 405 train loss: 0.5280712842941284\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 406 train loss: 0.5274773836135864\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 407 train loss: 0.5268740057945251\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 408 train loss: 0.5262641310691833\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 409 train loss: 0.5256491303443909\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 410 train loss: 0.5250294208526611\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 411 train loss: 0.5244085788726807\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 412 train loss: 0.5237882137298584\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 413 train loss: 0.5231680870056152\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 414 train loss: 0.5225391983985901\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 415 train loss: 0.5219069123268127\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 416 train loss: 0.5212984681129456\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 417 train loss: 0.5207077860832214\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 418 train loss: 0.5201148390769958\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 419 train loss: 0.5195229053497314\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 420 train loss: 0.5189415216445923\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 421 train loss: 0.5183728933334351\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 422 train loss: 0.5178105235099792\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 423 train loss: 0.5172529220581055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 424 train loss: 0.5166975855827332\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 425 train loss: 0.5161449313163757\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 426 train loss: 0.5155931711196899\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 427 train loss: 0.5150409936904907\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 428 train loss: 0.5144862532615662\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 429 train loss: 0.513922929763794\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 430 train loss: 0.513346791267395\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 431 train loss: 0.5127566456794739\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 432 train loss: 0.5121480226516724\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 433 train loss: 0.5115123987197876\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 434 train loss: 0.5108423233032227\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 435 train loss: 0.5101290941238403\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 436 train loss: 0.5093641877174377\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 437 train loss: 0.5085417032241821\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 438 train loss: 0.507655143737793\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 439 train loss: 0.5066986680030823\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 440 train loss: 0.5056633353233337\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 441 train loss: 0.5045442581176758\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 442 train loss: 0.5033308863639832\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 443 train loss: 0.5020148754119873\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 444 train loss: 0.500586748123169\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 445 train loss: 0.4990399479866028\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 446 train loss: 0.49736493825912476\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 447 train loss: 0.4955592751502991\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 448 train loss: 0.49362650513648987\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 449 train loss: 0.4915809631347656\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 450 train loss: 0.48944908380508423\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 451 train loss: 0.4872683584690094\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 452 train loss: 0.4850994646549225\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 453 train loss: 0.4830220639705658\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 454 train loss: 0.4811042547225952\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 455 train loss: 0.4794001877307892\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 456 train loss: 0.47795772552490234\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 457 train loss: 0.4767948389053345\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 458 train loss: 0.47589385509490967\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 459 train loss: 0.475202351808548\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 460 train loss: 0.47465670108795166\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 461 train loss: 0.47419503331184387\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 462 train loss: 0.4737693965435028\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 463 train loss: 0.47333720326423645\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 464 train loss: 0.4728774428367615\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 465 train loss: 0.4723798334598541\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 466 train loss: 0.47185346484184265\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 467 train loss: 0.47131112217903137\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 468 train loss: 0.4707678258419037\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 469 train loss: 0.47024574875831604\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 470 train loss: 0.4697472155094147\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 471 train loss: 0.4692838788032532\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 472 train loss: 0.46885907649993896\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 473 train loss: 0.4684727191925049\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 474 train loss: 0.468122273683548\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 475 train loss: 0.4678054749965668\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 476 train loss: 0.4675142467021942\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 477 train loss: 0.4672454595565796\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 478 train loss: 0.46697232127189636\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 479 train loss: 0.4666725993156433\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 480 train loss: 0.46635738015174866\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 481 train loss: 0.46606680750846863\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 482 train loss: 0.46579161286354065\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 483 train loss: 0.4654981791973114\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 484 train loss: 0.4651910066604614\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 485 train loss: 0.4649060070514679\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 486 train loss: 0.4646415412425995\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 487 train loss: 0.4643703103065491\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 488 train loss: 0.4640961289405823\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 489 train loss: 0.46383750438690186\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 490 train loss: 0.4635874032974243\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 491 train loss: 0.46332651376724243\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 492 train loss: 0.4630601704120636\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 493 train loss: 0.4628022313117981\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 494 train loss: 0.4625450670719147\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 495 train loss: 0.4622792601585388\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 496 train loss: 0.46201175451278687\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 497 train loss: 0.461750328540802\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 498 train loss: 0.4614890515804291\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 499 train loss: 0.4612243175506592\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 500 train loss: 0.4609609842300415\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 501 train loss: 0.4607044458389282\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 502 train loss: 0.46044906973838806\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 503 train loss: 0.4601924419403076\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 504 train loss: 0.4599376320838928\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 505 train loss: 0.4596851170063019\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 506 train loss: 0.4594325125217438\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 507 train loss: 0.4591752886772156\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 508 train loss: 0.45892083644866943\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 509 train loss: 0.4586699306964874\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 510 train loss: 0.45840975642204285\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 511 train loss: 0.4581477642059326\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 512 train loss: 0.45788541436195374\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 513 train loss: 0.4576171040534973\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 514 train loss: 0.4573483467102051\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 515 train loss: 0.457077294588089\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 516 train loss: 0.4568018615245819\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 517 train loss: 0.45652520656585693\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 518 train loss: 0.45624566078186035\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 519 train loss: 0.45596665143966675\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 520 train loss: 0.45568379759788513\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 521 train loss: 0.45539623498916626\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 522 train loss: 0.4551088511943817\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 523 train loss: 0.45481938123703003\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 524 train loss: 0.45452725887298584\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 525 train loss: 0.4542335867881775\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 526 train loss: 0.45393726229667664\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 527 train loss: 0.4536377787590027\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 528 train loss: 0.4533364772796631\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 529 train loss: 0.4530349671840668\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 530 train loss: 0.45273011922836304\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 531 train loss: 0.4524240791797638\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 532 train loss: 0.4521157145500183\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 533 train loss: 0.45180580019950867\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 534 train loss: 0.45149317383766174\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 535 train loss: 0.4511782228946686\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 536 train loss: 0.45086148381233215\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 537 train loss: 0.45054227113723755\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 538 train loss: 0.45021992921829224\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 539 train loss: 0.4498941898345947\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 540 train loss: 0.449565052986145\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 541 train loss: 0.44923096895217896\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 542 train loss: 0.44889140129089355\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 543 train loss: 0.448546826839447\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 544 train loss: 0.44819560647010803\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 545 train loss: 0.44783690571784973\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 546 train loss: 0.4474699795246124\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 547 train loss: 0.4470934569835663\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 548 train loss: 0.44670599699020386\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 549 train loss: 0.44630688428878784\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 550 train loss: 0.44589415192604065\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 551 train loss: 0.44546639919281006\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 552 train loss: 0.44502362608909607\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 553 train loss: 0.444563090801239\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 554 train loss: 0.44408321380615234\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 555 train loss: 0.44358277320861816\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 556 train loss: 0.44305840134620667\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 557 train loss: 0.442507266998291\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 558 train loss: 0.44192835688591003\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 559 train loss: 0.4413226544857025\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 560 train loss: 0.4406872093677521\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 561 train loss: 0.4400204122066498\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 562 train loss: 0.4393192529678345\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 563 train loss: 0.4385838806629181\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 564 train loss: 0.4378131330013275\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 565 train loss: 0.43700850009918213\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 566 train loss: 0.43616944551467896\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 567 train loss: 0.43530234694480896\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 568 train loss: 0.43440860509872437\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 569 train loss: 0.4334926903247833\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 570 train loss: 0.4325609505176544\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 571 train loss: 0.43162232637405396\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 572 train loss: 0.4306853413581848\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 573 train loss: 0.4297608435153961\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 574 train loss: 0.4288587272167206\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 575 train loss: 0.4279860854148865\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 576 train loss: 0.42715317010879517\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 577 train loss: 0.426363468170166\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 578 train loss: 0.42562031745910645\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 579 train loss: 0.42492446303367615\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 580 train loss: 0.4242748022079468\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 581 train loss: 0.42368149757385254\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 582 train loss: 0.4231483042240143\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 583 train loss: 0.4226606786251068\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 584 train loss: 0.4221150875091553\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 585 train loss: 0.42146679759025574\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 586 train loss: 0.42085957527160645\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 587 train loss: 0.4203810691833496\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 588 train loss: 0.4198971688747406\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 589 train loss: 0.4193207621574402\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 590 train loss: 0.41878437995910645\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 591 train loss: 0.41835108399391174\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 592 train loss: 0.4178864359855652\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 593 train loss: 0.4173877537250519\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 594 train loss: 0.41697460412979126\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 595 train loss: 0.41659608483314514\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 596 train loss: 0.4161760210990906\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 597 train loss: 0.41578441858291626\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 598 train loss: 0.41544947028160095\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 599 train loss: 0.4150998890399933\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 600 train loss: 0.41473686695098877\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 601 train loss: 0.414412260055542\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 602 train loss: 0.41410210728645325\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 603 train loss: 0.4137730896472931\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 604 train loss: 0.41345590353012085\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 605 train loss: 0.41316235065460205\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 606 train loss: 0.4128643572330475\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 607 train loss: 0.4125646948814392\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 608 train loss: 0.41227811574935913\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 609 train loss: 0.41200053691864014\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 610 train loss: 0.4117204248905182\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 611 train loss: 0.4114431142807007\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 612 train loss: 0.4111745059490204\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 613 train loss: 0.4109095335006714\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 614 train loss: 0.4106450080871582\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 615 train loss: 0.4103822708129883\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 616 train loss: 0.41012313961982727\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 617 train loss: 0.4098661541938782\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 618 train loss: 0.4096109867095947\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 619 train loss: 0.40935760736465454\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 620 train loss: 0.4091062843799591\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 621 train loss: 0.4088580012321472\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 622 train loss: 0.4086109399795532\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 623 train loss: 0.4083642363548279\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 624 train loss: 0.40811824798583984\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 625 train loss: 0.40787404775619507\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 626 train loss: 0.4076325297355652\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 627 train loss: 0.4073917865753174\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 628 train loss: 0.4071520268917084\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 629 train loss: 0.40691351890563965\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 630 train loss: 0.40667489171028137\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 631 train loss: 0.40643638372421265\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 632 train loss: 0.4061974585056305\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 633 train loss: 0.40595880150794983\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 634 train loss: 0.40572240948677063\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 635 train loss: 0.4054860472679138\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 636 train loss: 0.40525001287460327\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 637 train loss: 0.4050152003765106\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 638 train loss: 0.404780775308609\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 639 train loss: 0.4045470654964447\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 640 train loss: 0.4043138325214386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 641 train loss: 0.404081791639328\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 642 train loss: 0.4038521349430084\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 643 train loss: 0.40362271666526794\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 644 train loss: 0.40339353680610657\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 645 train loss: 0.4031648337841034\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 646 train loss: 0.40293845534324646\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 647 train loss: 0.40271398425102234\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 648 train loss: 0.4024900794029236\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 649 train loss: 0.4022666811943054\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 650 train loss: 0.4020434021949768\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 651 train loss: 0.40181851387023926\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 652 train loss: 0.4015931487083435\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 653 train loss: 0.4013674557209015\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 654 train loss: 0.40114107728004456\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 655 train loss: 0.40091514587402344\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 656 train loss: 0.4006876051425934\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 657 train loss: 0.40045779943466187\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 658 train loss: 0.40022507309913635\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 659 train loss: 0.3999953269958496\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 660 train loss: 0.3997594118118286\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 661 train loss: 0.39952296018600464\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 662 train loss: 0.39928504824638367\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 663 train loss: 0.3990432322025299\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 664 train loss: 0.398799866437912\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 665 train loss: 0.3985578715801239\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 666 train loss: 0.3983142077922821\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 667 train loss: 0.39806869626045227\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 668 train loss: 0.3978220820426941\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 669 train loss: 0.3975737690925598\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 670 train loss: 0.3973236680030823\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 671 train loss: 0.397072434425354\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 672 train loss: 0.3968205153942108\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 673 train loss: 0.3965659439563751\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 674 train loss: 0.39630889892578125\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 675 train loss: 0.3960496783256531\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 676 train loss: 0.3957890570163727\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 677 train loss: 0.3955267667770386\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 678 train loss: 0.39526426792144775\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 679 train loss: 0.39500102400779724\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 680 train loss: 0.3947359323501587\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 681 train loss: 0.39447298645973206\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 682 train loss: 0.39422640204429626\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 683 train loss: 0.39403367042541504\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 684 train loss: 0.3939082622528076\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 685 train loss: 0.3936750590801239\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 686 train loss: 0.3932324945926666\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 687 train loss: 0.39293792843818665\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 688 train loss: 0.39280393719673157\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 689 train loss: 0.3925287425518036\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 690 train loss: 0.3921562135219574\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 691 train loss: 0.3919554352760315\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 692 train loss: 0.3917369842529297\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 693 train loss: 0.39140114188194275\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 694 train loss: 0.3911362588405609\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 695 train loss: 0.39092227816581726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 696 train loss: 0.39062628149986267\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 697 train loss: 0.3903118073940277\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 698 train loss: 0.39007285237312317\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 699 train loss: 0.38980069756507874\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 700 train loss: 0.3894743323326111\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 701 train loss: 0.3891843259334564\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 702 train loss: 0.38892221450805664\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 703 train loss: 0.38860511779785156\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 704 train loss: 0.38828134536743164\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 705 train loss: 0.3879835605621338\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 706 train loss: 0.3876740336418152\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 707 train loss: 0.3873445689678192\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 708 train loss: 0.38699162006378174\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 709 train loss: 0.3866431713104248\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 710 train loss: 0.3862960934638977\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 711 train loss: 0.38593998551368713\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 712 train loss: 0.38555654883384705\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 713 train loss: 0.385155588388443\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 714 train loss: 0.38474297523498535\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 715 train loss: 0.38431987166404724\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 716 train loss: 0.3838813602924347\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 717 train loss: 0.38344067335128784\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 718 train loss: 0.3829960227012634\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 719 train loss: 0.3825591206550598\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 720 train loss: 0.38216471672058105\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 721 train loss: 0.3818768560886383\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 722 train loss: 0.3816843032836914\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 723 train loss: 0.38146257400512695\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 724 train loss: 0.3807494640350342\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 725 train loss: 0.3800297677516937\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 726 train loss: 0.37977391481399536\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 727 train loss: 0.3796335756778717\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 728 train loss: 0.3791464567184448\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 729 train loss: 0.3785235285758972\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 730 train loss: 0.37828582525253296\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 731 train loss: 0.3781015872955322\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 732 train loss: 0.37757307291030884\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 733 train loss: 0.37712839245796204\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 734 train loss: 0.3769396245479584\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 735 train loss: 0.37662434577941895\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 736 train loss: 0.37616464495658875\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 737 train loss: 0.3758534789085388\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 738 train loss: 0.37564149498939514\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 739 train loss: 0.375304639339447\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 740 train loss: 0.37491631507873535\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 741 train loss: 0.37466707825660706\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 742 train loss: 0.374444842338562\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 743 train loss: 0.37411072850227356\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 744 train loss: 0.3737824857234955\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 745 train loss: 0.3735484480857849\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 746 train loss: 0.3733205795288086\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 747 train loss: 0.37303048372268677\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 748 train loss: 0.37272873520851135\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 749 train loss: 0.37248536944389343\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 750 train loss: 0.37227293848991394\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 751 train loss: 0.3720243275165558\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 752 train loss: 0.37174978852272034\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 753 train loss: 0.37149539589881897\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 754 train loss: 0.3712783753871918\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 755 train loss: 0.3710692822933197\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 756 train loss: 0.3708280026912689\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 757 train loss: 0.37057340145111084\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 758 train loss: 0.3703303039073944\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 759 train loss: 0.37011200189590454\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 760 train loss: 0.3699067234992981\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 761 train loss: 0.3696950078010559\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 762 train loss: 0.3694777488708496\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 763 train loss: 0.3692456781864166\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 764 train loss: 0.3690168261528015\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 765 train loss: 0.3687919080257416\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 766 train loss: 0.36857765913009644\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 767 train loss: 0.3683689832687378\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 768 train loss: 0.36816203594207764\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 769 train loss: 0.3679563105106354\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 770 train loss: 0.3677462637424469\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 771 train loss: 0.36753514409065247\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 772 train loss: 0.36731961369514465\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 773 train loss: 0.3671032786369324\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 774 train loss: 0.3668842017650604\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 775 train loss: 0.3666672110557556\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 776 train loss: 0.3664500415325165\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 777 train loss: 0.36623454093933105\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 778 train loss: 0.366018146276474\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 779 train loss: 0.36580321192741394\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 780 train loss: 0.3655886650085449\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 781 train loss: 0.3653763234615326\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 782 train loss: 0.3651667535305023\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 783 train loss: 0.36496293544769287\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 784 train loss: 0.36476707458496094\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 785 train loss: 0.364584356546402\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 786 train loss: 0.3644089698791504\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 787 train loss: 0.3642619252204895\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 788 train loss: 0.3640955686569214\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 789 train loss: 0.3639319837093353\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 790 train loss: 0.36367708444595337\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 791 train loss: 0.36340048909187317\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 792 train loss: 0.36311131715774536\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 793 train loss: 0.36288824677467346\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 794 train loss: 0.36273130774497986\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 795 train loss: 0.36259302496910095\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 796 train loss: 0.3624376058578491\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 797 train loss: 0.36221590638160706\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 798 train loss: 0.3619776964187622\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 799 train loss: 0.36175480484962463\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 800 train loss: 0.36158037185668945\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 801 train loss: 0.36143454909324646\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 802 train loss: 0.3612753748893738\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 803 train loss: 0.36108750104904175\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 804 train loss: 0.36087366938591003\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 805 train loss: 0.360672265291214\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 806 train loss: 0.36049726605415344\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 807 train loss: 0.3603423535823822\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 808 train loss: 0.36018723249435425\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 809 train loss: 0.3600151240825653\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 810 train loss: 0.3598291277885437\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 811 train loss: 0.3596414625644684\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 812 train loss: 0.3594668209552765\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 813 train loss: 0.35930633544921875\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 814 train loss: 0.35915201902389526\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 815 train loss: 0.35899290442466736\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 816 train loss: 0.35882407426834106\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 817 train loss: 0.3586512804031372\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 818 train loss: 0.3584801256656647\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 819 train loss: 0.35831418633461\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 820 train loss: 0.3581543564796448\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 821 train loss: 0.3579980432987213\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 822 train loss: 0.35784196853637695\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 823 train loss: 0.35768336057662964\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 824 train loss: 0.35752156376838684\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 825 train loss: 0.35735586285591125\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 826 train loss: 0.35719063878059387\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 827 train loss: 0.35702669620513916\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 828 train loss: 0.3568641245365143\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 829 train loss: 0.3567027449607849\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 830 train loss: 0.3565421998500824\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 831 train loss: 0.3563810884952545\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 832 train loss: 0.35621875524520874\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 833 train loss: 0.3560550808906555\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 834 train loss: 0.3558896481990814\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 835 train loss: 0.35572293400764465\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 836 train loss: 0.3555544316768646\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 837 train loss: 0.35538479685783386\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 838 train loss: 0.3552139699459076\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 839 train loss: 0.3550412058830261\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 840 train loss: 0.35486626625061035\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 841 train loss: 0.3546893298625946\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 842 train loss: 0.3545103669166565\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 843 train loss: 0.35432982444763184\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 844 train loss: 0.3541465103626251\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 845 train loss: 0.35396066308021545\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 846 train loss: 0.3537740111351013\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 847 train loss: 0.3535875678062439\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 848 train loss: 0.35339877009391785\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 849 train loss: 0.35321226716041565\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 850 train loss: 0.3530218303203583\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 851 train loss: 0.3528327941894531\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 852 train loss: 0.3526356518268585\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 853 train loss: 0.35244983434677124\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 854 train loss: 0.3522365093231201\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 855 train loss: 0.35202065110206604\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 856 train loss: 0.3517579734325409\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 857 train loss: 0.3514828681945801\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 858 train loss: 0.35119497776031494\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 859 train loss: 0.3509179949760437\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 860 train loss: 0.35065919160842896\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 861 train loss: 0.35041436553001404\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 862 train loss: 0.3501760959625244\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 863 train loss: 0.3499363660812378\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 864 train loss: 0.34970226883888245\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 865 train loss: 0.34945249557495117\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 866 train loss: 0.3491942286491394\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 867 train loss: 0.34890803694725037\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 868 train loss: 0.3486149311065674\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 869 train loss: 0.34830141067504883\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 870 train loss: 0.3479870855808258\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 871 train loss: 0.3476732671260834\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 872 train loss: 0.34736767411231995\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 873 train loss: 0.34706664085388184\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 874 train loss: 0.3467727601528168\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 875 train loss: 0.3464828431606293\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 876 train loss: 0.34619250893592834\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 877 train loss: 0.345906138420105\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 878 train loss: 0.34562960267066956\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 879 train loss: 0.3453827202320099\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 880 train loss: 0.3451973795890808\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 881 train loss: 0.34518685936927795\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 882 train loss: 0.3453420102596283\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 883 train loss: 0.34567707777023315\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 884 train loss: 0.3450227379798889\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 885 train loss: 0.34408801794052124\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 886 train loss: 0.3433893024921417\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 887 train loss: 0.34356632828712463\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 888 train loss: 0.34379449486732483\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 889 train loss: 0.3428644835948944\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 890 train loss: 0.3422813415527344\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 891 train loss: 0.3424512445926666\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 892 train loss: 0.3422042429447174\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 893 train loss: 0.3415997326374054\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 894 train loss: 0.3412724733352661\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 895 train loss: 0.3412157893180847\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 896 train loss: 0.34098386764526367\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 897 train loss: 0.34048816561698914\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 898 train loss: 0.34019920229911804\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 899 train loss: 0.34012553095817566\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 900 train loss: 0.3398132026195526\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 901 train loss: 0.33937862515449524\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 902 train loss: 0.33915838599205017\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 903 train loss: 0.33900073170661926\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 904 train loss: 0.33869925141334534\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 905 train loss: 0.33834752440452576\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 906 train loss: 0.3380895256996155\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 907 train loss: 0.337899774312973\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 908 train loss: 0.3376244008541107\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 909 train loss: 0.3372825086116791\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 910 train loss: 0.3370153605937958\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 911 train loss: 0.33679887652397156\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 912 train loss: 0.33653900027275085\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 913 train loss: 0.33623600006103516\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 914 train loss: 0.3359353244304657\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 915 train loss: 0.33567488193511963\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 916 train loss: 0.33543500304222107\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 917 train loss: 0.3351638615131378\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 918 train loss: 0.33486005663871765\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 919 train loss: 0.33456361293792725\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 920 train loss: 0.3342854678630829\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 921 train loss: 0.33401983976364136\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 922 train loss: 0.3337462842464447\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 923 train loss: 0.3334483802318573\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 924 train loss: 0.33313602209091187\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 925 train loss: 0.3328268826007843\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 926 train loss: 0.3325199782848358\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 927 train loss: 0.33221814036369324\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 928 train loss: 0.3319154977798462\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 929 train loss: 0.33160465955734253\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 930 train loss: 0.33128705620765686\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 931 train loss: 0.33096495270729065\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 932 train loss: 0.3306312561035156\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 933 train loss: 0.3302915692329407\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 934 train loss: 0.32994529604911804\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 935 train loss: 0.32959359884262085\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 936 train loss: 0.3292292058467865\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 937 train loss: 0.3288627564907074\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 938 train loss: 0.3284926116466522\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 939 train loss: 0.3281223773956299\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 940 train loss: 0.3277445435523987\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 941 train loss: 0.3273802101612091\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 942 train loss: 0.3270058333873749\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 943 train loss: 0.3266635537147522\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 944 train loss: 0.3262901306152344\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 945 train loss: 0.3259211778640747\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 946 train loss: 0.3254058063030243\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 947 train loss: 0.32485952973365784\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 948 train loss: 0.3242361843585968\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 949 train loss: 0.32366448640823364\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 950 train loss: 0.32317206263542175\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 951 train loss: 0.32274818420410156\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 952 train loss: 0.32236164808273315\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 953 train loss: 0.32196909189224243\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 954 train loss: 0.3215601444244385\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 955 train loss: 0.3210863769054413\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 956 train loss: 0.3205898404121399\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 957 train loss: 0.32005828619003296\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 958 train loss: 0.3195471465587616\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 959 train loss: 0.31907662749290466\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 960 train loss: 0.3186533749103546\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 961 train loss: 0.3182588517665863\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 962 train loss: 0.3178812861442566\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 963 train loss: 0.3175179362297058\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 964 train loss: 0.31719842553138733\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 965 train loss: 0.3169274628162384\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 966 train loss: 0.316646546125412\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 967 train loss: 0.316386342048645\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 968 train loss: 0.3159277141094208\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 969 train loss: 0.31544622778892517\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 970 train loss: 0.31485113501548767\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 971 train loss: 0.3143309950828552\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 972 train loss: 0.31392645835876465\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 973 train loss: 0.3136152923107147\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 974 train loss: 0.31334659457206726\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 975 train loss: 0.3130699694156647\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 976 train loss: 0.31278467178344727\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 977 train loss: 0.3123871684074402\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 978 train loss: 0.3119560182094574\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 979 train loss: 0.3114880621433258\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 980 train loss: 0.31105417013168335\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 981 train loss: 0.3106524348258972\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 982 train loss: 0.3102957010269165\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 983 train loss: 0.30997711420059204\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 984 train loss: 0.3096808195114136\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 985 train loss: 0.30940577387809753\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 986 train loss: 0.3091263771057129\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 987 train loss: 0.30889007449150085\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 988 train loss: 0.3086184561252594\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 989 train loss: 0.3084249794483185\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 990 train loss: 0.30807602405548096\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 991 train loss: 0.30768024921417236\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 992 train loss: 0.30700626969337463\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 993 train loss: 0.3064509630203247\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 994 train loss: 0.306152880191803\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 995 train loss: 0.3059881627559662\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 996 train loss: 0.3058006167411804\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 997 train loss: 0.3054030239582062\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 998 train loss: 0.30505606532096863\n",
      "weights:  tensor([0.2591, 0.2646, 0.2604,  ..., 0.2538, 0.2286, 0.2238]) min:  tensor(0.1754) max:  tensor(0.6469)\n",
      "epoch: 999 train loss: 0.30469924211502075\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training loop starts\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Generating output\n",
    "    out = model(xt_tensor)\n",
    "    hidden_feature = model.encoder(xt_tensor)\n",
    "#         print('out', out.shape, out[0])\n",
    "    query_hidden_feat = model.encoder(xq_learn_tensor)\n",
    "\n",
    "    # Calculating loss\n",
    "    if epoch == 0:\n",
    "        print('out', out.shape, out) \n",
    "        print('xt_tensor', xt_tensor.shape, xt_tensor)\n",
    "        print('hidden_feature', hidden_feature.shape, hidden_feature)\n",
    "#         print('centroid_vectors', centroid_vectors.shape, centroid_vectors)\n",
    "#         print('partition_id_list', partition_id_list.shape, partition_id_list)\n",
    "        \n",
    "    loss = loss_func(out, xt_tensor, xq_learn_tensor, hidden_feature, query_hidden_feat)\n",
    "\n",
    "    # Updating weights according\n",
    "    # to the calculated loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    clip_gradient = 1\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_gradient) # clip gradieent\n",
    "    optimizer.step()\n",
    "\n",
    "    # Incrementing loss\n",
    "    running_loss = loss.item()\n",
    "\n",
    "    train_loss.append(running_loss)\n",
    "    \n",
    "#     out_val = model(x_val)\n",
    "#     out_hidden = \n",
    "#     loss_val = loss_func(out_val, x_val)\n",
    "#     validation_loss.append(loss_val)\n",
    "    \n",
    "    print(\"epoch:\", epoch, \"train loss:\", running_loss)\n",
    "#     print(\"epoch:\", epoch, \"train loss:\", running_loss, \"val loss: \", loss_val.item())\n",
    "\n",
    "    # Storing useful images and\n",
    "    # reconstructed outputs for the last batch\n",
    "    outputs[epoch+1] = {'in': xt_tensor, 'out': out}\n",
    "    \n",
    "\n",
    "# Plotting the training loss\n",
    "plt.plot(range(1,num_epochs+1),train_loss)\n",
    "# plt.plot(range(1,num_epochs+1),validation_loss)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f3bc8",
   "metadata": {},
   "source": [
    "## K-means + Recall (on trained vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a48c91eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7397]\n"
     ]
    }
   ],
   "source": [
    "# Compute nearest neighbor\n",
    "def scan_all(query_vec, vector_set):\n",
    "    \n",
    "    min_dist = 1e10\n",
    "    min_dist_ID = None\n",
    "    for vec_id, dataset_vec in enumerate(vector_set):\n",
    "        dist = np.linalg.norm(query_vec - dataset_vec)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_dist_ID = vec_id\n",
    "        elif dist == min_dist:\n",
    "            print('tied distance, keep previous ID, skip current ID')\n",
    "            \n",
    "    return min_dist_ID\n",
    "\n",
    "gt_xq_learn = [] \n",
    "for query in xq_learn:\n",
    "    gt_xq_learn.append(scan_all(query, xt))\n",
    "print(gt_xq_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e795f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "757643e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32)\n",
      "[-0.89485466  0.34480774  0.47402635 -0.06020072 -2.0539231   3.635032\n",
      " -0.62083983 -1.0993295   2.2461667  -3.6708348  -2.6218154  -0.898036\n",
      "  2.2595727   1.3020664   0.54303455 -0.39367303  0.7861176  -2.4788344\n",
      " -0.96311283  2.5448287  -2.969053    1.7663007  -1.8186846  -1.5289774\n",
      " -1.1942096   1.2435136  -3.0043976   1.267514   -3.4240587  -1.1178623\n",
      "  1.3548738  -1.2918974 ]\n"
     ]
    }
   ],
   "source": [
    "hidden_features_10K = model.encoder(torch.FloatTensor(xt)).detach().numpy()\n",
    "print(hidden_features_10K.shape)\n",
    "print(hidden_features_10K[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a69a56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 25 25 ... 23 28  3]\n",
      "[[-1.0519848   1.6776824  -0.6670559  ... -2.4929235   1.689954\n",
      "  -0.56035924]\n",
      " [-1.0514224   0.47505525  1.6248525  ... -0.11530817 -0.01116067\n",
      "  -2.0756214 ]\n",
      " [-1.1493677   1.8693256   0.4979278  ... -2.4362707   1.1461761\n",
      "  -2.3129468 ]\n",
      " ...\n",
      " [-0.40516418  0.23725754  2.022428   ... -0.22720355 -0.0975706\n",
      "  -2.1657333 ]\n",
      " [-0.57192934  0.08126658  1.8468227  ... -0.03009343 -0.1424846\n",
      "  -2.093493  ]\n",
      " [-0.9624461   0.9851203   0.19244081 ... -1.1816211   1.0242264\n",
      "  -0.6410636 ]]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=32)\n",
    "kmeans.fit(hidden_features_10K)\n",
    "partition_IDs = kmeans.predict(hidden_features_10K)\n",
    "print(partition_IDs)\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49588372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items in partition  0 290 average = 312\n",
      "items in partition  1 463 average = 312\n",
      "items in partition  2 288 average = 312\n",
      "items in partition  3 365 average = 312\n",
      "items in partition  4 328 average = 312\n",
      "items in partition  5 408 average = 312\n",
      "items in partition  6 230 average = 312\n",
      "items in partition  7 254 average = 312\n",
      "items in partition  8 194 average = 312\n",
      "items in partition  9 398 average = 312\n",
      "items in partition  10 174 average = 312\n",
      "items in partition  11 359 average = 312\n",
      "items in partition  12 282 average = 312\n",
      "items in partition  13 265 average = 312\n",
      "items in partition  14 352 average = 312\n",
      "items in partition  15 172 average = 312\n",
      "items in partition  16 406 average = 312\n",
      "items in partition  17 197 average = 312\n",
      "items in partition  18 182 average = 312\n",
      "items in partition  19 177 average = 312\n",
      "items in partition  20 282 average = 312\n",
      "items in partition  21 330 average = 312\n",
      "items in partition  22 343 average = 312\n",
      "items in partition  23 354 average = 312\n",
      "items in partition  24 359 average = 312\n",
      "items in partition  25 390 average = 312\n",
      "items in partition  26 292 average = 312\n",
      "items in partition  27 205 average = 312\n",
      "items in partition  28 413 average = 312\n",
      "items in partition  29 422 average = 312\n",
      "items in partition  30 461 average = 312\n",
      "items in partition  31 365 average = 312\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping: partition ID -> {list of vector IDs}\n",
    "\n",
    "num_partition = 32\n",
    "\n",
    "partition_id_vec_id_list_100K = dict()\n",
    "for i in range(num_partition):\n",
    "    partition_id_vec_id_list_100K[i] = []\n",
    "\n",
    "\n",
    "for i in range(num_vec_train):\n",
    "    partition_ID = int(partition_IDs[i])\n",
    "    partition_id_vec_id_list_100K[partition_ID].append(i)\n",
    "    \n",
    "for i in range(num_partition):\n",
    "    print('items in partition ', i, len(partition_id_vec_id_list_100K[i]), 'average =', int(num_vec_train/num_partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75015606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def scan_partition(query_vec, partition_id_list, vector_set):\n",
    "    \"\"\"\n",
    "    query_vec = (128, )\n",
    "    partition_id_list = (N_num_vec, )\n",
    "    vector_set = 1M dataset (1M, 128)\n",
    "    \"\"\"\n",
    "    min_dist = 1e10\n",
    "    min_dist_ID = None\n",
    "    for vec_id in partition_id_list:\n",
    "        dataset_vec = vector_set[vec_id]\n",
    "        dist = np.linalg.norm(query_vec - dataset_vec)\n",
    "        if dist <= min_dist:\n",
    "            min_dist = dist\n",
    "            min_dist_ID = vec_id\n",
    "            \n",
    "    return min_dist_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94aaa054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N= 1\n",
      "(10000, 32)\n",
      "0 7408\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors = []\n",
    "\n",
    "N = num_query_learn\n",
    "print('N=', N)\n",
    "#### Wenqi: here had a bug: previously xb, now xq\n",
    "query_hidden_feature = model.encoder(torch.FloatTensor(xq)).detach().numpy()\n",
    "print(query_hidden_feature.shape)\n",
    "query_partition = kmeans.predict(query_hidden_feature)\n",
    "\n",
    "for i in range(N):\n",
    "    partition_id = int(query_partition[i])\n",
    "    nearest_neighbor_ID = scan_partition(xq[i], partition_id_vec_id_list_100K[partition_id], xb)\n",
    "    nearest_neighbors.append(nearest_neighbor_ID)\n",
    "    print(i, nearest_neighbor_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b77d0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 recall@1 =  0.0\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "for i in range(N):\n",
    "    if nearest_neighbors[i] == gt_xq_learn[i]:\n",
    "        correct_count += 1\n",
    "        \n",
    "print(correct_count, 'recall@1 = ', correct_count / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585cb511",
   "metadata": {},
   "source": [
    "## K-means + Recall (on 1M vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c974d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ced4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_features_10K = model.encoder(torch.FloatTensor(xt)).detach().numpy()\n",
    "print(hidden_features_10K.shape)\n",
    "print(hidden_features_10K[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c75c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=32)\n",
    "kmeans.fit(hidden_features_10K)\n",
    "y_kmeans = kmeans.predict(hidden_features_10K)\n",
    "print(y_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_feature_1M = model.encoder(torch.FloatTensor(xb)).detach().numpy()\n",
    "\n",
    "partition_IDs = kmeans.predict(hidden_feature_1M)\n",
    "print(partition_IDs)\n",
    "print(partition_IDs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping: partition ID -> {list of vector IDs}\n",
    "\n",
    "num_partition = 32\n",
    "\n",
    "partition_id_vec_id_list_1M = dict()\n",
    "for i in range(num_partition):\n",
    "    partition_id_vec_id_list_1M[i] = []\n",
    "\n",
    "\n",
    "for i in range(int(1e6)):\n",
    "    partition_ID = int(partition_IDs[i])\n",
    "    partition_id_vec_id_list_1M[partition_ID].append(i)\n",
    "    \n",
    "for i in range(num_partition):\n",
    "    print('items in partition ', i, len(partition_id_vec_id_list_1M[i]), 'average =', int(1e6/num_partition))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def scan_partition(query_vec, partition_id_list, vector_set):\n",
    "    \"\"\"\n",
    "    query_vec = (128, )\n",
    "    partition_id_list = (N_num_vec, )\n",
    "    vector_set = 1M dataset (1M, 128)\n",
    "    \"\"\"\n",
    "    min_dist = 1e10\n",
    "    min_dist_ID = None\n",
    "    for vec_id in partition_id_list:\n",
    "        dataset_vec = vector_set[vec_id]\n",
    "        dist = np.linalg.norm(query_vec - dataset_vec)\n",
    "        if dist <= min_dist:\n",
    "            min_dist = dist\n",
    "            min_dist_ID = vec_id\n",
    "            \n",
    "    return min_dist_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67861cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors = []\n",
    "\n",
    "N = num_query_learn\n",
    "print('N=', N)\n",
    "#### Wenqi: here had a bug: previously xb, now xq\n",
    "query_hidden_feature = model.encoder(torch.FloatTensor(xq)).detach().numpy()\n",
    "print(query_hidden_feature.shape)\n",
    "query_partition = kmeans.predict(query_hidden_feature)\n",
    "\n",
    "for i in range(N):\n",
    "    partition_id = int(query_partition[i])\n",
    "    nearest_neighbor_ID = scan_partition(xq[i], partition_id_vec_id_list_1M[partition_id], xb)\n",
    "    nearest_neighbors.append(nearest_neighbor_ID)\n",
    "    print(i, nearest_neighbor_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea62cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall (first 10 query vec to overfit; 1000 iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fdd2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = 0\n",
    "for i in range(N):\n",
    "    if nearest_neighbors[i] == gt[i][0]:\n",
    "        correct_count += 1\n",
    "        \n",
    "print(correct_count, 'recall@1 = ', correct_count / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e9716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
