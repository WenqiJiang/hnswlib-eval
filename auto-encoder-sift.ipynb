{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b7f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 15, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612285d0",
   "metadata": {},
   "source": [
    "## Load SIFT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2d7735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmap_fvecs(fname):\n",
    "    x = np.memmap(fname, dtype='int32', mode='r')\n",
    "    d = x[0]\n",
    "    return x.view('float32').reshape(-1, d + 1)[:, 1:]\n",
    "\n",
    "def mmap_bvecs(fname):\n",
    "    x = np.memmap(fname, dtype='uint8', mode='r')\n",
    "    d = x[:4].view('int32')[0]\n",
    "    return x.reshape(-1, d + 4)[:, 4:]\n",
    "\n",
    "def ivecs_read(fname):\n",
    "    a = np.fromfile(fname, dtype='int32')\n",
    "    d = a[0]\n",
    "    # Wenqi: Format of ground truth (for 10000 query vectors):\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #        ...     ...\n",
    "    #   1000(topK), [1000 ids]\n",
    "    # 10000 rows in total, 10000 * 1001 elements, 10000 * 1001 * 4 bytes\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy()\n",
    "\n",
    "def fvecs_read(fname):\n",
    "    return ivecs_read(fname).view('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e3e22db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shapes:\n",
      "Base vector xb:  (1000000, 128)\n",
      "Query vector xq:  (10000, 128)\n",
      "Ground truth gt:  (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "dbname = 'SIFT1M'\n",
    "\n",
    "if dbname.startswith('SIFT'):\n",
    "    # SIFT1M to SIFT1000M\n",
    "    dbsize = int(dbname[4:-1])\n",
    "    xb = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_base.bvecs')\n",
    "    xq = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_query.bvecs')\n",
    "    gt = ivecs_read('/mnt/scratch/wenqi/Faiss_experiments/bigann/gnd/idx_%dM.ivecs' % dbsize)\n",
    "\n",
    "    N_VEC = int(dbsize * 1000 * 1000)\n",
    "\n",
    "    # trim xb to correct size\n",
    "    xb = xb[:dbsize * 1000 * 1000]\n",
    "\n",
    "    # Wenqi: load xq to main memory and reshape\n",
    "    xq = xq.astype('float32').copy()\n",
    "#     xq = np.array(xq, dtype=np.float32)\n",
    "    xb = xb.astype('float32').copy()\n",
    "    gt = np.array(gt, dtype=np.int32)\n",
    "\n",
    "    print(\"Vector shapes:\")\n",
    "    print(\"Base vector xb: \", xb.shape)\n",
    "    print(\"Query vector xq: \", xq.shape)\n",
    "    print(\"Ground truth gt: \", gt.shape)\n",
    "else:\n",
    "    print('unknown dataset', dbname, file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "dim = xb.shape[1] # should be 128\n",
    "nq = xq.shape[0]\n",
    "\n",
    "# Normalize all to 0~1\n",
    "xb = xb / 256\n",
    "xq = xq / 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b381dae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Using the first 10K vectors for training, just as the graph & IVF experiments\n",
    "n_train = int(1e4)\n",
    "xt = xb[:n_train]\n",
    "print(xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fdbf68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.00390625, 0.03125   ,\n",
       "       0.02734375, 0.01171875, 0.0078125 , 0.01953125, 0.        ,\n",
       "       0.        , 0.01171875, 0.01953125, 0.02734375, 0.04296875,\n",
       "       0.12109375, 0.05078125, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11328125, 0.4140625 , 0.41796875, 0.05078125,\n",
       "       0.        , 0.        , 0.        , 0.00390625, 0.23828125,\n",
       "       0.2734375 , 0.1640625 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00390625, 0.08984375, 0.109375  , 0.0625    ,\n",
       "       0.24609375, 0.015625  , 0.        , 0.        , 0.        ,\n",
       "       0.0234375 , 0.32421875, 0.31640625, 0.45703125, 0.3359375 ,\n",
       "       0.09765625, 0.05859375, 0.06640625, 0.1953125 , 0.328125  ,\n",
       "       0.45703125, 0.12109375, 0.08984375, 0.0703125 , 0.13671875,\n",
       "       0.37890625, 0.45703125, 0.19140625, 0.09375   , 0.265625  ,\n",
       "       0.10546875, 0.        , 0.        , 0.        , 0.015625  ,\n",
       "       0.11328125, 0.27734375, 0.31640625, 0.18359375, 0.05078125,\n",
       "       0.0390625 , 0.125     , 0.33984375, 0.45703125, 0.45703125,\n",
       "       0.17578125, 0.296875  , 0.15625   , 0.0859375 , 0.234375  ,\n",
       "       0.2734375 , 0.16015625, 0.03515625, 0.02734375, 0.08203125,\n",
       "       0.11328125, 0.15234375, 0.20703125, 0.08203125, 0.015625  ,\n",
       "       0.00390625, 0.21484375, 0.28125   , 0.01171875, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03515625, 0.25390625,\n",
       "       0.45703125, 0.28515625, 0.14453125, 0.109375  , 0.08984375,\n",
       "       0.06640625, 0.1328125 , 0.04296875, 0.04296875, 0.10546875,\n",
       "       0.23828125, 0.25      , 0.09765625, 0.015625  , 0.        ,\n",
       "       0.1640625 , 0.05078125, 0.00390625, 0.00390625, 0.00390625,\n",
       "       0.0546875 , 0.0390625 , 0.0234375 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6ce610",
   "metadata": {},
   "source": [
    "## Model Declaration\n",
    "\n",
    "Maybe I need to get rid of the sigmoid function, because there are a lot of 0s and 255s, and using sigmoid to get those values are very hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e72f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DeepAutoencoder class\n",
    "\n",
    "# We partition the data to 32 shards (bottleneck dimension)\n",
    "# The input data is 128-dimension\n",
    "class DeepAutoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "#             torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Instantiating the model and hyperparameters\n",
    "model = DeepAutoencoder()\n",
    "# loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04dbedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_func(reconstructed_feat, input_feat, hidden_feat, centroid_vectors, partition_ids):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     \"\"\"\n",
    "#     full_dim = reconstructed_feat.shape[1]\n",
    "#     hidden_dim = hidden_feat.shape[1]\n",
    "#     hidden_feat_factor = full_dim / hidden_dim\n",
    "    \n",
    "#     batch_size = reconstructed_feat.shape[0]\n",
    "# #     assert batch_size == input_feat.shape[0] and batch_size == hidden_feat.shape[0] and \\\n",
    "# #         batch_size == partition_ids.shape[0]]\n",
    "# #     assert hidden_dim == centroid_vectors.shape[1]\n",
    "\n",
    "#     min_square_error = (reconstructed_feat - input_feat)**2\n",
    "#     cluster_error = 0\n",
    "#     for i in range(hidden_feat.shape[0]):\n",
    "#         cluster_error += torch.sum((hidden_feat[i] - centroid_vectors[partition_ids[i]]) ** 2)\n",
    "#     ave_cluster_error = hidden_feat_factor * cluster_error / batch_size / hidden_dim\n",
    "#     loss = torch.mean(min_square_error) + ave_cluster_error\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69692b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(reconstructed_feat, input_feat, hidden_feat, centroid_vectors):\n",
    "    \"\"\"\n",
    "    centroid vectors: same shape as hidden_feat\n",
    "    \"\"\"\n",
    "    \n",
    "    num_vec = reconstructed_feat.shape[0]\n",
    "    full_dim = reconstructed_feat.shape[1]\n",
    "    hidden_dim = hidden_feat.shape[1]\n",
    "    num_partition = hidden_dim\n",
    "    hidden_feat_factor = full_dim / hidden_dim\n",
    "    \n",
    "    reconstruct_dist = (reconstructed_feat - input_feat)**2\n",
    "    min_square_error = torch.sum(reconstruct_dist) / num_vec\n",
    "    \n",
    "    cluster_error = 0\n",
    "    for i in range(num_vec):\n",
    "        dist_mat = (hidden_feature[i].repeat(num_partition, 1) - centroid_vectors) ** 2\n",
    "        dist_to_each_cluster = torch.sum(dist_mat, dim=1)\n",
    "        min_cluster_dist = torch.min(dist_to_each_cluster)\n",
    "        cluster_error += min_cluster_dist\n",
    "#         print('dist_mat', dist_mat)\n",
    "#         print('dist_to_each_cluster', dist_to_each_cluster)\n",
    "#         print('min_cluster_dist', min_cluster_dist)\n",
    "#         print('cluster_error', cluster_error)\n",
    "    \n",
    "#     cluster_error = \n",
    "    ave_cluster_error = hidden_feat_factor * cluster_error / num_vec\n",
    "    \n",
    "    print('min_square_error', min_square_error)\n",
    "    print('ave_cluster_error', ave_cluster_error)\n",
    "    \n",
    "    loss = min_square_error + ave_cluster_error\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3cfdba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DeepAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=32, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dbbbff",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda7dc28",
   "metadata": {},
   "source": [
    "Model 1: no sigmoid in the middle, with sigmoid in the end; 3 layers\n",
    "Init loss (sigmoid) = 0.06 (given batch size = 100, ~600)\n",
    "loss after 100 epoch = 0.005 (~10% init)\n",
    "loss after 1100 epoch = 0.00236 (~4% init)\n",
    "\n",
    "Model 2: no sigmoid in the middle, no sigmoid in the end; 4 layers\n",
    "Init loss = 492\n",
    "loss after 100 epoch = 221 (~50% init)\n",
    "loss after 600 epoch = 161\n",
    "loss after 1100 epoch = 153 (switching batch size to 1000): \n",
    "\n",
    "Model 3: no sigmoid in the middle, with sigmoid in the end; 4 layers\n",
    "Init loss (sigmoid) = 8\n",
    "loss after 100 epoch = 0.500 (~10% init)\n",
    "loss after 1100 epoch = 0.260 (~3% init)\n",
    "\n",
    "Model 3: with sigmoid in the middle, with sigmoid in the end; 4 layers\n",
    "Init loss (sigmoid) = 5.4\n",
    "loss after 100 epoch = 0.57 (~10% init)\n",
    "loss after 1000 epoch = 0.29 (~3% init)\n",
    "\n",
    "\n",
    "Model 3: no sigmoid in the middle, no sigmoid in the end; 5 layers\n",
    "Init loss (sigmoid) = 1.0\n",
    "loss after 100 epoch = xx (~10% init)\n",
    "loss after 1000 epoch = xx (~3% init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cd8d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroid_vectors(hidden_feature, centroid_vectors):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "      - hidden_feature (torch tensor): (num_vectors, num_partitions)\n",
    "      - center_vectors: (num_partitions, num_partitions)\n",
    "    \n",
    "    Output (updated):\n",
    "      - center_vectors (torch tensor): (num_partitions, num_partitions) \n",
    "          first dim: different centroid IDs\n",
    "          second dim: an entire centroid vector\n",
    "    \"\"\"\n",
    "    num_vectors = hidden_feature.shape[0]\n",
    "    num_partition = hidden_feature.shape[1]\n",
    "    \n",
    "    min_cluster_ID_list = [] # len = num vectors \n",
    "    for i in range(num_vectors):\n",
    "        dist_mat = (hidden_feature[i].repeat(32, 1) - centroid_vectors) ** 2\n",
    "        dist_to_each_cluster = torch.sum(dist_mat, dim=1)\n",
    "        min_cluster_ID = torch.argmin(dist_to_each_cluster)\n",
    "        min_cluster_ID_list.append(int(min_cluster_ID))\n",
    "    print('min_cluster_ID_list (first 10)', len(min_cluster_ID_list), min_cluster_ID_list[:10])\n",
    "    \n",
    "    num_vectors_per_partition = np.zeros(num_partition)\n",
    "    new_centroid_vectors_sum = torch.zeros(num_partitions, num_partitions) \n",
    "    for vec_id in range(num_vectors):\n",
    "        partition_ID = min_cluster_ID_list[vec_id]\n",
    "        num_vectors_per_partition[partition_ID] += 1\n",
    "        new_centroid_vectors_sum[partition_ID] += hidden_feature[vec_id]\n",
    "        \n",
    "#     print('new_centroid_vectors_sum', new_centroid_vectors_sum)\n",
    "    new_centroid_vectors = torch.zeros(num_partitions, num_partitions) \n",
    "    for partition_id in range(num_partition):\n",
    "        new_centroid_vectors[partition_id] = \\\n",
    "            new_centroid_vectors_sum[partition_id] / num_vectors_per_partition[partition_id] \n",
    "    \n",
    "    return new_centroid_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f866c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_centroid_vectors(hidden_feature, partition_id_list):\n",
    "#     \"\"\"\n",
    "#     Input: \n",
    "#       - hidden_feature (torch tensor): (num_vectors, num_partitions)\n",
    "#       - ID list (array): size = vector number; vec ID -> partition ID\n",
    "    \n",
    "#     Output (updated):\n",
    "#       - centroids info (torch tensor): (num_partitions, num_partitions) \n",
    "#           first dim: different centroid IDs\n",
    "#           second dim: an entire centroid vector\n",
    "#     \"\"\"\n",
    "#     num_vectors = len(partition_id_list)\n",
    "#     num_partition = hidden_feature.shape[1]\n",
    "    \n",
    "#     num_vectors_per_partition = np.zeros(num_partition)\n",
    "#     new_centroid_vectors_sum = torch.zeros(num_partitions, num_partitions) \n",
    "#     for vec_id in range(num_vectors):\n",
    "#         partition_ID = partition_id_list[vec_id]\n",
    "#         num_vectors_per_partition[partition_ID] += 1\n",
    "#         new_centroid_vectors_sum[partition_ID] += hidden_feature[vec_id]\n",
    "        \n",
    "#     new_centroid_vectors = torch.zeros(num_partitions, num_partitions) \n",
    "#     for partition_id in range(num_partition):\n",
    "#         new_centroid_vectors[partition_id] = \\\n",
    "#             new_centroid_vectors_sum[partition_id] / num_vectors_per_partition[partition_id] \n",
    "    \n",
    "#     return new_centroid_vectors\n",
    "\n",
    "# def update_centroids_info(hidden_feature, partition_id_list, centroid_vectors):\n",
    "#     \"\"\"\n",
    "#     Input: \n",
    "#       - hidden_feature (torch tensor): (num_vectors, num_partitions)\n",
    "#       - ID list (array): size = vector number; vec ID -> partition ID\n",
    "#       - centroids info (torch tensor): (num_partitions, num_partitions) \n",
    "#           first dim: different centroid IDs\n",
    "#           second dim: an entire centroid vector\n",
    "    \n",
    "#     Output (updated):\n",
    "#       - ID list (array): vec ID -> partition ID\n",
    "#       - centroids info (torch tensor): (num_partitions, num_partitions) \n",
    "#           first dim: different centroid IDs\n",
    "#           second dim: an entire centroid vector\n",
    "#     \"\"\"\n",
    "    \n",
    "#     num_vectors = len(partition_id_list)\n",
    "#     num_partition = centroid_vectors.shape[0]\n",
    "    \n",
    "#     num_vectors_per_partition = np.zeros(num_partition)\n",
    "#     new_centroid_vectors_sum = torch.zeros(num_partitions, num_partitions) \n",
    "#     for vec_id in range(num_vectors):\n",
    "#         partition_ID = partition_id_list[vec_id]\n",
    "#         num_vectors_per_partition[partition_ID] += 1\n",
    "#         new_centroid_vectors_sum[partition_ID] += hidden_feature[vec_id]\n",
    "        \n",
    "#     new_centroid_vectors = torch.zeros(num_partitions, num_partitions) \n",
    "#     for partition_id in range(num_partition):\n",
    "#         new_centroid_vectors[partition_id] = \\\n",
    "#             new_centroid_vectors_sum[partition_id] / num_vectors_per_partition[partition_id]\n",
    "        \n",
    "#     new_L2_dist = torch.zeros((num_partition, num_vectors))\n",
    "#     for paritition_id in range(num_partition):\n",
    "#         # L2 distance to the i th centroid, output size = num_vec\n",
    "#         new_L2_dist[paritition_id] = torch.sum((hidden_feature - centroid_vectors[paritition_id]) ** 2, dim=1)\n",
    "#     new_partition_id_list = torch.argmin(new_L2_dist, dim = 0)\n",
    "    \n",
    "#     return new_partition_id_list, new_centroid_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a131aeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centroid vectors tensor([[-0.0028,  0.1540,  0.0474,  ..., -0.0945, -0.0572, -0.0862],\n",
      "        [-0.0059,  0.1389,  0.0396,  ..., -0.1163, -0.0445, -0.0839],\n",
      "        [-0.0137,  0.1601,  0.0562,  ..., -0.1031, -0.0604, -0.0754],\n",
      "        ...,\n",
      "        [ 0.0040,  0.1552,  0.0719,  ..., -0.1166, -0.0390, -0.0862],\n",
      "        [-0.0011,  0.1667,  0.0589,  ..., -0.0964, -0.0484, -0.0779],\n",
      "        [-0.0080,  0.1607,  0.0551,  ..., -0.1108, -0.0573, -0.0815]])\n",
      "centroid_vectors tensor([[-0.0028,  0.1540,  0.0474,  ..., -0.0945, -0.0572, -0.0862],\n",
      "        [-0.0059,  0.1389,  0.0396,  ..., -0.1163, -0.0445, -0.0839],\n",
      "        [-0.0137,  0.1601,  0.0562,  ..., -0.1031, -0.0604, -0.0754],\n",
      "        ...,\n",
      "        [ 0.0040,  0.1552,  0.0719,  ..., -0.1166, -0.0390, -0.0862],\n",
      "        [-0.0011,  0.1667,  0.0589,  ..., -0.0964, -0.0484, -0.0779],\n",
      "        [-0.0080,  0.1607,  0.0551,  ..., -0.1108, -0.0573, -0.0815]])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10 # Wenqi: original iteration = 100\n",
    "batch_size = 100\n",
    "num_train_vectors = int(1e4)\n",
    "num_partitions = 32\n",
    "\n",
    "# List that will store the training loss\n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Dictionary that will store the\n",
    "# different images and outputs for\n",
    "# various epochs\n",
    "outputs = {}\n",
    "# x_val = torch.FloatTensor(xb[num_train_vectors: 2 * num_train_vectors])\n",
    "xt_tensor = torch.FloatTensor(xt)\n",
    "\n",
    "hidden_feature = model.encoder(xt_tensor)\n",
    "# Initiation: each vector starts with a random partition ID; \n",
    "#   each partition has a center vector\n",
    "# xt_closest_partition_id = np.random.randint(0, high=num_partitions, size=num_train_vectors)\n",
    "# centroid_vectors = get_centroid_vectors(hidden_feature, xt_closest_partition_id)\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=32)\n",
    "kmeans.fit(hidden_feature.detach().numpy())\n",
    "centroid_vectors = torch.tensor(kmeans.cluster_centers_)\n",
    "\n",
    "# print('partition ID list', xt_closest_partition_id)\n",
    "print('centroid vectors', centroid_vectors)\n",
    "\n",
    "# Check\n",
    "print('centroid_vectors', centroid_vectors)\n",
    "for i in range(centroid_vectors.shape[0]):\n",
    "    for j in range(centroid_vectors.shape[1]):\n",
    "        if torch.isnan(centroid_vectors[i][j]):\n",
    "            print('Error: centroid_vectors[{}][{}] is NaN!'.format(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fceee20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_cluster_ID_list (first 10) 10000 [30, 7, 30, 18, 30, 9, 7, 30, 17, 29]\n",
      "centroid_vectors tensor([[-0.0028,  0.1540,  0.0474,  ..., -0.0945, -0.0572, -0.0862],\n",
      "        [-0.0059,  0.1389,  0.0396,  ..., -0.1163, -0.0445, -0.0839],\n",
      "        [-0.0137,  0.1601,  0.0562,  ..., -0.1031, -0.0604, -0.0754],\n",
      "        ...,\n",
      "        [ 0.0040,  0.1552,  0.0719,  ..., -0.1166, -0.0390, -0.0862],\n",
      "        [-0.0011,  0.1667,  0.0589,  ..., -0.0964, -0.0484, -0.0779],\n",
      "        [-0.0080,  0.1607,  0.0551,  ..., -0.1108, -0.0573, -0.0815]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "centroid_vectors = update_centroid_vectors(hidden_feature, centroid_vectors)\n",
    "print('centroid_vectors', centroid_vectors)\n",
    "for i in range(centroid_vectors.shape[0]):\n",
    "    for j in range(centroid_vectors.shape[1]):\n",
    "        if torch.isnan(centroid_vectors[i][j]):\n",
    "            print('Error: centroid_vectors[{}][{}] is NaN!'.format(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf8296b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128])\n",
      "tensor(0.1074) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "sift_in_batch = xt_tensor[0 * batch_size: (0 + 1) * batch_size]\n",
    "print((sift_in_batch ** 2).shape)\n",
    "print(torch.mean(sift_in_batch), torch.mean(sift_in_batch).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed29f2",
   "metadata": {},
   "source": [
    "Initial loss: 22\n",
    "\n",
    "After 10 iteration: 2.3 (batch size = 100, training sample = 100K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ecfd68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_square_error tensor(22.5145, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(22.4619, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(22.4636, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(22.4664, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(21.9538, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(21.9538, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(22.0210, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(21.8290, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(21.9610, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(21.4383, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(21.9474, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(21.4359, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(21.6074, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(21.3363, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(20.9439, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(20.3469, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(20.4333, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(20.6462, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(20.2255, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(19.7041, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(18.9547, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(18.9835, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(17.8553, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(17.8113, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(17.1848, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(16.6782, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(16.4665, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(14.9359, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(14.0286, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(12.7973, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(12.2618, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(10.4922, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(9.3069, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(8.2995, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(6.9335, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(6.0978, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(4.8589, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(4.1260, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0091, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.5117, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0093, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.2319, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0098, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.0960, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0097, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6893, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0095, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.7564, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0101, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.8237, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.9462, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0100, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.9388, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.0655, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.9759, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0105, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.1285, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0114, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.1405, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0106, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.0253, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0114, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.0274, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0114, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.1699, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0114, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.2479, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.9589, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.9458, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.9334, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.0294, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.2192, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.2697, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.2246, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0116, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(3.1681, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6506, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.8534, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0116, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6633, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6913, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.8063, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.7644, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6829, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5469, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5416, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0145, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5120, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4404, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0145, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_square_error tensor(2.4703, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4067, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3945, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2773, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3209, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0144, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4736, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5633, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3040, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4008, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2762, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0153, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3749, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3621, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0153, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4038, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3265, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0156, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2967, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0157, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2372, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0168, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3294, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0173, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.1773, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0165, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2909, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0181, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.1906, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0171, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.1932, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0201, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.1379, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0208, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.1756, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0245, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2473, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0252, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2201, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0280, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2322, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0334, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2905, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.0422, grad_fn=<DivBackward0>)\n",
      "epoch: 0 train loss: 8.240184605121613\n",
      "tensor([[ 0.8177, -0.2566, -0.7460,  ..., -0.8754,  0.4845, -0.5202],\n",
      "        [ 0.0248,  0.1355, -0.0047,  ..., -0.1260,  0.0199, -0.0787],\n",
      "        [ 1.1489, -0.5332, -1.0756,  ..., -1.2342,  0.7291, -0.7859],\n",
      "        ...,\n",
      "        [ 0.6712, -0.3013, -0.6028,  ..., -0.7719,  0.5349, -0.5688],\n",
      "        [ 0.5530, -0.2489, -0.4817,  ..., -0.6295,  0.4745, -0.5103],\n",
      "        [ 1.1575, -0.4976, -1.1004,  ..., -1.2589,  0.7165, -0.7731]])\n",
      "min_square_error tensor(2.3320, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.6783, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.1569, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1337, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.1235, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1393, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.1326, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1345, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2994, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1354, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2975, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1406, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2414, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1398, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2875, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1389, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2893, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1406, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2826, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1445, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3512, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1412, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2580, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1410, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3235, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1451, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2892, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1451, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2239, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1449, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2666, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1465, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3345, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1453, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2513, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1532, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2925, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1524, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2117, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1541, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2569, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1686, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2519, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1646, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2567, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1541, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2672, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1717, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3023, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1666, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2100, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1589, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2321, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1732, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3402, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1747, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3078, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1692, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2978, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1841, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3142, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1783, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2476, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1846, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2462, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1905, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2540, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1882, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2321, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1805, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2685, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1911, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2742, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1872, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2286, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1933, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3471, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1957, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3376, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2023, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3450, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2043, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4099, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.1993, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_square_error tensor(2.3821, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2147, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2925, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2275, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3687, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2168, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2501, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2277, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3431, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2216, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.1741, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2313, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3547, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2172, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3506, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2396, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3032, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2388, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2873, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2601, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2623, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2703, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2587, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2603, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4138, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2708, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3425, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2945, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2920, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.2984, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3091, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.3543, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3772, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.3434, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4705, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.3543, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4295, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.3534, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3723, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.3581, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4011, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.3730, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5221, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.3748, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3744, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.3644, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4419, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.4138, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4469, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.4365, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4926, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.4391, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4313, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.4951, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3390, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.4946, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3753, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.4953, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4036, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.5071, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3033, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.5101, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3897, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.5013, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2282, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.5101, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3323, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.5073, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3234, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.5132, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5077, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.6090, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.7525, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.5665, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.8719, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.6275, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5383, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.6633, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6497, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.6870, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5090, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.7493, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.7936, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.7671, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5269, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.8037, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.7055, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.7830, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6916, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.8075, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6324, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.8263, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5018, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.8387, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6082, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(0.8281, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6918, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(1.2521, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4148, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(1.2891, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5408, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(1.4593, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6622, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(1.8855, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4314, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(1.8686, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4375, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(1.8035, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5410, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(2.0616, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4194, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(2.4558, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5774, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(3.0582, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6072, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(3.8814, grad_fn=<DivBackward0>)\n",
      "epoch: 1 train loss: 2.878612780570984\n",
      "tensor([[  0.8621,  -0.4026,  -0.6729,  ...,  -0.8345,   0.5512,  -0.4067],\n",
      "        [ 29.2070, -28.7912, -23.7741,  ..., -13.8154, -13.4588,  24.0020],\n",
      "        [ 14.6834, -14.3703, -11.0467,  ...,  -7.8220,  -5.6068,  10.6015],\n",
      "        ...,\n",
      "        [ 20.6026, -20.3297, -16.3648,  ..., -10.4286,  -8.7826,  16.1475],\n",
      "        [ 33.8261, -32.9401, -27.9898,  ..., -13.7273, -17.4496,  29.3034],\n",
      "        [ 44.9629, -43.4707, -37.4351,  ..., -16.8516, -23.9880,  39.5299]])\n",
      "min_square_error tensor(2.6650, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(216.4067, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2888, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(5.7695, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2965, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(5.8534, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.2133, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(6.6197, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6710, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(6.3963, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6266, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(7.4673, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5288, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(9.5183, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5562, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(7.1024, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4858, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(6.8259, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4858, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(9.5514, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_square_error tensor(2.6133, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(10.8502, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4342, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(10.7777, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5046, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(11.1223, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4381, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(9.3074, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4122, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(12.3266, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4521, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(10.0413, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5452, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(11.6350, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3856, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(15.5933, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4438, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(15.6608, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4659, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(15.3149, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5611, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(17.1987, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4208, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(14.9557, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4840, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(17.8160, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5230, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(17.6135, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5087, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(18.8330, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3408, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(19.9612, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3497, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(19.5006, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6254, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(20.0074, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5162, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(19.0686, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4364, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(19.1508, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.6456, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(24.7419, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.3761, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(20.5870, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.5222, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(26.7719, grad_fn=<DivBackward0>)\n",
      "min_square_error tensor(2.4857, grad_fn=<DivBackward0>)\n",
      "ave_cluster_error tensor(22.2835, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop starts\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Initializing variable for storing\n",
    "    # loss\n",
    "    running_loss = 0\n",
    "    \n",
    "    # Iterating over the training dataset\n",
    "    batch_num_per_epoch = int(np.ceil(n_train / batch_size))\n",
    "#     print(batch_num_per_epoch)\n",
    "\n",
    "    hidden_feature = model.encoder(xt_tensor)\n",
    "    for i_batch in range(batch_num_per_epoch):\n",
    "\n",
    "        start_id = i_batch * batch_size\n",
    "        end_id = (i_batch + 1) * batch_size\n",
    "        \n",
    "        # WENQI: NOTE That the input is mapped to 0~1 -> such that we use sigmod to get the output value\n",
    "        sift_in_batch = xt_tensor[start_id: end_id]\n",
    "#         print('in', sift_in_batch.shape, sift_in_batch[0])\n",
    "\n",
    "        # Generating output\n",
    "        out_batch = model(sift_in_batch)\n",
    "        hidden_feature_batch = model.encoder(sift_in_batch)\n",
    "        hidden_feature[start_id: end_id] = hidden_feature_batch\n",
    "#         print('out', out.shape, out[0])\n",
    "\n",
    "        # Calculating loss\n",
    "        loss = loss_func(out_batch, sift_in_batch, hidden_feature_batch, centroid_vectors)\n",
    "\n",
    "        # Updating weights according\n",
    "        # to the calculated loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Incrementing loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # update centroids after a mini-batch\n",
    "        kmeans.fit(hidden_feature.detach().numpy())\n",
    "        centroid_vectors = torch.tensor(kmeans.cluster_centers_)\n",
    "\n",
    "    # Averaging out loss over entire batch\n",
    "    running_loss /= batch_size\n",
    "    train_loss.append(running_loss)\n",
    "    \n",
    "#     out_val = model(x_val)\n",
    "#     out_hidden = \n",
    "#     loss_val = loss_func(out_val, x_val)\n",
    "#     validation_loss.append(loss_val)\n",
    "    \n",
    "    print(\"epoch:\", epoch, \"train loss:\", running_loss)\n",
    "    print(centroid_vectors)\n",
    "#     print(\"epoch:\", epoch, \"train loss:\", running_loss, \"val loss: \", loss_val.item())\n",
    "\n",
    "    # Storing useful images and\n",
    "    # reconstructed outputs for the last batch\n",
    "#     outputs[epoch+1] = {'in': sift_in_batch, 'out': out}\n",
    "    \n",
    "    # update centroids after an epoch\n",
    "#     xt_closest_partition_id, centroid_vectors = update_centroids_info(\n",
    "#         hidden_feature, xt_closest_partition_id, centroid_vectors)\n",
    "#     kmeans.fit(hidden_feature.detach().numpy())\n",
    "#     centroid_vectors = torch.tensor(kmeans.cluster_centers_)\n",
    "    \n",
    "\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.plot(range(1,num_epochs+1),train_loss)\n",
    "# plt.plot(range(1,num_epochs+1),validation_loss)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ab8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee011f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop starts ===》 WITHOUT BATCH LOOP ===> HUGE BATCH\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Initializing variable for storing\n",
    "    # loss\n",
    "    running_loss = 0\n",
    "        \n",
    "    # WENQI: NOTE That the input is mapped to 0~1 -> such that we use sigmod to get the output value\n",
    "    sift_in_batch = xt_tensor\n",
    "#         print('in', sift_in_batch.shape, sift_in_batch[0])\n",
    "\n",
    "    # Generating output\n",
    "    out_batch = model(sift_in_batch)\n",
    "    hidden_feature = model.encoder(sift_in_batch)\n",
    "\n",
    "    # Calculating loss\n",
    "    print('out_batch', out_batch.shape, out_batch)\n",
    "    print('sift_in_batch', sift_in_batch.shape, sift_in_batch)\n",
    "    print('hidden_feature', hidden_feature.shape, hidden_feature)\n",
    "    print('centroid_vectors', centroid_vectors.shape, centroid_vectors)\n",
    "    \n",
    "    loss = loss_func(out_batch, xt_tensor, hidden_feature, centroid_vectors)\n",
    "    \n",
    "    if torch.isnan(loss):\n",
    "        print('NaN loss, break...')\n",
    "        break\n",
    "    # Updating weights according\n",
    "    # to the calculated loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    clip_gradient = 1\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_gradient) # clip gradieent\n",
    "    optimizer.step()\n",
    "\n",
    "    # Incrementing loss\n",
    "    running_loss += loss.item()\n",
    "        \n",
    "    train_loss.append(running_loss)\n",
    "    \n",
    "#     out_val = model(x_val)\n",
    "#     out_hidden = \n",
    "#     loss_val = loss_func(out_val, x_val)\n",
    "#     validation_loss.append(loss_val)\n",
    "    \n",
    "    print(\"epoch:\", epoch, \"train loss:\", running_loss)\n",
    "#     print(\"epoch:\", epoch, \"train loss:\", running_loss, \"val loss: \", loss_val.item())\n",
    "\n",
    "    # Storing useful images and\n",
    "    # reconstructed outputs for the last batch\n",
    "    outputs[epoch+1] = {'in': sift_in_batch, 'out': out_batch}\n",
    "    \n",
    "    # update centroids after an epoch\n",
    "#     xt_closest_partition_id, centroid_vectors = update_centroids_info(\n",
    "#         hidden_feature, xt_closest_partition_id, centroid_vectors)\n",
    "    kmeans.fit(hidden_feature.detach().numpy())\n",
    "    centroid_vectors = torch.tensor(kmeans.cluster_centers_)\n",
    "    \n",
    "\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.plot(range(1,num_epochs+1),train_loss)\n",
    "# plt.plot(range(1,num_epochs+1),validation_loss)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4aae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the result of a single vector\n",
    "\n",
    "i_batch = 0\n",
    "\n",
    "# Loading image(s) and\n",
    "# reshaping it into a 1-d vector\n",
    "sift_in_batch = torch.FloatTensor(xt[i_batch * batch_size: (i_batch + 1) * batch_size])\n",
    "print('in', sift_in_batch.shape, sift_in_batch[0])\n",
    "\n",
    "# Generating output\n",
    "out = model(sift_in_batch)\n",
    "print('out', out.shape, out[0])\n",
    "    \n",
    "for i in range(128):\n",
    "    print('in: ', sift_in_batch[0][i], 'out: ', out[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7793180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sift_in_batch / 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863834e6",
   "metadata": {},
   "source": [
    "## K means on hidden feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cffcf614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f43e7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32)\n",
      "[  75.355736   -70.56735    -60.421646   -16.449442   -29.355614\n",
      "  -85.70103    -42.211414   -48.778187    35.060528    20.643835\n",
      "   50.602898   -40.27656   -104.86069     82.05809    -13.047523\n",
      "  -48.313778   -27.030766    -3.285881   -36.619286   -16.039133\n",
      "  -59.87265    -10.0474615   95.26721     27.586655   -78.94651\n",
      "  -30.944077   -92.002266   -17.261911   -45.702194   -36.359085\n",
      "  -30.051886    59.703403 ]\n"
     ]
    }
   ],
   "source": [
    "hidden_features_10K = model.encoder(torch.FloatTensor(xt)).detach().numpy()\n",
    "print(hidden_features_10K.shape)\n",
    "print(hidden_features_10K[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a95c0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=32)\n",
    "kmeans.fit(hidden_features_10K)\n",
    "y_kmeans = kmeans.predict(hidden_features_10K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2ff4740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 21 16 ...  2 19 16]\n"
     ]
    }
   ],
   "source": [
    "print(y_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fdec3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 21 16 ... 14  6 20]\n",
      "(1000000,)\n"
     ]
    }
   ],
   "source": [
    "hidden_feature_1M = model.encoder(torch.FloatTensor(xb)).detach().numpy()\n",
    "\n",
    "partition_IDs = kmeans.predict(hidden_feature_1M)\n",
    "print(partition_IDs)\n",
    "print(partition_IDs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "554d366d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items in partition  0 49766 average = 31250\n",
      "items in partition  1 25322 average = 31250\n",
      "items in partition  2 38218 average = 31250\n",
      "items in partition  3 45423 average = 31250\n",
      "items in partition  4 14486 average = 31250\n",
      "items in partition  5 33688 average = 31250\n",
      "items in partition  6 46448 average = 31250\n",
      "items in partition  7 12505 average = 31250\n",
      "items in partition  8 10576 average = 31250\n",
      "items in partition  9 41493 average = 31250\n",
      "items in partition  10 24963 average = 31250\n",
      "items in partition  11 49621 average = 31250\n",
      "items in partition  12 39266 average = 31250\n",
      "items in partition  13 29643 average = 31250\n",
      "items in partition  14 17298 average = 31250\n",
      "items in partition  15 47384 average = 31250\n",
      "items in partition  16 32738 average = 31250\n",
      "items in partition  17 4790 average = 31250\n",
      "items in partition  18 33463 average = 31250\n",
      "items in partition  19 49141 average = 31250\n",
      "items in partition  20 12246 average = 31250\n",
      "items in partition  21 17563 average = 31250\n",
      "items in partition  22 43524 average = 31250\n",
      "items in partition  23 7063 average = 31250\n",
      "items in partition  24 47187 average = 31250\n",
      "items in partition  25 27232 average = 31250\n",
      "items in partition  26 50036 average = 31250\n",
      "items in partition  27 41713 average = 31250\n",
      "items in partition  28 8512 average = 31250\n",
      "items in partition  29 42801 average = 31250\n",
      "items in partition  30 34324 average = 31250\n",
      "items in partition  31 21567 average = 31250\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping: partition ID -> {list of vector IDs}\n",
    "\n",
    "num_partition = 32\n",
    "\n",
    "partition_id_vec_id_list_1M = dict()\n",
    "for i in range(num_partition):\n",
    "    partition_id_vec_id_list_1M[i] = []\n",
    "\n",
    "\n",
    "for i in range(int(1e6)):\n",
    "    partition_ID = int(partition_IDs[i])\n",
    "    partition_id_vec_id_list_1M[partition_ID].append(i)\n",
    "    \n",
    "for i in range(num_partition):\n",
    "    print('items in partition ', i, len(partition_id_vec_id_list_1M[i]), 'average =', int(1e6/num_partition))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c886944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def scan_partition(query_vec, partition_id_list, vector_set):\n",
    "    \"\"\"\n",
    "    query_vec = (128, )\n",
    "    partition_id_list = (N_num_vec, )\n",
    "    vector_set = 1M dataset (1M, 128)\n",
    "    \"\"\"\n",
    "    min_dist = 1e10\n",
    "    min_dist_ID = None\n",
    "    for vec_id in partition_id_list:\n",
    "        dataset_vec = vector_set[vec_id]\n",
    "        dist = np.linalg.norm(query_vec - dataset_vec)\n",
    "        if dist <= min_dist:\n",
    "            min_dist = dist\n",
    "            min_dist_ID = vec_id\n",
    "            \n",
    "    return min_dist_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b52c99b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32)\n",
      "0 133171\n",
      "1 790327\n",
      "2 502074\n",
      "3 814387\n",
      "4 921963\n",
      "5 175043\n",
      "6 843295\n",
      "7 848955\n",
      "8 76782\n",
      "9 413852\n",
      "10 878295\n",
      "11 742512\n",
      "12 696749\n",
      "13 462513\n",
      "14 839470\n",
      "15 372248\n",
      "16 423980\n",
      "17 862268\n",
      "18 602078\n",
      "19 947435\n",
      "20 68023\n",
      "21 334619\n",
      "22 712156\n",
      "23 804213\n",
      "24 272283\n",
      "25 297655\n",
      "26 387383\n",
      "27 221028\n",
      "28 95867\n",
      "29 460882\n",
      "30 125853\n",
      "31 397619\n",
      "32 16108\n",
      "33 61454\n",
      "34 656767\n",
      "35 538502\n",
      "36 942225\n",
      "37 252712\n",
      "38 452205\n",
      "39 176702\n",
      "40 709856\n",
      "41 79741\n",
      "42 137064\n",
      "43 845200\n",
      "44 77190\n",
      "45 469831\n",
      "46 302575\n",
      "47 521108\n",
      "48 468406\n",
      "49 489446\n",
      "50 849174\n",
      "51 802781\n",
      "52 365999\n",
      "53 441473\n",
      "54 226422\n",
      "55 120806\n",
      "56 480173\n",
      "57 628861\n",
      "58 408532\n",
      "59 951575\n",
      "60 859542\n",
      "61 833563\n",
      "62 437031\n",
      "63 331243\n",
      "64 121265\n",
      "65 715285\n",
      "66 827936\n",
      "67 850184\n",
      "68 60810\n",
      "69 847531\n",
      "70 761548\n",
      "71 452789\n",
      "72 5106\n",
      "73 556801\n",
      "74 178558\n",
      "75 518522\n",
      "76 712139\n",
      "77 781746\n",
      "78 680004\n",
      "79 95340\n",
      "80 48350\n",
      "81 380662\n",
      "82 16235\n",
      "83 254697\n",
      "84 594097\n",
      "85 53667\n",
      "86 419880\n",
      "87 635731\n",
      "88 51333\n",
      "89 407749\n",
      "90 640347\n",
      "91 972183\n",
      "92 190435\n",
      "93 978575\n",
      "94 147900\n",
      "95 171630\n",
      "96 242602\n",
      "97 763985\n",
      "98 842667\n",
      "99 767376\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors = []\n",
    "\n",
    "N = 100\n",
    "#### Wenqi: here had a bug: previously xb, now xq\n",
    "query_hidden_feature = model.encoder(torch.FloatTensor(xq)).detach().numpy()\n",
    "print(query_hidden_feature.shape)\n",
    "query_partition = kmeans.predict(query_hidden_feature)\n",
    "\n",
    "for i in range(N):\n",
    "    partition_id = int(query_partition[i])\n",
    "    nearest_neighbor_ID = scan_partition(xq[i], partition_id_vec_id_list_1M[partition_id], xb)\n",
    "    nearest_neighbors.append(nearest_neighbor_ID)\n",
    "    print(i, nearest_neighbor_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bdecc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[504814]\n",
      " [588616]\n",
      " [552515]\n",
      " [335355]\n",
      " [482427]\n",
      " [508403]\n",
      " [167240]\n",
      " [327960]\n",
      " [834657]\n",
      " [592948]\n",
      " [878295]\n",
      " [771023]\n",
      " [215771]\n",
      " [717949]\n",
      " [368047]\n",
      " [776345]\n",
      " [373550]\n",
      " [862239]\n",
      " [602078]\n",
      " [ 84644]\n",
      " [ 68023]\n",
      " [671173]\n",
      " [ 47363]\n",
      " [258880]\n",
      " [698614]\n",
      " [838692]\n",
      " [922290]\n",
      " [221028]\n",
      " [962851]\n",
      " [785288]\n",
      " [425493]\n",
      " [407192]\n",
      " [229032]\n",
      " [909787]\n",
      " [303455]\n",
      " [825435]\n",
      " [602485]\n",
      " [812777]\n",
      " [341091]\n",
      " [856200]\n",
      " [982373]\n",
      " [499763]\n",
      " [915089]\n",
      " [ 43368]\n",
      " [640127]\n",
      " [858815]\n",
      " [436180]\n",
      " [283703]\n",
      " [ 28097]\n",
      " [426503]\n",
      " [849174]\n",
      " [467756]\n",
      " [803688]\n",
      " [ 74183]\n",
      " [819365]\n",
      " [882827]\n",
      " [535502]\n",
      " [779586]\n",
      " [702690]\n",
      " [ 12747]\n",
      " [534495]\n",
      " [895845]\n",
      " [795958]\n",
      " [ 45494]\n",
      " [943477]\n",
      " [809566]\n",
      " [  3367]\n",
      " [850184]\n",
      " [185761]\n",
      " [434096]\n",
      " [116383]\n",
      " [753756]\n",
      " [222336]\n",
      " [787309]\n",
      " [436415]\n",
      " [629700]\n",
      " [470987]\n",
      " [717995]\n",
      " [680004]\n",
      " [694111]\n",
      " [659382]\n",
      " [380662]\n",
      " [555466]\n",
      " [ 34810]\n",
      " [632431]\n",
      " [853398]\n",
      " [299980]\n",
      " [138747]\n",
      " [698591]\n",
      " [158669]\n",
      " [ 32380]\n",
      " [490742]\n",
      " [190435]\n",
      " [617987]\n",
      " [292659]\n",
      " [330662]\n",
      " [898436]\n",
      " [763985]\n",
      " [532460]\n",
      " [ 72670]]\n"
     ]
    }
   ],
   "source": [
    "print(gt[:100, :1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb1cea9",
   "metadata": {},
   "source": [
    "First 100 queries: recall@1 =  0.69\n",
    "First 1000 queries: recall@1 =  0.636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a162b32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 recall@1 =  0.1\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "for i in range(N):\n",
    "    if nearest_neighbors[i] == gt[i][0]:\n",
    "        correct_count += 1\n",
    "        \n",
    "print(correct_count, 'recall@1 = ', correct_count / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8142b7",
   "metadata": {},
   "source": [
    "## Wenqi: naive auto-encoder is basically a dimensionality reduction technique\n",
    "\n",
    "It cannot improve the k-means quality, because the clustering with the full dimensions should perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cfe81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
