{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36833915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 15, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03b49c",
   "metadata": {},
   "source": [
    "## Load SIFT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c089305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmap_fvecs(fname):\n",
    "    x = np.memmap(fname, dtype='int32', mode='r')\n",
    "    d = x[0]\n",
    "    return x.view('float32').reshape(-1, d + 1)[:, 1:]\n",
    "\n",
    "def mmap_bvecs(fname):\n",
    "    x = np.memmap(fname, dtype='uint8', mode='r')\n",
    "    d = x[:4].view('int32')[0]\n",
    "    return x.reshape(-1, d + 4)[:, 4:]\n",
    "\n",
    "def ivecs_read(fname):\n",
    "    a = np.fromfile(fname, dtype='int32')\n",
    "    d = a[0]\n",
    "    # Wenqi: Format of ground truth (for 10000 query vectors):\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #        ...     ...\n",
    "    #   1000(topK), [1000 ids]\n",
    "    # 10000 rows in total, 10000 * 1001 elements, 10000 * 1001 * 4 bytes\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy()\n",
    "\n",
    "def fvecs_read(fname):\n",
    "    return ivecs_read(fname).view('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4331178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shapes:\n",
      "Base vector xb:  (1000000, 128)\n",
      "Query vector xq:  (10000, 128)\n",
      "Ground truth gt:  (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "dbname = 'SIFT1M'\n",
    "\n",
    "if dbname.startswith('SIFT'):\n",
    "    # SIFT1M to SIFT1000M\n",
    "    dbsize = int(dbname[4:-1])\n",
    "    xb = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_base.bvecs')\n",
    "    xq = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_query.bvecs')\n",
    "    gt = ivecs_read('/mnt/scratch/wenqi/Faiss_experiments/bigann/gnd/idx_%dM.ivecs' % dbsize)\n",
    "\n",
    "    N_VEC = int(dbsize * 1000 * 1000)\n",
    "\n",
    "    # trim xb to correct size\n",
    "    xb = xb[:dbsize * 1000 * 1000]\n",
    "\n",
    "    # Wenqi: load xq to main memory and reshape\n",
    "    xq = xq.astype('float32').copy()\n",
    "#     xq = np.array(xq, dtype=np.float32)\n",
    "    xb = xb.astype('float32').copy()\n",
    "    gt = np.array(gt, dtype=np.int32)\n",
    "\n",
    "    print(\"Vector shapes:\")\n",
    "    print(\"Base vector xb: \", xb.shape)\n",
    "    print(\"Query vector xq: \", xq.shape)\n",
    "    print(\"Ground truth gt: \", gt.shape)\n",
    "else:\n",
    "    print('unknown dataset', dbname, file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "dim = xb.shape[1] # should be 128\n",
    "nq = xq.shape[0]\n",
    "\n",
    "# Normalize all to 0~1\n",
    "xb = xb / 256\n",
    "xq = xq / 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "635de4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Using the first 10K vectors for training, just as the graph & IVF experiments\n",
    "n_train = int(1e4)\n",
    "xt = xb[:n_train]\n",
    "print(xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d6306e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.00390625, 0.03125   ,\n",
       "       0.02734375, 0.01171875, 0.0078125 , 0.01953125, 0.        ,\n",
       "       0.        , 0.01171875, 0.01953125, 0.02734375, 0.04296875,\n",
       "       0.12109375, 0.05078125, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11328125, 0.4140625 , 0.41796875, 0.05078125,\n",
       "       0.        , 0.        , 0.        , 0.00390625, 0.23828125,\n",
       "       0.2734375 , 0.1640625 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00390625, 0.08984375, 0.109375  , 0.0625    ,\n",
       "       0.24609375, 0.015625  , 0.        , 0.        , 0.        ,\n",
       "       0.0234375 , 0.32421875, 0.31640625, 0.45703125, 0.3359375 ,\n",
       "       0.09765625, 0.05859375, 0.06640625, 0.1953125 , 0.328125  ,\n",
       "       0.45703125, 0.12109375, 0.08984375, 0.0703125 , 0.13671875,\n",
       "       0.37890625, 0.45703125, 0.19140625, 0.09375   , 0.265625  ,\n",
       "       0.10546875, 0.        , 0.        , 0.        , 0.015625  ,\n",
       "       0.11328125, 0.27734375, 0.31640625, 0.18359375, 0.05078125,\n",
       "       0.0390625 , 0.125     , 0.33984375, 0.45703125, 0.45703125,\n",
       "       0.17578125, 0.296875  , 0.15625   , 0.0859375 , 0.234375  ,\n",
       "       0.2734375 , 0.16015625, 0.03515625, 0.02734375, 0.08203125,\n",
       "       0.11328125, 0.15234375, 0.20703125, 0.08203125, 0.015625  ,\n",
       "       0.00390625, 0.21484375, 0.28125   , 0.01171875, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03515625, 0.25390625,\n",
       "       0.45703125, 0.28515625, 0.14453125, 0.109375  , 0.08984375,\n",
       "       0.06640625, 0.1328125 , 0.04296875, 0.04296875, 0.10546875,\n",
       "       0.23828125, 0.25      , 0.09765625, 0.015625  , 0.        ,\n",
       "       0.1640625 , 0.05078125, 0.00390625, 0.00390625, 0.00390625,\n",
       "       0.0546875 , 0.0390625 , 0.0234375 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8003379",
   "metadata": {},
   "source": [
    "## Model Declaration\n",
    "\n",
    "Maybe I need to get rid of the sigmoid function, because there are a lot of 0s and 255s, and using sigmoid to get those values are very hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce44512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DeepAutoencoder class\n",
    "\n",
    "# We partition the data to 32 shards (bottleneck dimension)\n",
    "# The input data is 128-dimension\n",
    "class DeepAutoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "#             torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Instantiating the model and hyperparameters\n",
    "model = DeepAutoencoder()\n",
    "# loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7116796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_func(reconstructed_feat, input_feat, hidden_feat, centroid_vectors, partition_ids):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     \"\"\"\n",
    "#     full_dim = reconstructed_feat.shape[1]\n",
    "#     hidden_dim = hidden_feat.shape[1]\n",
    "#     hidden_feat_factor = full_dim / hidden_dim\n",
    "    \n",
    "#     batch_size = reconstructed_feat.shape[0]\n",
    "# #     assert batch_size == input_feat.shape[0] and batch_size == hidden_feat.shape[0] and \\\n",
    "# #         batch_size == partition_ids.shape[0]]\n",
    "# #     assert hidden_dim == centroid_vectors.shape[1]\n",
    "\n",
    "#     min_square_error = (reconstructed_feat - input_feat)**2\n",
    "#     cluster_error = 0\n",
    "#     for i in range(hidden_feat.shape[0]):\n",
    "#         cluster_error += torch.sum((hidden_feat[i] - centroid_vectors[partition_ids[i]]) ** 2)\n",
    "#     ave_cluster_error = hidden_feat_factor * cluster_error / batch_size / hidden_dim\n",
    "#     loss = torch.mean(min_square_error) + ave_cluster_error\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44cf8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(reconstructed_feat, input_feat, hidden_feat, centroid_vectors, partition_id_list):\n",
    "    \"\"\"\n",
    "    centroid vectors: same shape as hidden_feat\n",
    "    \"\"\"\n",
    "    \n",
    "    full_dim = reconstructed_feat.shape[1]\n",
    "    hidden_dim = hidden_feat.shape[1]\n",
    "    hidden_feat_factor = full_dim / hidden_dim\n",
    "    \n",
    "    min_square_error = (reconstructed_feat - input_feat)**2\n",
    "    \n",
    "    cluster_error = 0\n",
    "    for i in range(hidden_feat.shape[0]):\n",
    "        cluster_error += torch.sum((hidden_feat[i] - centroid_vectors[partition_id_list[i]]) ** 2)\n",
    "        \n",
    "    ave_cluster_error = hidden_feat_factor * cluster_error / batch_size / hidden_dim\n",
    "    loss = torch.mean(min_square_error) + ave_cluster_error\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a47cf5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DeepAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=32, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56fb80",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066566c",
   "metadata": {},
   "source": [
    "Model 1: no sigmoid in the middle, with sigmoid in the end; 3 layers\n",
    "Init loss (sigmoid) = 0.06 (given batch size = 100, ~600)\n",
    "loss after 100 epoch = 0.005 (~10% init)\n",
    "loss after 1100 epoch = 0.00236 (~4% init)\n",
    "\n",
    "Model 2: no sigmoid in the middle, no sigmoid in the end; 4 layers\n",
    "Init loss = 492\n",
    "loss after 100 epoch = 221 (~50% init)\n",
    "loss after 600 epoch = 161\n",
    "loss after 1100 epoch = 153 (switching batch size to 1000): \n",
    "\n",
    "Model 3: no sigmoid in the middle, with sigmoid in the end; 4 layers\n",
    "Init loss (sigmoid) = 8\n",
    "loss after 100 epoch = 0.500 (~10% init)\n",
    "loss after 1100 epoch = 0.260 (~3% init)\n",
    "\n",
    "Model 3: with sigmoid in the middle, with sigmoid in the end; 4 layers\n",
    "Init loss (sigmoid) = 5.4\n",
    "loss after 100 epoch = 0.57 (~10% init)\n",
    "loss after 1000 epoch = 0.29 (~3% init)\n",
    "\n",
    "\n",
    "Model 3: no sigmoid in the middle, no sigmoid in the end; 5 layers\n",
    "Init loss (sigmoid) = 1.0\n",
    "loss after 100 epoch = xx (~10% init)\n",
    "loss after 1000 epoch = xx (~3% init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf86b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid_vectors(hidden_feature, partition_id_list):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "      - hidden_feature (torch tensor): (num_vectors, num_partitions)\n",
    "      - ID list (array): size = vector number; vec ID -> partition ID\n",
    "    \n",
    "    Output (updated):\n",
    "      - centroids info (torch tensor): (num_partitions, num_partitions) \n",
    "          first dim: different centroid IDs\n",
    "          second dim: an entire centroid vector\n",
    "    \"\"\"\n",
    "    num_vectors = len(partition_id_list)\n",
    "    num_partition = hidden_feature.shape[1]\n",
    "    \n",
    "    num_vectors_per_partition = np.zeros(num_partition)\n",
    "    new_centroid_vectors_sum = torch.zeros(num_partitions, num_partitions) \n",
    "    for vec_id in range(num_vectors):\n",
    "        partition_ID = partition_id_list[vec_id]\n",
    "        num_vectors_per_partition[partition_ID] += 1\n",
    "        new_centroid_vectors_sum[partition_ID] += hidden_feature[vec_id]\n",
    "        \n",
    "    new_centroid_vectors = torch.zeros(num_partitions, num_partitions) \n",
    "    for partition_id in range(num_partition):\n",
    "        new_centroid_vectors[partition_id] = \\\n",
    "            new_centroid_vectors_sum[partition_id] / num_vectors_per_partition[partition_id] \n",
    "    \n",
    "    return new_centroid_vectors\n",
    "\n",
    "def update_centroids_info(hidden_feature, partition_id_list, centroid_vectors):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "      - hidden_feature (torch tensor): (num_vectors, num_partitions)\n",
    "      - ID list (array): size = vector number; vec ID -> partition ID\n",
    "      - centroids info (torch tensor): (num_partitions, num_partitions) \n",
    "          first dim: different centroid IDs\n",
    "          second dim: an entire centroid vector\n",
    "    \n",
    "    Output (updated):\n",
    "      - ID list (array): vec ID -> partition ID\n",
    "      - centroids info (torch tensor): (num_partitions, num_partitions) \n",
    "          first dim: different centroid IDs\n",
    "          second dim: an entire centroid vector\n",
    "    \"\"\"\n",
    "    \n",
    "    num_vectors = len(partition_id_list)\n",
    "    num_partition = centroid_vectors.shape[0]\n",
    "    \n",
    "    num_vectors_per_partition = np.zeros(num_partition)\n",
    "    new_centroid_vectors_sum = torch.zeros(num_partitions, num_partitions) \n",
    "    for vec_id in range(num_vectors):\n",
    "        partition_ID = partition_id_list[vec_id]\n",
    "        num_vectors_per_partition[partition_ID] += 1\n",
    "        new_centroid_vectors_sum[partition_ID] += hidden_feature[vec_id]\n",
    "        \n",
    "    new_centroid_vectors = torch.zeros(num_partitions, num_partitions) \n",
    "    for partition_id in range(num_partition):\n",
    "        new_centroid_vectors[partition_id] = \\\n",
    "            new_centroid_vectors_sum[partition_id] / num_vectors_per_partition[partition_id]\n",
    "        \n",
    "    new_L2_dist = torch.zeros((num_partition, num_vectors))\n",
    "    for paritition_id in range(num_partition):\n",
    "        # L2 distance to the i th centroid, output size = num_vec\n",
    "        new_L2_dist[paritition_id] = torch.sum((hidden_feature - centroid_vectors[paritition_id]) ** 2, dim=1)\n",
    "    new_partition_id_list = torch.argmin(new_L2_dist, dim = 0)\n",
    "    \n",
    "    return new_partition_id_list, new_centroid_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc28c3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition ID list [19 30  9 ... 31 12  0]\n",
      "centroid vectors tensor([[ 0.0381,  0.0966, -0.0289,  ...,  0.0128,  0.0071, -0.0853],\n",
      "        [ 0.0386,  0.0976, -0.0284,  ...,  0.0128,  0.0073, -0.0852],\n",
      "        [ 0.0381,  0.0966, -0.0287,  ...,  0.0126,  0.0069, -0.0854],\n",
      "        ...,\n",
      "        [ 0.0381,  0.0964, -0.0283,  ...,  0.0137,  0.0066, -0.0852],\n",
      "        [ 0.0382,  0.0965, -0.0287,  ...,  0.0126,  0.0067, -0.0853],\n",
      "        [ 0.0387,  0.0962, -0.0295,  ...,  0.0131,  0.0061, -0.0847]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000 # Wenqi: original iteration = 100\n",
    "batch_size = 100\n",
    "num_train_vectors = int(1e4)\n",
    "num_partitions = 32\n",
    "\n",
    "# List that will store the training loss\n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Dictionary that will store the\n",
    "# different images and outputs for\n",
    "# various epochs\n",
    "outputs = {}\n",
    "x_val = torch.FloatTensor(xb[num_train_vectors: 2 * num_train_vectors])\n",
    "xt_tensor = torch.FloatTensor(xt)\n",
    "\n",
    "hidden_feature = model.encoder(xt_tensor)\n",
    "# Initiation: each vector starts with a random partition ID; \n",
    "#   each partition has a center vector\n",
    "xt_closest_partition_id = np.random.randint(0, high=num_partitions, size=num_train_vectors)\n",
    "centroid_vectors = get_centroid_vectors(hidden_feature, xt_closest_partition_id)\n",
    "\n",
    "print('partition ID list', xt_closest_partition_id)\n",
    "print('centroid vectors', centroid_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5930b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0381,  0.0966, -0.0289,  ...,  0.0128,  0.0071, -0.0853],\n",
       "        [ 0.0386,  0.0976, -0.0284,  ...,  0.0128,  0.0073, -0.0852],\n",
       "        [ 0.0381,  0.0966, -0.0287,  ...,  0.0126,  0.0069, -0.0854],\n",
       "        ...,\n",
       "        [ 0.0381,  0.0964, -0.0283,  ...,  0.0137,  0.0066, -0.0852],\n",
       "        [ 0.0382,  0.0965, -0.0287,  ...,  0.0126,  0.0067, -0.0853],\n",
       "        [ 0.0387,  0.0962, -0.0295,  ...,  0.0131,  0.0061, -0.0847]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7034b76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition ID list tensor([19, 16, 11,  ...,  2, 28, 10])\n",
      "centroid vectors tensor([[ 0.0381,  0.0966, -0.0289,  ...,  0.0128,  0.0071, -0.0853],\n",
      "        [ 0.0386,  0.0976, -0.0284,  ...,  0.0128,  0.0073, -0.0852],\n",
      "        [ 0.0381,  0.0966, -0.0287,  ...,  0.0126,  0.0069, -0.0854],\n",
      "        ...,\n",
      "        [ 0.0381,  0.0964, -0.0283,  ...,  0.0137,  0.0066, -0.0852],\n",
      "        [ 0.0382,  0.0965, -0.0287,  ...,  0.0126,  0.0067, -0.0853],\n",
      "        [ 0.0387,  0.0962, -0.0295,  ...,  0.0131,  0.0061, -0.0847]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "xt_closest_partition_id, centroid_vectors = update_centroids_info(hidden_feature, xt_closest_partition_id, centroid_vectors)\n",
    "print('partition ID list', xt_closest_partition_id)\n",
    "print('centroid vectors', centroid_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "951fe8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128])\n",
      "tensor(0.1074) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "sift_in_batch = xt_tensor[0 * batch_size: (0 + 1) * batch_size]\n",
    "print((sift_in_batch ** 2).shape)\n",
    "print(torch.mean(sift_in_batch), torch.mean(sift_in_batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95f8e939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_batch torch.Size([10000, 128]) tensor([[0.5130, 0.4774, 0.4915,  ..., 0.4823, 0.5457, 0.4795],\n",
      "        [0.5133, 0.4775, 0.4920,  ..., 0.4825, 0.5455, 0.4796],\n",
      "        [0.5130, 0.4772, 0.4914,  ..., 0.4824, 0.5460, 0.4791],\n",
      "        ...,\n",
      "        [0.5130, 0.4776, 0.4917,  ..., 0.4825, 0.5455, 0.4799],\n",
      "        [0.5128, 0.4776, 0.4916,  ..., 0.4823, 0.5454, 0.4801],\n",
      "        [0.5128, 0.4771, 0.4913,  ..., 0.4822, 0.5461, 0.4790]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "sift_in_batch torch.Size([10000, 128]) tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0547, 0.0391, 0.0234],\n",
      "        [0.2539, 0.1367, 0.0312,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0273, 0.0195, 0.0273,  ..., 0.1758, 0.0625, 0.0898],\n",
      "        [0.1484, 0.0000, 0.0000,  ..., 0.1055, 0.0156, 0.0312],\n",
      "        [0.0000, 0.0000, 0.0078,  ..., 0.1172, 0.0117, 0.0000]])\n",
      "hidden_feature torch.Size([10000, 32]) tensor([[ 0.0534,  0.0953, -0.0180,  ..., -0.0033,  0.0208, -0.0790],\n",
      "        [ 0.0357,  0.0836, -0.0283,  ...,  0.0150,  0.0004, -0.0815],\n",
      "        [ 0.0505,  0.0945, -0.0204,  ...,  0.0019,  0.0216, -0.0616],\n",
      "        ...,\n",
      "        [ 0.0426,  0.1020, -0.0228,  ...,  0.0102,  0.0053, -0.0787],\n",
      "        [ 0.0447,  0.1056, -0.0213,  ...,  0.0108,  0.0043, -0.0858],\n",
      "        [ 0.0359,  0.0989, -0.0198,  ...,  0.0126,  0.0291, -0.0564]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "centroid_vectors torch.Size([32, 32]) tensor([[ 0.0381,  0.0966, -0.0289,  ...,  0.0128,  0.0071, -0.0853],\n",
      "        [ 0.0386,  0.0976, -0.0284,  ...,  0.0128,  0.0073, -0.0852],\n",
      "        [ 0.0381,  0.0966, -0.0287,  ...,  0.0126,  0.0069, -0.0854],\n",
      "        ...,\n",
      "        [ 0.0381,  0.0964, -0.0283,  ...,  0.0137,  0.0066, -0.0852],\n",
      "        [ 0.0382,  0.0965, -0.0287,  ...,  0.0126,  0.0067, -0.0853],\n",
      "        [ 0.0387,  0.0962, -0.0295,  ...,  0.0131,  0.0061, -0.0847]],\n",
      "       grad_fn=<CopySlices>)\n",
      "xt_closest_partition_id (10000,) [ 3  5 21 ... 27 25 18]\n",
      "epoch: 0 train loss: 0.21481658518314362\n",
      "out_batch torch.Size([10000, 128]) tensor([[0.5122, 0.4756, 0.4905,  ..., 0.4808, 0.5442, 0.4780],\n",
      "        [0.5125, 0.4757, 0.4911,  ..., 0.4810, 0.5439, 0.4781],\n",
      "        [0.5122, 0.4754, 0.4904,  ..., 0.4809, 0.5444, 0.4776],\n",
      "        ...,\n",
      "        [0.5121, 0.4758, 0.4907,  ..., 0.4810, 0.5440, 0.4784],\n",
      "        [0.5120, 0.4758, 0.4906,  ..., 0.4808, 0.5439, 0.4785],\n",
      "        [0.5120, 0.4754, 0.4903,  ..., 0.4807, 0.5444, 0.4776]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "sift_in_batch torch.Size([10000, 128]) tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0547, 0.0391, 0.0234],\n",
      "        [0.2539, 0.1367, 0.0312,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0273, 0.0195, 0.0273,  ..., 0.1758, 0.0625, 0.0898],\n",
      "        [0.1484, 0.0000, 0.0000,  ..., 0.1055, 0.0156, 0.0312],\n",
      "        [0.0000, 0.0000, 0.0078,  ..., 0.1172, 0.0117, 0.0000]])\n",
      "hidden_feature torch.Size([10000, 32]) tensor([[ 0.0464,  0.0920, -0.0221,  ..., -0.0003,  0.0203, -0.0741],\n",
      "        [ 0.0304,  0.0835, -0.0299,  ...,  0.0142,  0.0020, -0.0812],\n",
      "        [ 0.0436,  0.0916, -0.0242,  ...,  0.0050,  0.0214, -0.0594],\n",
      "        ...,\n",
      "        [ 0.0392,  0.0984, -0.0259,  ...,  0.0116,  0.0041, -0.0754],\n",
      "        [ 0.0374,  0.1022, -0.0265,  ...,  0.0137,  0.0045, -0.0802],\n",
      "        [ 0.0324,  0.0974, -0.0228,  ...,  0.0139,  0.0277, -0.0567]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "centroid_vectors torch.Size([32, 32]) tensor([[ 0.0388,  0.0961, -0.0287,  ...,  0.0129,  0.0056, -0.0851],\n",
      "        [ 0.0391,  0.0967, -0.0284,  ...,  0.0125,  0.0067, -0.0857],\n",
      "        [ 0.0388,  0.0955, -0.0290,  ...,  0.0132,  0.0065, -0.0840],\n",
      "        ...,\n",
      "        [ 0.0375,  0.0969, -0.0294,  ...,  0.0136,  0.0064, -0.0845],\n",
      "        [ 0.0373,  0.0966, -0.0293,  ...,  0.0143,  0.0063, -0.0851],\n",
      "        [ 0.0377,  0.0959, -0.0294,  ...,  0.0137,  0.0065, -0.0843]],\n",
      "       grad_fn=<CopySlices>)\n",
      "xt_closest_partition_id torch.Size([10000]) tensor([19, 16, 11,  ...,  2, 28, 10])\n",
      "epoch: 1 train loss: 0.20662736892700195\n",
      "out_batch torch.Size([10000, 128]) tensor([[0.5112, 0.4740, 0.4896,  ..., 0.4793, 0.5426, 0.4766],\n",
      "        [0.5115, 0.4742, 0.4901,  ..., 0.4795, 0.5424, 0.4767],\n",
      "        [0.5112, 0.4738, 0.4895,  ..., 0.4794, 0.5429, 0.4762],\n",
      "        ...,\n",
      "        [0.5112, 0.4742, 0.4898,  ..., 0.4795, 0.5425, 0.4769],\n",
      "        [0.5110, 0.4741, 0.4897,  ..., 0.4793, 0.5425, 0.4770],\n",
      "        [0.5111, 0.4738, 0.4894,  ..., 0.4792, 0.5428, 0.4762]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "sift_in_batch torch.Size([10000, 128]) tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0547, 0.0391, 0.0234],\n",
      "        [0.2539, 0.1367, 0.0312,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0273, 0.0195, 0.0273,  ..., 0.1758, 0.0625, 0.0898],\n",
      "        [0.1484, 0.0000, 0.0000,  ..., 0.1055, 0.0156, 0.0312],\n",
      "        [0.0000, 0.0000, 0.0078,  ..., 0.1172, 0.0117, 0.0000]])\n",
      "hidden_feature torch.Size([10000, 32]) tensor([[ 0.0397,  0.0890, -0.0258,  ...,  0.0029,  0.0201, -0.0706],\n",
      "        [ 0.0264,  0.0826, -0.0320,  ...,  0.0136,  0.0038, -0.0801],\n",
      "        [ 0.0372,  0.0891, -0.0276,  ...,  0.0084,  0.0215, -0.0584],\n",
      "        ...,\n",
      "        [ 0.0348,  0.0946, -0.0297,  ...,  0.0127,  0.0036, -0.0722],\n",
      "        [ 0.0305,  0.0996, -0.0316,  ...,  0.0163,  0.0052, -0.0760],\n",
      "        [ 0.0281,  0.0952, -0.0263,  ...,  0.0150,  0.0262, -0.0565]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "centroid_vectors torch.Size([32, 32]) tensor([[ 0.0376,  0.0906, -0.0290,  ...,  0.0075,  0.0149, -0.0819],\n",
      "        [ 0.0453,  0.0996, -0.0248,  ...,  0.0068,  0.0130, -0.0827],\n",
      "        [ 0.0410,  0.0930, -0.0252,  ...,  0.0057,  0.0112, -0.0902],\n",
      "        ...,\n",
      "        [ 0.0369,  0.0930, -0.0248,  ...,  0.0156,  0.0084, -0.0864],\n",
      "        [ 0.0332,  0.0944, -0.0303,  ...,  0.0082,  0.0062, -0.0830],\n",
      "        [ 0.0439,  0.0909, -0.0319,  ...,  0.0094,  0.0021, -0.0829]],\n",
      "       grad_fn=<CopySlices>)\n",
      "xt_closest_partition_id torch.Size([10000]) tensor([27, 17, 22,  ...,  1,  1, 27])\n",
      "epoch: 2 train loss: 0.21060271561145782\n",
      "out_batch torch.Size([10000, 128]) tensor([[0.5102, 0.4726, 0.4886,  ..., 0.4778, 0.5412, 0.4752],\n",
      "        [0.5105, 0.4728, 0.4890,  ..., 0.4780, 0.5410, 0.4755],\n",
      "        [0.5102, 0.4724, 0.4885,  ..., 0.4779, 0.5415, 0.4749],\n",
      "        ...,\n",
      "        [0.5102, 0.4727, 0.4888,  ..., 0.4780, 0.5411, 0.4755],\n",
      "        [0.5100, 0.4726, 0.4886,  ..., 0.4778, 0.5411, 0.4756],\n",
      "        [0.5101, 0.4724, 0.4884,  ..., 0.4778, 0.5414, 0.4750]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "sift_in_batch torch.Size([10000, 128]) tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0547, 0.0391, 0.0234],\n",
      "        [0.2539, 0.1367, 0.0312,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0273, 0.0195, 0.0273,  ..., 0.1758, 0.0625, 0.0898],\n",
      "        [0.1484, 0.0000, 0.0000,  ..., 0.1055, 0.0156, 0.0312],\n",
      "        [0.0000, 0.0000, 0.0078,  ..., 0.1172, 0.0117, 0.0000]])\n",
      "hidden_feature torch.Size([10000, 32]) tensor([[ 0.0341,  0.0862, -0.0295,  ...,  0.0063,  0.0194, -0.0686],\n",
      "        [ 0.0234,  0.0812, -0.0339,  ...,  0.0140,  0.0054, -0.0788],\n",
      "        [ 0.0321,  0.0869, -0.0306,  ...,  0.0119,  0.0212, -0.0582],\n",
      "        ...,\n",
      "        [ 0.0303,  0.0907, -0.0328,  ...,  0.0144,  0.0037, -0.0707],\n",
      "        [ 0.0254,  0.0964, -0.0355,  ...,  0.0181,  0.0055, -0.0722],\n",
      "        [ 0.0240,  0.0923, -0.0297,  ...,  0.0162,  0.0246, -0.0563]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "centroid_vectors torch.Size([32, 32]) tensor([[ 0.0354,  0.0913, -0.0347,  ...,  0.0135,  0.0005, -0.0800],\n",
      "        [ 0.0378,  0.0960, -0.0291,  ...,  0.0074,  0.0120, -0.0838],\n",
      "        [ 0.0372,  0.0878, -0.0300,  ...,  0.0109,  0.0108, -0.0760],\n",
      "        ...,\n",
      "        [ 0.0241,  0.0975, -0.0390,  ...,  0.0176,  0.0054, -0.0776],\n",
      "        [ 0.0195,  0.0953, -0.0370,  ...,  0.0215,  0.0100, -0.0785],\n",
      "        [ 0.0238,  0.0879, -0.0382,  ...,  0.0181,  0.0073, -0.0749]],\n",
      "       grad_fn=<CopySlices>)\n",
      "xt_closest_partition_id torch.Size([10000]) tensor([ 0, 16, 11,  ..., 25, 28, 10])\n",
      "NaN loss, break...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1000,) and (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     xt_closest_partition_id, centroid_vectors \u001b[38;5;241m=\u001b[39m update_centroids_info(\n\u001b[1;32m     55\u001b[0m         hidden_feature, xt_closest_partition_id, centroid_vectors)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Plotting the training loss\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# plt.plot(range(1,num_epochs+1),validation_loss)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/hnswlib/lib/python3.8/site-packages/matplotlib/pyplot.py:2757\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2755\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2759\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hnswlib/lib/python3.8/site-packages/matplotlib/axes/_axes.py:1632\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1632\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/anaconda3/envs/hnswlib/lib/python3.8/site-packages/matplotlib/axes/_base.py:312\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    311\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hnswlib/lib/python3.8/site-packages/matplotlib/axes/_base.py:498\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1000,) and (3,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAAJDCAYAAABOhiZdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXlklEQVR4nO3dUYjm913v8c/XXQtaPUbMKnWTYJC1cYVG2jH2QjGeco67uXARFJKKwSAswUa8bK70ojd6IUhp2mUpIfTGXByDrofYcG60B2owG6hptyVlSDnJnBSSWKnQgmHbrxczOZ3Omc38d/LM7H6Z1wsG5v///+aZ78WPGd7zf+Z5qrsDAADAHD9wowcAAADg+gg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYZs+Qq6rHq+q1qvrSNa5XVX28qtar6oWqev/qxwQAAOAtS+7IPZHkzNtcP5vk1NbH+SSfeudjAQAAcC17hlx3fy7JN95mybkkn+lNzya5pares6oBAQAA+H6r+B+5k0le2Xa8sXUOAACAA3B8BY9Ru5zrXRdWnc/m0y/z7ne/+wN33XXXCr49AADAPM8///wb3X1iP1+7ipDbSHL7tuPbkry628LuvpjkYpKsra315cuXV/DtAQAA5qmq/7Pfr13FUysvJXlw69UrP5jkm9399RU8LgAAALvY845cVf1VknuT3FpVG0n+NMkPJkl3X0jydJL7kqwn+XaShw5qWAAAABaEXHc/sMf1TvKRlU0EAADA21rFUysBAAA4REIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYZlHIVdWZqnqxqtar6tFdrv9YVf1dVf1LVV2pqodWPyoAAADJgpCrqmNJHktyNsnpJA9U1ekdyz6S5MvdfXeSe5P8RVW9a8WzAgAAkGV35O5Jst7dL3X3m0meTHJux5pO8qNVVUl+JMk3klxd6aQAAAAkWRZyJ5O8su14Y+vcdp9I8vNJXk3yxSR/3N3fXcmEAAAAfJ8lIVe7nOsdx7+R5AtJfjrJLyb5RFX9l//vgarOV9Xlqrr8+uuvX+eoAAAAJMtCbiPJ7duOb8vmnbftHkryVG9aT/K1JHftfKDuvtjda929duLEif3ODAAAcKQtCbnnkpyqqju3XsDk/iSXdqx5OcmHkqSqfirJe5O8tMpBAQAA2HR8rwXdfbWqHknyTJJjSR7v7itV9fDW9QtJPpbkiar6YjafivnR7n7jAOcGAAA4svYMuSTp7qeTPL3j3IVtn7+a5L+vdjQAAAB2s+gNwQEAALh5CDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGCYRSFXVWeq6sWqWq+qR6+x5t6q+kJVXamqf1ztmAAAALzl+F4LqupYkseS/LckG0meq6pL3f3lbWtuSfLJJGe6++Wq+skDmhcAAODIW3JH7p4k6939Une/meTJJOd2rPlwkqe6++Uk6e7XVjsmAAAAb1kScieTvLLteGPr3HY/l+THq+ofqur5qnpwVQMCAADw/fZ8amWS2uVc7/I4H0jyoSQ/lOSfqurZ7v7q9z1Q1fkk55PkjjvuuP5pAQAAWHRHbiPJ7duOb0vy6i5rPtvd3+ruN5J8LsndOx+ouy9291p3r504cWK/MwMAABxpS0LuuSSnqurOqnpXkvuTXNqx5m+T/GpVHa+qH07yy0m+stpRAQAASBY8tbK7r1bVI0meSXIsyePdfaWqHt66fqG7v1JVn03yQpLvJvl0d3/pIAcHAAA4qqp757+7HY61tbW+fPnyDfneAAAAN1pVPd/da/v52kVvCA4AAMDNQ8gBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGWRRyVXWmql6sqvWqevRt1v1SVX2nqn57dSMCAACw3Z4hV1XHkjyW5GyS00keqKrT11j350meWfWQAAAAfM+SO3L3JFnv7pe6+80kTyY5t8u6P0ry10leW+F8AAAA7LAk5E4meWXb8cbWuf+nqk4m+a0kF1Y3GgAAALtZEnK1y7necfyXST7a3d952weqOl9Vl6vq8uuvv75wRAAAALY7vmDNRpLbtx3fluTVHWvWkjxZVUlya5L7qupqd//N9kXdfTHJxSRZW1vbGYMAAAAssCTknktyqqruTPJ/k9yf5MPbF3T3nW99XlVPJPmfOyMOAACA1dgz5Lr7alU9ks1XozyW5PHuvlJVD29d939xAAAAh2jJHbl099NJnt5xbteA6+7ff+djAQAAcC2L3hAcAACAm4eQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhlkUclV1pqperKr1qnp0l+u/W1UvbH18vqruXv2oAAAAJAtCrqqOJXksydkkp5M8UFWndyz7WpJf6+73JflYkourHhQAAIBNS+7I3ZNkvbtf6u43kzyZ5Nz2Bd39+e7+t63DZ5PcttoxAQAAeMuSkDuZ5JVtxxtb567lD5L8/TsZCgAAgGs7vmBN7XKud11Y9evZDLlfucb180nOJ8kdd9yxcEQAAAC2W3JHbiPJ7duOb0vy6s5FVfW+JJ9Ocq67/3W3B+rui9291t1rJ06c2M+8AAAAR96SkHsuyamqurOq3pXk/iSXti+oqjuSPJXk97r7q6sfEwAAgLfs+dTK7r5aVY8keSbJsSSPd/eVqnp46/qFJH+S5CeSfLKqkuRqd68d3NgAAABHV3Xv+u9uB25tba0vX758Q743AADAjVZVz+/3BtiiNwQHAADg5iHkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYYQcAADAMEIOAABgGCEHAAAwjJADAAAYRsgBAAAMI+QAAACGEXIAAADDCDkAAIBhhBwAAMAwQg4AAGAYIQcAADCMkAMAABhGyAEAAAwj5AAAAIYRcgAAAMMIOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhhFyAAAAwwg5AACAYRaFXFWdqaoXq2q9qh7d5XpV1ce3rr9QVe9f/agAAAAkC0Kuqo4leSzJ2SSnkzxQVad3LDub5NTWx/kkn1rxnAAAAGxZckfuniTr3f1Sd7+Z5Mkk53asOZfkM73p2SS3VNV7VjwrAAAAWRZyJ5O8su14Y+vc9a4BAABgBY4vWFO7nOt9rElVnc/mUy+T5D+q6ksLvj/cCLcmeeNGDwG7sDe5Wdmb3MzsT25W793vFy4JuY0kt287vi3Jq/tYk+6+mORiklTV5e5eu65p4ZDYn9ys7E1uVvYmNzP7k5tVVV3e79cueWrlc0lOVdWdVfWuJPcnubRjzaUkD269euUHk3yzu7++36EAAAC4tj3vyHX31ap6JMkzSY4leby7r1TVw1vXLyR5Osl9SdaTfDvJQwc3MgAAwNG25KmV6e6nsxlr289d2PZ5J/nIdX7vi9e5Hg6T/cnNyt7kZmVvcjOzP7lZ7Xtv1maDAQAAMMWS/5EDAADgJnLgIVdVZ6rqxapar6pHd7leVfXxresvVNX7D3omSBbtzd/d2pMvVNXnq+ruGzEnR9Ne+3Pbul+qqu9U1W8f5nwcXUv2ZlXdW1VfqKorVfWPhz0jR9OC3+s/VlV/V1X/srU3vaYDh6KqHq+q16711mv77aEDDbmqOpbksSRnk5xO8kBVnd6x7GySU1sf55N86iBngmTx3vxakl/r7vcl+Vg8v55DsnB/vrXuz7P5YlRw4Jbszaq6Jcknk/xmd/9Ckt857Dk5ehb+3PxIki93991J7k3yF1uvyA4H7YkkZ97m+r566KDvyN2TZL27X+ruN5M8meTcjjXnknymNz2b5Jaqes8BzwV77s3u/nx3/9vW4bPZfH9EOAxLfnYmyR8l+eskrx3mcBxpS/bmh5M81d0vJ0l3258chiV7s5P8aFVVkh9J8o0kVw93TI6i7v5cNvfbteyrhw465E4meWXb8cbWuetdA6t2vfvuD5L8/YFOBN+z5/6sqpNJfivJhcDhWfKz8+eS/HhV/UNVPV9VDx7adBxlS/bmJ5L8fJJXk3wxyR9393cPZzx4W/vqoUVvP/AO1C7ndr5M5pI1sGqL911V/Xo2Q+5XDnQi+J4l+/Mvk3y0u7+z+cdlOBRL9ubxJB9I8qEkP5Tkn6rq2e7+6kEPx5G2ZG/+RpIvJPmvSX42yf+qqv/d3f9+wLPBXvbVQwcdchtJbt92fFs2/wpyvWtg1Rbtu6p6X5JPJznb3f96SLPBkv25luTJrYi7Ncl9VXW1u//mUCbkqFr6e/2N7v5Wkm9V1eeS3J1EyHGQluzNh5L82db7H69X1deS3JXknw9nRLimffXQQT+18rkkp6rqzq1/Jr0/yaUday4leXDr1Vo+mOSb3f31A54L9tybVXVHkqeS/J6/JHPI9tyf3X1nd/9Md/9Mkv+R5A9FHIdgye/1v03yq1V1vKp+OMkvJ/nKIc/J0bNkb76czTvFqaqfSvLeJC8d6pSwu3310IHekevuq1X1SDZfUe1Ykse7+0pVPbx1/UKSp5Pcl2Q9ybez+dcSOFAL9+afJPmJJJ/cuutxtbvXbtTMHB0L9yccuiV7s7u/UlWfTfJCku8m+XR37/qS27AqC39ufizJE1X1xWw+le2j3f3GDRuaI6Oq/iqbr5R6a1VtJPnTJD+YvLMeqs27ywAAAExx4G8IDgAAwGoJOQAAgGGEHAAAwDBCDgAAYBghBwAAMIyQAwAAGEbIAQAADCPkAAAAhvlPMkzHYdd5HZ0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training loop starts ===ã€‹ WITHOUT BATCH LOOP ===> HUGE BATCH\n",
    "xt_closest_partition_id = np.random.randint(0, 32, size=10000)\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Initializing variable for storing\n",
    "    # loss\n",
    "    running_loss = 0\n",
    "    \n",
    "        \n",
    "    # WENQI: NOTE That the input is mapped to 0~1 -> such that we use sigmod to get the output value\n",
    "    sift_in_batch = xt_tensor\n",
    "#         print('in', sift_in_batch.shape, sift_in_batch[0])\n",
    "\n",
    "    # Generating output\n",
    "    out_batch = model(sift_in_batch)\n",
    "    hidden_feature = model.encoder(sift_in_batch)\n",
    "\n",
    "    # Calculating loss\n",
    "    print('out_batch', out_batch.shape, out_batch)\n",
    "    print('sift_in_batch', sift_in_batch.shape, sift_in_batch)\n",
    "    print('hidden_feature', hidden_feature.shape, hidden_feature)\n",
    "    print('centroid_vectors', centroid_vectors.shape, centroid_vectors)\n",
    "    print('xt_closest_partition_id', xt_closest_partition_id.shape, xt_closest_partition_id)\n",
    "    loss = loss_func(out_batch, sift_in_batch, \n",
    "                     hidden_feature, centroid_vectors, \n",
    "                     xt_closest_partition_id)\n",
    "    if torch.isnan(loss):\n",
    "        print('NaN loss, break...')\n",
    "        break\n",
    "    # Updating weights according\n",
    "    # to the calculated loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Incrementing loss\n",
    "    running_loss += loss.item()\n",
    "        \n",
    "    train_loss.append(running_loss)\n",
    "    \n",
    "#     out_val = model(x_val)\n",
    "#     out_hidden = \n",
    "#     loss_val = loss_func(out_val, x_val)\n",
    "#     validation_loss.append(loss_val)\n",
    "    \n",
    "    print(\"epoch:\", epoch, \"train loss:\", running_loss)\n",
    "#     print(\"epoch:\", epoch, \"train loss:\", running_loss, \"val loss: \", loss_val.item())\n",
    "\n",
    "    # Storing useful images and\n",
    "    # reconstructed outputs for the last batch\n",
    "    outputs[epoch+1] = {'in': sift_in_batch, 'out': out_batch}\n",
    "    \n",
    "    # update centroids after an epoch\n",
    "    xt_closest_partition_id, centroid_vectors = update_centroids_info(\n",
    "        hidden_feature, xt_closest_partition_id, centroid_vectors)\n",
    "    \n",
    "\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.plot(range(1,num_epochs+1),train_loss)\n",
    "# plt.plot(range(1,num_epochs+1),validation_loss)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db204173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c53c5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: nan\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, running_loss)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#     print(\"epoch:\", epoch, \"train loss:\", running_loss, \"val loss: \", loss_val.item())\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Storing useful images and\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# reconstructed outputs for the last batch\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     outputs[epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m'\u001b[39m: sift_in_batch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mout\u001b[49m}\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# update centroids after an epoch\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     xt_closest_partition_id, centroid_vectors \u001b[38;5;241m=\u001b[39m update_centroids_info(\n\u001b[1;32m     62\u001b[0m         hidden_feature, xt_closest_partition_id, centroid_vectors)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop starts\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Initializing variable for storing\n",
    "    # loss\n",
    "    running_loss = 0\n",
    "    \n",
    "    # Iterating over the training dataset\n",
    "    batch_num_per_epoch = int(np.ceil(n_train / batch_size))\n",
    "#     print(batch_num_per_epoch)\n",
    "\n",
    "    hidden_feature = model.encoder(xt_tensor)\n",
    "    for i_batch in range(batch_num_per_epoch):\n",
    "\n",
    "        start_id = i_batch * batch_size\n",
    "        end_id = (i_batch + 1) * batch_size\n",
    "        \n",
    "        # WENQI: NOTE That the input is mapped to 0~1 -> such that we use sigmod to get the output value\n",
    "        sift_in_batch = xt_tensor[start_id: end_id]\n",
    "#         print('in', sift_in_batch.shape, sift_in_batch[0])\n",
    "\n",
    "        # Generating output\n",
    "        out_batch = model(sift_in_batch)\n",
    "        hidden_feature_batch = model.encoder(sift_in_batch)\n",
    "        hidden_feature[start_id: end_id] = hidden_feature_batch\n",
    "#         print('out', out.shape, out[0])\n",
    "\n",
    "        # Calculating loss\n",
    "        xt_closest_partition_id_batch = xt_closest_partition_id[start_id: end_id]\n",
    "        loss = loss_func(out_batch, sift_in_batch, \n",
    "                         hidden_feature_batch, centroid_vectors, \n",
    "                         xt_closest_partition_id_batch)\n",
    "\n",
    "        # Updating weights according\n",
    "        # to the calculated loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Incrementing loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "\n",
    "    # Averaging out loss over entire batch\n",
    "    running_loss /= batch_size\n",
    "    train_loss.append(running_loss)\n",
    "    \n",
    "#     out_val = model(x_val)\n",
    "#     out_hidden = \n",
    "#     loss_val = loss_func(out_val, x_val)\n",
    "#     validation_loss.append(loss_val)\n",
    "    \n",
    "    print(\"epoch:\", epoch, \"train loss:\", running_loss)\n",
    "#     print(\"epoch:\", epoch, \"train loss:\", running_loss, \"val loss: \", loss_val.item())\n",
    "\n",
    "    # Storing useful images and\n",
    "    # reconstructed outputs for the last batch\n",
    "    outputs[epoch+1] = {'in': sift_in_batch, 'out': out}\n",
    "    \n",
    "    # update centroids after an epoch\n",
    "    xt_closest_partition_id, centroid_vectors = update_centroids_info(\n",
    "        hidden_feature, xt_closest_partition_id, centroid_vectors)\n",
    "    \n",
    "\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.plot(range(1,num_epochs+1),train_loss)\n",
    "# plt.plot(range(1,num_epochs+1),validation_loss)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fb3d3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in torch.Size([100, 128]) tensor([0.0000, 0.0000, 0.0000, 0.0039, 0.0312, 0.0273, 0.0117, 0.0078, 0.0195,\n",
      "        0.0000, 0.0000, 0.0117, 0.0195, 0.0273, 0.0430, 0.1211, 0.0508, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.1133, 0.4141, 0.4180, 0.0508, 0.0000, 0.0000,\n",
      "        0.0000, 0.0039, 0.2383, 0.2734, 0.1641, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0039, 0.0898, 0.1094, 0.0625, 0.2461, 0.0156, 0.0000, 0.0000, 0.0000,\n",
      "        0.0234, 0.3242, 0.3164, 0.4570, 0.3359, 0.0977, 0.0586, 0.0664, 0.1953,\n",
      "        0.3281, 0.4570, 0.1211, 0.0898, 0.0703, 0.1367, 0.3789, 0.4570, 0.1914,\n",
      "        0.0938, 0.2656, 0.1055, 0.0000, 0.0000, 0.0000, 0.0156, 0.1133, 0.2773,\n",
      "        0.3164, 0.1836, 0.0508, 0.0391, 0.1250, 0.3398, 0.4570, 0.4570, 0.1758,\n",
      "        0.2969, 0.1562, 0.0859, 0.2344, 0.2734, 0.1602, 0.0352, 0.0273, 0.0820,\n",
      "        0.1133, 0.1523, 0.2070, 0.0820, 0.0156, 0.0039, 0.2148, 0.2812, 0.0117,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0352, 0.2539, 0.4570, 0.2852, 0.1445,\n",
      "        0.1094, 0.0898, 0.0664, 0.1328, 0.0430, 0.0430, 0.1055, 0.2383, 0.2500,\n",
      "        0.0977, 0.0156, 0.0000, 0.1641, 0.0508, 0.0039, 0.0039, 0.0039, 0.0547,\n",
      "        0.0391, 0.0234])\n",
      "out torch.Size([100, 128]) tensor([ 1.8044e-03,  7.7066e-03,  6.8448e-02,  7.5585e-02,  3.9884e-02,\n",
      "         3.2535e-02,  3.2257e-02,  2.2615e-02,  4.5143e-02,  1.9384e-02,\n",
      "         2.6830e-02,  2.1662e-02,  1.1869e-02,  1.7345e-02,  1.1319e-01,\n",
      "         1.2335e-01,  1.5057e-01,  2.5658e-02,  2.2012e-02, -1.2162e-04,\n",
      "        -7.9779e-03,  3.7872e-02,  3.5758e-01,  3.8254e-01,  9.2553e-02,\n",
      "         8.4460e-02,  7.2701e-02,  2.2059e-03,  2.9443e-02,  2.2742e-01,\n",
      "         3.1094e-01,  1.5533e-01,  8.6537e-02,  2.5103e-02,  2.0323e-02,\n",
      "         5.9506e-02,  4.2792e-02,  6.4665e-02,  8.3449e-02,  1.2051e-01,\n",
      "         2.8662e-01,  7.1395e-02,  3.0499e-02,  2.4832e-02,  1.1905e-02,\n",
      "         6.4960e-02,  2.1373e-01,  1.9611e-01,  4.7987e-01,  2.1829e-01,\n",
      "         6.2334e-02,  1.2911e-02,  5.5500e-02,  9.5320e-02,  1.6224e-01,\n",
      "         3.8630e-01,  7.3157e-02,  1.2239e-01,  2.0226e-01,  1.0423e-01,\n",
      "         3.1454e-01,  3.3679e-01,  1.5484e-01,  6.8584e-02,  3.0548e-01,\n",
      "         1.4833e-01,  3.0138e-02,  1.3590e-02,  3.6677e-02,  8.0385e-02,\n",
      "         8.9711e-02,  2.8315e-01,  1.8562e-01,  1.2262e-01,  6.5130e-02,\n",
      "        -1.4557e-02,  1.4875e-02,  2.8955e-01,  5.2030e-01,  3.5868e-01,\n",
      "         3.4726e-01,  2.7403e-01,  9.8266e-02,  6.3805e-02,  1.0802e-01,\n",
      "         1.5951e-01,  2.4557e-01,  1.8975e-01,  6.7199e-03,  1.7904e-01,\n",
      "         1.9421e-01,  2.1077e-01,  2.9885e-01,  1.3272e-01,  4.5655e-02,\n",
      "        -4.5657e-02,  2.8863e-01,  2.3912e-01,  3.6543e-02,  7.9602e-03,\n",
      "         7.3143e-02,  1.0871e-01, -1.1661e-02,  7.8891e-02,  3.0758e-01,\n",
      "         3.6321e-01,  1.9802e-01,  2.5959e-02,  2.1799e-02,  1.0074e-01,\n",
      "         1.5196e-02,  7.0928e-02,  1.0639e-01,  1.2562e-01,  8.5568e-02,\n",
      "         7.6609e-02,  1.6652e-01,  2.0020e-01,  6.8226e-02,  3.5650e-02,\n",
      "         2.9100e-02,  6.2955e-02,  5.0333e-02,  8.6612e-02,  1.2796e-01,\n",
      "         1.1495e-01,  8.7364e-02,  3.9777e-02], grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0018, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0077, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0684, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0039) out:  tensor(0.0756, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0312) out:  tensor(0.0399, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0273) out:  tensor(0.0325, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0117) out:  tensor(0.0323, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0078) out:  tensor(0.0226, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0195) out:  tensor(0.0451, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0194, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0268, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0117) out:  tensor(0.0217, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0195) out:  tensor(0.0119, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0273) out:  tensor(0.0173, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0430) out:  tensor(0.1132, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1211) out:  tensor(0.1233, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0508) out:  tensor(0.1506, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0257, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0220, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(-0.0001, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(-0.0080, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1133) out:  tensor(0.0379, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.4141) out:  tensor(0.3576, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.4180) out:  tensor(0.3825, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0508) out:  tensor(0.0926, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0845, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0727, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0022, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0039) out:  tensor(0.0294, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2383) out:  tensor(0.2274, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2734) out:  tensor(0.3109, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1641) out:  tensor(0.1553, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0865, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0251, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0203, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0595, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0039) out:  tensor(0.0428, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0898) out:  tensor(0.0647, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1094) out:  tensor(0.0834, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0625) out:  tensor(0.1205, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2461) out:  tensor(0.2866, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0156) out:  tensor(0.0714, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0305, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0248, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0119, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0234) out:  tensor(0.0650, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.3242) out:  tensor(0.2137, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.3164) out:  tensor(0.1961, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.4570) out:  tensor(0.4799, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.3359) out:  tensor(0.2183, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0977) out:  tensor(0.0623, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0586) out:  tensor(0.0129, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0664) out:  tensor(0.0555, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1953) out:  tensor(0.0953, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.3281) out:  tensor(0.1622, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.4570) out:  tensor(0.3863, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1211) out:  tensor(0.0732, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0898) out:  tensor(0.1224, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0703) out:  tensor(0.2023, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1367) out:  tensor(0.1042, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.3789) out:  tensor(0.3145, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.4570) out:  tensor(0.3368, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1914) out:  tensor(0.1548, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0938) out:  tensor(0.0686, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2656) out:  tensor(0.3055, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1055) out:  tensor(0.1483, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0301, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0136, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0367, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0156) out:  tensor(0.0804, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1133) out:  tensor(0.0897, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2773) out:  tensor(0.2832, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.3164) out:  tensor(0.1856, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1836) out:  tensor(0.1226, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0508) out:  tensor(0.0651, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0391) out:  tensor(-0.0146, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1250) out:  tensor(0.0149, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.3398) out:  tensor(0.2895, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.4570) out:  tensor(0.5203, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.4570) out:  tensor(0.3587, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1758) out:  tensor(0.3473, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2969) out:  tensor(0.2740, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1562) out:  tensor(0.0983, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0859) out:  tensor(0.0638, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2344) out:  tensor(0.1080, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2734) out:  tensor(0.1595, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1602) out:  tensor(0.2456, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0352) out:  tensor(0.1898, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0273) out:  tensor(0.0067, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0820) out:  tensor(0.1790, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1133) out:  tensor(0.1942, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1523) out:  tensor(0.2108, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2070) out:  tensor(0.2989, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0820) out:  tensor(0.1327, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0156) out:  tensor(0.0457, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0039) out:  tensor(-0.0457, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2148) out:  tensor(0.2886, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2812) out:  tensor(0.2391, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0117) out:  tensor(0.0365, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0080, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0731, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.1087, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(-0.0117, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0352) out:  tensor(0.0789, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2539) out:  tensor(0.3076, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.4570) out:  tensor(0.3632, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2852) out:  tensor(0.1980, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1445) out:  tensor(0.0260, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1094) out:  tensor(0.0218, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0898) out:  tensor(0.1007, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0664) out:  tensor(0.0152, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1328) out:  tensor(0.0709, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0430) out:  tensor(0.1064, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0430) out:  tensor(0.1256, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1055) out:  tensor(0.0856, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2383) out:  tensor(0.0766, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.2500) out:  tensor(0.1665, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0977) out:  tensor(0.2002, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0156) out:  tensor(0.0682, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.) out:  tensor(0.0357, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.1641) out:  tensor(0.0291, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0508) out:  tensor(0.0630, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0039) out:  tensor(0.0503, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0039) out:  tensor(0.0866, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0039) out:  tensor(0.1280, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0547) out:  tensor(0.1150, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0391) out:  tensor(0.0874, grad_fn=<SelectBackward>)\n",
      "in:  tensor(0.0234) out:  tensor(0.0398, grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the result of a single vector\n",
    "\n",
    "i_batch = 0\n",
    "\n",
    "# Loading image(s) and\n",
    "# reshaping it into a 1-d vector\n",
    "sift_in_batch = torch.FloatTensor(xt[i_batch * batch_size: (i_batch + 1) * batch_size])\n",
    "print('in', sift_in_batch.shape, sift_in_batch[0])\n",
    "\n",
    "# Generating output\n",
    "out = model(sift_in_batch)\n",
    "print('out', out.shape, out[0])\n",
    "    \n",
    "for i in range(128):\n",
    "    print('in: ', sift_in_batch[0][i], 'out: ', out[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "955f8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sift_in_batch / 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc5f1c2",
   "metadata": {},
   "source": [
    "## K means on hidden feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1333173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "47f1e102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32)\n",
      "[ 0.19215745  1.2356458  -0.55529815 -0.580348   -0.70802015  0.38527507\n",
      "  0.24764977  0.08810449  0.55208504  0.0084953  -0.43993902 -0.74304897\n",
      "  1.139927   -1.0337955   0.5081504  -0.20082477  0.02990077 -0.07197974\n",
      " -0.14414987  0.1186108  -0.4632683   0.39389193 -0.57805717 -0.32012156\n",
      " -0.27312028  0.36488637  0.15000263  0.10088865  0.03375911  0.03263886\n",
      "  0.27500322  0.6829792 ]\n"
     ]
    }
   ],
   "source": [
    "hidden_features_10K = model.encoder(torch.FloatTensor(xt)).detach().numpy()\n",
    "print(hidden_features_10K.shape)\n",
    "print(hidden_features_10K[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f0e2ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=32)\n",
    "kmeans.fit(hidden_features_10K)\n",
    "y_kmeans = kmeans.predict(hidden_features_10K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8bdc3345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13  1 13 ... 20 14 12]\n"
     ]
    }
   ],
   "source": [
    "print(y_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "63fbac77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13  1 13 ...  9  8 15]\n",
      "(1000000,)\n"
     ]
    }
   ],
   "source": [
    "hidden_feature_1M = model.encoder(torch.FloatTensor(xb)).detach().numpy()\n",
    "\n",
    "partition_IDs = kmeans.predict(hidden_feature_1M)\n",
    "print(partition_IDs)\n",
    "print(partition_IDs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2bdb8fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items in partition  0 31246 average = 31250\n",
      "items in partition  1 24537 average = 31250\n",
      "items in partition  2 31969 average = 31250\n",
      "items in partition  3 29831 average = 31250\n",
      "items in partition  4 41228 average = 31250\n",
      "items in partition  5 24541 average = 31250\n",
      "items in partition  6 37040 average = 31250\n",
      "items in partition  7 23075 average = 31250\n",
      "items in partition  8 21908 average = 31250\n",
      "items in partition  9 30168 average = 31250\n",
      "items in partition  10 45163 average = 31250\n",
      "items in partition  11 30958 average = 31250\n",
      "items in partition  12 45456 average = 31250\n",
      "items in partition  13 30161 average = 31250\n",
      "items in partition  14 26478 average = 31250\n",
      "items in partition  15 25687 average = 31250\n",
      "items in partition  16 29251 average = 31250\n",
      "items in partition  17 26623 average = 31250\n",
      "items in partition  18 44392 average = 31250\n",
      "items in partition  19 35272 average = 31250\n",
      "items in partition  20 25693 average = 31250\n",
      "items in partition  21 28595 average = 31250\n",
      "items in partition  22 36466 average = 31250\n",
      "items in partition  23 27473 average = 31250\n",
      "items in partition  24 20421 average = 31250\n",
      "items in partition  25 25677 average = 31250\n",
      "items in partition  26 34341 average = 31250\n",
      "items in partition  27 44170 average = 31250\n",
      "items in partition  28 29350 average = 31250\n",
      "items in partition  29 23064 average = 31250\n",
      "items in partition  30 42776 average = 31250\n",
      "items in partition  31 26990 average = 31250\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping: partition ID -> {list of vector IDs}\n",
    "\n",
    "num_partition = 32\n",
    "\n",
    "partition_id_vec_id_list_1M = dict()\n",
    "for i in range(num_partition):\n",
    "    partition_id_vec_id_list_1M[i] = []\n",
    "\n",
    "\n",
    "for i in range(int(1e6)):\n",
    "    partition_ID = int(partition_IDs[i])\n",
    "    partition_id_vec_id_list_1M[partition_ID].append(i)\n",
    "    \n",
    "for i in range(num_partition):\n",
    "    print('items in partition ', i, len(partition_id_vec_id_list_1M[i]), 'average =', int(1e6/num_partition))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2fd8ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def scan_partition(query_vec, partition_id_list, vector_set):\n",
    "    \"\"\"\n",
    "    query_vec = (128, )\n",
    "    partition_id_list = (N_num_vec, )\n",
    "    vector_set = 1M dataset (1M, 128)\n",
    "    \"\"\"\n",
    "    min_dist = 1e10\n",
    "    min_dist_ID = None\n",
    "    for vec_id in partition_id_list:\n",
    "        dataset_vec = vector_set[vec_id]\n",
    "        dist = np.linalg.norm(query_vec - dataset_vec)\n",
    "        if dist <= min_dist:\n",
    "            min_dist = dist\n",
    "            min_dist_ID = vec_id\n",
    "            \n",
    "    return min_dist_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d7dabe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32)\n",
      "0 504814\n",
      "1 790327\n",
      "2 552515\n",
      "3 335355\n",
      "4 189069\n",
      "5 508403\n",
      "6 342520\n",
      "7 327960\n",
      "8 834657\n",
      "9 360082\n",
      "10 878295\n",
      "11 353408\n",
      "12 215771\n",
      "13 653682\n",
      "14 368047\n",
      "15 776345\n",
      "16 373550\n",
      "17 615740\n",
      "18 602078\n",
      "19 84644\n",
      "20 68023\n",
      "21 114330\n",
      "22 756153\n",
      "23 436181\n",
      "24 698614\n",
      "25 593360\n",
      "26 922290\n",
      "27 427711\n",
      "28 495856\n",
      "29 784549\n",
      "30 429906\n",
      "31 407192\n",
      "32 295009\n",
      "33 190504\n",
      "34 554182\n",
      "35 825435\n",
      "36 428565\n",
      "37 812777\n",
      "38 341091\n",
      "39 856200\n",
      "40 982373\n",
      "41 499763\n",
      "42 915089\n",
      "43 43368\n",
      "44 77190\n",
      "45 858815\n",
      "46 436180\n",
      "47 135525\n",
      "48 28097\n",
      "49 426503\n",
      "50 629910\n",
      "51 467756\n",
      "52 803688\n",
      "53 74183\n",
      "54 819365\n",
      "55 882827\n",
      "56 535502\n",
      "57 636955\n",
      "58 111148\n",
      "59 12747\n",
      "60 534495\n",
      "61 833563\n",
      "62 611787\n",
      "63 45494\n",
      "64 943477\n",
      "65 809566\n",
      "66 3367\n",
      "67 850184\n",
      "68 570247\n",
      "69 434096\n",
      "70 116383\n",
      "71 753756\n",
      "72 222336\n",
      "73 787309\n",
      "74 436415\n",
      "75 629700\n",
      "76 470987\n",
      "77 245786\n",
      "78 680004\n",
      "79 694111\n",
      "80 23773\n",
      "81 380662\n",
      "82 26209\n",
      "83 34810\n",
      "84 632431\n",
      "85 853398\n",
      "86 299980\n",
      "87 138747\n",
      "88 698591\n",
      "89 845760\n",
      "90 32380\n",
      "91 490742\n",
      "92 190435\n",
      "93 76908\n",
      "94 657946\n",
      "95 330662\n",
      "96 898436\n",
      "97 763985\n",
      "98 532460\n",
      "99 72670\n",
      "100 614216\n",
      "101 88094\n",
      "102 869547\n",
      "103 193696\n",
      "104 88787\n",
      "105 911262\n",
      "106 647580\n",
      "107 158971\n",
      "108 839928\n",
      "109 183298\n",
      "110 46950\n",
      "111 110719\n",
      "112 314894\n",
      "113 479253\n",
      "114 743383\n",
      "115 827351\n",
      "116 223573\n",
      "117 128643\n",
      "118 684394\n",
      "119 387074\n",
      "120 521774\n",
      "121 944129\n",
      "122 781800\n",
      "123 364894\n",
      "124 151268\n",
      "125 276609\n",
      "126 747748\n",
      "127 80835\n",
      "128 531750\n",
      "129 721619\n",
      "130 620311\n",
      "131 67534\n",
      "132 521456\n",
      "133 276777\n",
      "134 435653\n",
      "135 712276\n",
      "136 433941\n",
      "137 507214\n",
      "138 736937\n",
      "139 921471\n",
      "140 206404\n",
      "141 284567\n",
      "142 506970\n",
      "143 400867\n",
      "144 939197\n",
      "145 180009\n",
      "146 100364\n",
      "147 615075\n",
      "148 740524\n",
      "149 610704\n",
      "150 931794\n",
      "151 299290\n",
      "152 458606\n",
      "153 211593\n",
      "154 163622\n",
      "155 647036\n",
      "156 600310\n",
      "157 349181\n",
      "158 363821\n",
      "159 658033\n",
      "160 558050\n",
      "161 100754\n",
      "162 715042\n",
      "163 417143\n",
      "164 730545\n",
      "165 686230\n",
      "166 16783\n",
      "167 127596\n",
      "168 890828\n",
      "169 618264\n",
      "170 387594\n",
      "171 574037\n",
      "172 726170\n",
      "173 601399\n",
      "174 815041\n",
      "175 615536\n",
      "176 255517\n",
      "177 813063\n",
      "178 99062\n",
      "179 16631\n",
      "180 426260\n",
      "181 405760\n",
      "182 654388\n",
      "183 157829\n",
      "184 773829\n",
      "185 505011\n",
      "186 24235\n",
      "187 474919\n",
      "188 348477\n",
      "189 82353\n",
      "190 532004\n",
      "191 229588\n",
      "192 728538\n",
      "193 676097\n",
      "194 720715\n",
      "195 502602\n",
      "196 99390\n",
      "197 487958\n",
      "198 408741\n",
      "199 688279\n",
      "200 482062\n",
      "201 319070\n",
      "202 891742\n",
      "203 893184\n",
      "204 562871\n",
      "205 293191\n",
      "206 312783\n",
      "207 154199\n",
      "208 700405\n",
      "209 704374\n",
      "210 659298\n",
      "211 114655\n",
      "212 370053\n",
      "213 40891\n",
      "214 868675\n",
      "215 62525\n",
      "216 462112\n",
      "217 392283\n",
      "218 662526\n",
      "219 255085\n",
      "220 127953\n",
      "221 59587\n",
      "222 490147\n",
      "223 227570\n",
      "224 248796\n",
      "225 38208\n",
      "226 280711\n",
      "227 358687\n",
      "228 218182\n",
      "229 900961\n",
      "230 543775\n",
      "231 58545\n",
      "232 329739\n",
      "233 69876\n",
      "234 345076\n",
      "235 453730\n",
      "236 341739\n",
      "237 336661\n",
      "238 397800\n",
      "239 923092\n",
      "240 841402\n",
      "241 19787\n",
      "242 829881\n",
      "243 241941\n",
      "244 283481\n",
      "245 621555\n",
      "246 433812\n",
      "247 687035\n",
      "248 227599\n",
      "249 940604\n",
      "250 275687\n",
      "251 534833\n",
      "252 12980\n",
      "253 696534\n",
      "254 750971\n",
      "255 594607\n",
      "256 39540\n",
      "257 406423\n",
      "258 904665\n",
      "259 871073\n",
      "260 194969\n",
      "261 975360\n",
      "262 176577\n",
      "263 179727\n",
      "264 525385\n",
      "265 311211\n",
      "266 622692\n",
      "267 919989\n",
      "268 882241\n",
      "269 387681\n",
      "270 843344\n",
      "271 764769\n",
      "272 897130\n",
      "273 864593\n",
      "274 91127\n",
      "275 890301\n",
      "276 410068\n",
      "277 381105\n",
      "278 314911\n",
      "279 502713\n",
      "280 652640\n",
      "281 228731\n",
      "282 235591\n",
      "283 637742\n",
      "284 954136\n",
      "285 284148\n",
      "286 547192\n",
      "287 374785\n",
      "288 829609\n",
      "289 54173\n",
      "290 413036\n",
      "291 212310\n",
      "292 822767\n",
      "293 341758\n",
      "294 505267\n",
      "295 359952\n",
      "296 862903\n",
      "297 707266\n",
      "298 40459\n",
      "299 610954\n",
      "300 694338\n",
      "301 221978\n",
      "302 255457\n",
      "303 826145\n",
      "304 971490\n",
      "305 50311\n",
      "306 570639\n",
      "307 106859\n",
      "308 485308\n",
      "309 713011\n",
      "310 450565\n",
      "311 832880\n",
      "312 776295\n",
      "313 246087\n",
      "314 512715\n",
      "315 633701\n",
      "316 457128\n",
      "317 988353\n",
      "318 710129\n",
      "319 742130\n",
      "320 443952\n",
      "321 600755\n",
      "322 105357\n",
      "323 277483\n",
      "324 159809\n",
      "325 179357\n",
      "326 261736\n",
      "327 489504\n",
      "328 952263\n",
      "329 975783\n",
      "330 161441\n",
      "331 41101\n",
      "332 404578\n",
      "333 105126\n",
      "334 429034\n",
      "335 270749\n",
      "336 702421\n",
      "337 20163\n",
      "338 213985\n",
      "339 349014\n",
      "340 759225\n",
      "341 544126\n",
      "342 423581\n",
      "343 732120\n",
      "344 51632\n",
      "345 15283\n",
      "346 55092\n",
      "347 274367\n",
      "348 303388\n",
      "349 396805\n",
      "350 993435\n",
      "351 14054\n",
      "352 32863\n",
      "353 841270\n",
      "354 474444\n",
      "355 465377\n",
      "356 906128\n",
      "357 368111\n",
      "358 577379\n",
      "359 941829\n",
      "360 710374\n",
      "361 389519\n",
      "362 226406\n",
      "363 740469\n",
      "364 194573\n",
      "365 522244\n",
      "366 834152\n",
      "367 636589\n",
      "368 89696\n",
      "369 437975\n",
      "370 7814\n",
      "371 775681\n",
      "372 542445\n",
      "373 822268\n",
      "374 791506\n",
      "375 836831\n",
      "376 661403\n",
      "377 833308\n",
      "378 486907\n",
      "379 424704\n",
      "380 348637\n",
      "381 492678\n",
      "382 572369\n",
      "383 413517\n",
      "384 33409\n",
      "385 27873\n",
      "386 652136\n",
      "387 211977\n",
      "388 901283\n",
      "389 379018\n",
      "390 466411\n",
      "391 593019\n",
      "392 158002\n",
      "393 915100\n",
      "394 390662\n",
      "395 146142\n",
      "396 354790\n",
      "397 883806\n",
      "398 406054\n",
      "399 669675\n",
      "400 190955\n",
      "401 519812\n",
      "402 297703\n",
      "403 489517\n",
      "404 409165\n",
      "405 60177\n",
      "406 976490\n",
      "407 858959\n",
      "408 225171\n",
      "409 575571\n",
      "410 69912\n",
      "411 919139\n",
      "412 79619\n",
      "413 8002\n",
      "414 21030\n",
      "415 968647\n",
      "416 881988\n",
      "417 486479\n",
      "418 380466\n",
      "419 377964\n",
      "420 350797\n",
      "421 318651\n",
      "422 897782\n",
      "423 138220\n",
      "424 511086\n",
      "425 30864\n",
      "426 929155\n",
      "427 229023\n",
      "428 307396\n",
      "429 119714\n",
      "430 618057\n",
      "431 198920\n",
      "432 367252\n",
      "433 654336\n",
      "434 877914\n",
      "435 847366\n",
      "436 505110\n",
      "437 666247\n",
      "438 991356\n",
      "439 283240\n",
      "440 347045\n",
      "441 170308\n",
      "442 540275\n",
      "443 505291\n",
      "444 180058\n",
      "445 769074\n",
      "446 245844\n",
      "447 16958\n",
      "448 341961\n",
      "449 638745\n",
      "450 272960\n",
      "451 446311\n",
      "452 262268\n",
      "453 550773\n",
      "454 517008\n",
      "455 288086\n",
      "456 175719\n",
      "457 472606\n",
      "458 852157\n",
      "459 770392\n",
      "460 497205\n",
      "461 49148\n",
      "462 197161\n",
      "463 832428\n",
      "464 575945\n",
      "465 364784\n",
      "466 692744\n",
      "467 339597\n",
      "468 902217\n",
      "469 109979\n",
      "470 87016\n",
      "471 666271\n",
      "472 371453\n",
      "473 245902\n",
      "474 669736\n",
      "475 374206\n",
      "476 550616\n",
      "477 400932\n",
      "478 726765\n",
      "479 339150\n",
      "480 296444\n",
      "481 80686\n",
      "482 588372\n",
      "483 790547\n",
      "484 745456\n",
      "485 649148\n",
      "486 988210\n",
      "487 109319\n",
      "488 715301\n",
      "489 654256\n",
      "490 821736\n",
      "491 252094\n",
      "492 479332\n",
      "493 954917\n",
      "494 781138\n",
      "495 869611\n",
      "496 789541\n",
      "497 307106\n",
      "498 200945\n",
      "499 61401\n",
      "500 704133\n",
      "501 484050\n",
      "502 103697\n",
      "503 485340\n",
      "504 305268\n",
      "505 479440\n",
      "506 306838\n",
      "507 244725\n",
      "508 925596\n",
      "509 464417\n",
      "510 139737\n",
      "511 541290\n",
      "512 934691\n",
      "513 458989\n",
      "514 348545\n",
      "515 441868\n",
      "516 790672\n",
      "517 574663\n",
      "518 614658\n",
      "519 898125\n",
      "520 262906\n",
      "521 503898\n",
      "522 8101\n",
      "523 677690\n",
      "524 375814\n",
      "525 560261\n",
      "526 66950\n",
      "527 700823\n",
      "528 910524\n",
      "529 323060\n",
      "530 516264\n",
      "531 200812\n",
      "532 792766\n",
      "533 201364\n",
      "534 18417\n",
      "535 955060\n",
      "536 170383\n",
      "537 735401\n",
      "538 524074\n",
      "539 407329\n",
      "540 409023\n",
      "541 335115\n",
      "542 106826\n",
      "543 97388\n",
      "544 106071\n",
      "545 663760\n",
      "546 17661\n",
      "547 986856\n",
      "548 770133\n",
      "549 665633\n",
      "550 945355\n",
      "551 346482\n",
      "552 275113\n",
      "553 945386\n",
      "554 664012\n",
      "555 426354\n",
      "556 669139\n",
      "557 463737\n",
      "558 2090\n",
      "559 62420\n",
      "560 178707\n",
      "561 850585\n",
      "562 26474\n",
      "563 133311\n",
      "564 6490\n",
      "565 187505\n",
      "566 916002\n",
      "567 728502\n",
      "568 318785\n",
      "569 424270\n",
      "570 518769\n",
      "571 136165\n",
      "572 179683\n",
      "573 363815\n",
      "574 207707\n",
      "575 797422\n",
      "576 892717\n",
      "577 284542\n",
      "578 780160\n",
      "579 24123\n",
      "580 926053\n",
      "581 831361\n",
      "582 153558\n",
      "583 865420\n",
      "584 168379\n",
      "585 326356\n",
      "586 190599\n",
      "587 323345\n",
      "588 618174\n",
      "589 798828\n",
      "590 178257\n",
      "591 826085\n",
      "592 528325\n",
      "593 443972\n",
      "594 176788\n",
      "595 3450\n",
      "596 749959\n",
      "597 5293\n",
      "598 211500\n",
      "599 548221\n",
      "600 948902\n",
      "601 67693\n",
      "602 745543\n",
      "603 952716\n",
      "604 563653\n",
      "605 962482\n",
      "606 660336\n",
      "607 572151\n",
      "608 180619\n",
      "609 683698\n",
      "610 538121\n",
      "611 42255\n",
      "612 614819\n",
      "613 527857\n",
      "614 427978\n",
      "615 320706\n",
      "616 885260\n",
      "617 470373\n",
      "618 132535\n",
      "619 575736\n",
      "620 195345\n",
      "621 45579\n",
      "622 359572\n",
      "623 528033\n",
      "624 752970\n",
      "625 553933\n",
      "626 288832\n",
      "627 386126\n",
      "628 612758\n",
      "629 580538\n",
      "630 521470\n",
      "631 748182\n",
      "632 549394\n",
      "633 882102\n",
      "634 761057\n",
      "635 38420\n",
      "636 705283\n",
      "637 466808\n",
      "638 244197\n",
      "639 331663\n",
      "640 693768\n",
      "641 539044\n",
      "642 815387\n",
      "643 274582\n",
      "644 955861\n",
      "645 788077\n",
      "646 565592\n",
      "647 987689\n",
      "648 184645\n",
      "649 599396\n",
      "650 573811\n",
      "651 749949\n",
      "652 711658\n",
      "653 997643\n",
      "654 563163\n",
      "655 187418\n",
      "656 696690\n",
      "657 702483\n",
      "658 78614\n",
      "659 972113\n",
      "660 442514\n",
      "661 717667\n",
      "662 829374\n",
      "663 738819\n",
      "664 28359\n",
      "665 976944\n",
      "666 38581\n",
      "667 617838\n",
      "668 89141\n",
      "669 683514\n",
      "670 166860\n",
      "671 882990\n",
      "672 727025\n",
      "673 247119\n",
      "674 240573\n",
      "675 996533\n",
      "676 234586\n",
      "677 703542\n",
      "678 820928\n",
      "679 324335\n",
      "680 876193\n",
      "681 646927\n",
      "682 914920\n",
      "683 360688\n",
      "684 672326\n",
      "685 133481\n",
      "686 209121\n",
      "687 223659\n",
      "688 49281\n",
      "689 874550\n",
      "690 294476\n",
      "691 744401\n",
      "692 978364\n",
      "693 57371\n",
      "694 348773\n",
      "695 490750\n",
      "696 914484\n",
      "697 563176\n",
      "698 422281\n",
      "699 92265\n",
      "700 914062\n",
      "701 558779\n",
      "702 88431\n",
      "703 363459\n",
      "704 668866\n",
      "705 8790\n",
      "706 346752\n",
      "707 57800\n",
      "708 235968\n",
      "709 413047\n",
      "710 677469\n",
      "711 56364\n",
      "712 834960\n",
      "713 695492\n",
      "714 380380\n",
      "715 239791\n",
      "716 479520\n",
      "717 868390\n",
      "718 272804\n",
      "719 485606\n",
      "720 934998\n",
      "721 164507\n",
      "722 269746\n",
      "723 223612\n",
      "724 239973\n",
      "725 91023\n",
      "726 155261\n",
      "727 952391\n",
      "728 37242\n",
      "729 895368\n",
      "730 121441\n",
      "731 57267\n",
      "732 964814\n",
      "733 918965\n",
      "734 536927\n",
      "735 349662\n",
      "736 536646\n",
      "737 162699\n",
      "738 821385\n",
      "739 390284\n",
      "740 431965\n",
      "741 549607\n",
      "742 97591\n",
      "743 249499\n",
      "744 331638\n",
      "745 824444\n",
      "746 33576\n",
      "747 686511\n",
      "748 526453\n",
      "749 564939\n",
      "750 73592\n",
      "751 630740\n",
      "752 691601\n",
      "753 512900\n",
      "754 675818\n",
      "755 676140\n",
      "756 250084\n",
      "757 559699\n",
      "758 49534\n",
      "759 864030\n",
      "760 49063\n",
      "761 679126\n",
      "762 585568\n",
      "763 645574\n",
      "764 952920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765 626998\n",
      "766 286990\n",
      "767 406153\n",
      "768 873450\n",
      "769 338094\n",
      "770 850300\n",
      "771 15926\n",
      "772 577218\n",
      "773 364685\n",
      "774 13901\n",
      "775 350272\n",
      "776 195831\n",
      "777 118452\n",
      "778 291543\n",
      "779 554970\n",
      "780 210716\n",
      "781 124102\n",
      "782 162055\n",
      "783 846066\n",
      "784 930962\n",
      "785 847884\n",
      "786 75426\n",
      "787 674141\n",
      "788 333432\n",
      "789 952892\n",
      "790 615013\n",
      "791 145455\n",
      "792 582759\n",
      "793 264499\n",
      "794 712932\n",
      "795 828717\n",
      "796 740540\n",
      "797 730877\n",
      "798 334333\n",
      "799 517289\n",
      "800 251837\n",
      "801 987659\n",
      "802 353968\n",
      "803 400426\n",
      "804 139296\n",
      "805 732260\n",
      "806 453757\n",
      "807 824656\n",
      "808 283818\n",
      "809 364339\n",
      "810 970904\n",
      "811 559136\n",
      "812 190387\n",
      "813 977042\n",
      "814 52001\n",
      "815 368665\n",
      "816 520919\n",
      "817 898180\n",
      "818 817187\n",
      "819 415637\n",
      "820 589020\n",
      "821 517952\n",
      "822 528733\n",
      "823 231175\n",
      "824 44088\n",
      "825 790393\n",
      "826 822125\n",
      "827 475844\n",
      "828 977449\n",
      "829 553157\n",
      "830 363525\n",
      "831 561089\n",
      "832 927232\n",
      "833 606764\n",
      "834 264686\n",
      "835 498594\n",
      "836 796565\n",
      "837 763906\n",
      "838 458628\n",
      "839 961898\n",
      "840 634868\n",
      "841 265298\n",
      "842 322403\n",
      "843 100085\n",
      "844 852061\n",
      "845 519099\n",
      "846 863005\n",
      "847 756718\n",
      "848 684671\n",
      "849 322370\n",
      "850 863065\n",
      "851 449281\n",
      "852 489485\n",
      "853 290520\n",
      "854 755704\n",
      "855 324754\n",
      "856 105429\n",
      "857 307888\n",
      "858 7245\n",
      "859 659041\n",
      "860 982492\n",
      "861 116232\n",
      "862 226940\n",
      "863 677699\n",
      "864 213587\n",
      "865 508565\n",
      "866 157682\n",
      "867 248660\n",
      "868 98453\n",
      "869 778342\n",
      "870 708844\n",
      "871 367510\n",
      "872 431490\n",
      "873 43858\n",
      "874 382661\n",
      "875 220613\n",
      "876 834058\n",
      "877 6284\n",
      "878 498227\n",
      "879 190056\n",
      "880 978178\n",
      "881 709922\n",
      "882 669046\n",
      "883 80615\n",
      "884 331127\n",
      "885 71538\n",
      "886 99830\n",
      "887 784237\n",
      "888 859677\n",
      "889 635551\n",
      "890 309674\n",
      "891 375228\n",
      "892 688826\n",
      "893 796213\n",
      "894 718021\n",
      "895 25383\n",
      "896 730454\n",
      "897 256990\n",
      "898 528079\n",
      "899 974271\n",
      "900 419293\n",
      "901 957048\n",
      "902 83329\n",
      "903 318554\n",
      "904 588605\n",
      "905 720010\n",
      "906 906208\n",
      "907 318119\n",
      "908 897511\n",
      "909 307249\n",
      "910 607149\n",
      "911 273000\n",
      "912 810559\n",
      "913 984957\n",
      "914 790169\n",
      "915 217606\n",
      "916 763439\n",
      "917 998993\n",
      "918 144853\n",
      "919 176317\n",
      "920 721260\n",
      "921 442855\n",
      "922 52935\n",
      "923 705999\n",
      "924 707690\n",
      "925 397337\n",
      "926 754258\n",
      "927 22424\n",
      "928 828652\n",
      "929 163760\n",
      "930 98464\n",
      "931 57059\n",
      "932 385617\n",
      "933 838930\n",
      "934 305018\n",
      "935 201591\n",
      "936 644706\n",
      "937 124744\n",
      "938 744539\n",
      "939 783854\n",
      "940 318878\n",
      "941 941802\n",
      "942 938563\n",
      "943 744875\n",
      "944 458192\n",
      "945 604912\n",
      "946 283191\n",
      "947 607375\n",
      "948 803830\n",
      "949 744181\n",
      "950 46548\n",
      "951 719667\n",
      "952 866178\n",
      "953 536005\n",
      "954 447204\n",
      "955 762614\n",
      "956 469460\n",
      "957 752654\n",
      "958 330165\n",
      "959 428008\n",
      "960 997816\n",
      "961 891854\n",
      "962 883756\n",
      "963 427743\n",
      "964 343354\n",
      "965 548561\n",
      "966 310451\n",
      "967 746794\n",
      "968 357567\n",
      "969 405769\n",
      "970 423384\n",
      "971 447774\n",
      "972 921551\n",
      "973 482371\n",
      "974 854133\n",
      "975 570855\n",
      "976 371938\n",
      "977 737877\n",
      "978 842184\n",
      "979 926336\n",
      "980 380434\n",
      "981 16129\n",
      "982 345453\n",
      "983 970730\n",
      "984 776461\n",
      "985 841843\n",
      "986 338504\n",
      "987 444140\n",
      "988 803299\n",
      "989 577327\n",
      "990 699701\n",
      "991 606414\n",
      "992 347805\n",
      "993 938758\n",
      "994 410594\n",
      "995 761482\n",
      "996 385382\n",
      "997 578453\n",
      "998 897902\n",
      "999 186943\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors = []\n",
    "\n",
    "N = 1000\n",
    "#### Wenqi: here had a bug: previously xb, now xq\n",
    "query_hidden_feature = model.encoder(torch.FloatTensor(xq)).detach().numpy()\n",
    "print(query_hidden_feature.shape)\n",
    "query_partition = kmeans.predict(query_hidden_feature)\n",
    "\n",
    "for i in range(N):\n",
    "    partition_id = int(query_partition[i])\n",
    "    nearest_neighbor_ID = scan_partition(xq[i], partition_id_vec_id_list_1M[partition_id], xb)\n",
    "    nearest_neighbors.append(nearest_neighbor_ID)\n",
    "    print(i, nearest_neighbor_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "798f9b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[504814]\n",
      " [588616]\n",
      " [552515]\n",
      " [335355]\n",
      " [482427]\n",
      " [508403]\n",
      " [167240]\n",
      " [327960]\n",
      " [834657]\n",
      " [592948]\n",
      " [878295]\n",
      " [771023]\n",
      " [215771]\n",
      " [717949]\n",
      " [368047]\n",
      " [776345]\n",
      " [373550]\n",
      " [862239]\n",
      " [602078]\n",
      " [ 84644]\n",
      " [ 68023]\n",
      " [671173]\n",
      " [ 47363]\n",
      " [258880]\n",
      " [698614]\n",
      " [838692]\n",
      " [922290]\n",
      " [221028]\n",
      " [962851]\n",
      " [785288]\n",
      " [425493]\n",
      " [407192]\n",
      " [229032]\n",
      " [909787]\n",
      " [303455]\n",
      " [825435]\n",
      " [602485]\n",
      " [812777]\n",
      " [341091]\n",
      " [856200]\n",
      " [982373]\n",
      " [499763]\n",
      " [915089]\n",
      " [ 43368]\n",
      " [640127]\n",
      " [858815]\n",
      " [436180]\n",
      " [283703]\n",
      " [ 28097]\n",
      " [426503]\n",
      " [849174]\n",
      " [467756]\n",
      " [803688]\n",
      " [ 74183]\n",
      " [819365]\n",
      " [882827]\n",
      " [535502]\n",
      " [779586]\n",
      " [702690]\n",
      " [ 12747]\n",
      " [534495]\n",
      " [895845]\n",
      " [795958]\n",
      " [ 45494]\n",
      " [943477]\n",
      " [809566]\n",
      " [  3367]\n",
      " [850184]\n",
      " [185761]\n",
      " [434096]\n",
      " [116383]\n",
      " [753756]\n",
      " [222336]\n",
      " [787309]\n",
      " [436415]\n",
      " [629700]\n",
      " [470987]\n",
      " [717995]\n",
      " [680004]\n",
      " [694111]\n",
      " [659382]\n",
      " [380662]\n",
      " [555466]\n",
      " [ 34810]\n",
      " [632431]\n",
      " [853398]\n",
      " [299980]\n",
      " [138747]\n",
      " [698591]\n",
      " [158669]\n",
      " [ 32380]\n",
      " [490742]\n",
      " [190435]\n",
      " [617987]\n",
      " [292659]\n",
      " [330662]\n",
      " [898436]\n",
      " [763985]\n",
      " [532460]\n",
      " [ 72670]]\n"
     ]
    }
   ],
   "source": [
    "print(gt[:100, :1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c4935",
   "metadata": {},
   "source": [
    "First 100 queries: recall@1 =  0.69\n",
    "First 1000 queries: recall@1 =  0.636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "dc94a598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622 recall@1 =  0.622\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "for i in range(N):\n",
    "    if nearest_neighbors[i] == gt[i][0]:\n",
    "        correct_count += 1\n",
    "        \n",
    "print(correct_count, 'recall@1 = ', correct_count / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d25be3",
   "metadata": {},
   "source": [
    "## Wenqi: naive auto-encoder is basically a dimensionality reduction technique\n",
    "\n",
    "It cannot improve the k-means quality, because the clustering with the full dimensions should perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ceccff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
