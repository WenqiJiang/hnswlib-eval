{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "290d2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np \n",
    "import struct\n",
    "import heapq\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6fa223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_eval(result_list, gt, k, query_num):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        result list: a 2-dim list\n",
    "            dim 1: query num\n",
    "            dim 2: topK\n",
    "        gt: a ground truth 2-d numpy array\n",
    "            dim 1: query num\n",
    "            dim 2: topK, 1000 for sift dataset\n",
    "        k: topK to be used for recall evaluation,\n",
    "            *** can be anything smaller than the dim2 of result_list ***)\n",
    "    Output:\n",
    "        recall\n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "    for i in range(query_num):\n",
    "        gt_set = set()\n",
    "        for j in range(k):\n",
    "            gt_set.add(gt[i][j])\n",
    "        for j in range(k):\n",
    "            vec_ID = result_list[i][j]\n",
    "            if vec_ID in gt_set:\n",
    "                count += 1\n",
    "    recall = count / (query_num * k)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba459dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, dirc, name):\n",
    "    # note use \"dir/\" in dirc\n",
    "    with open(os.path.join(dirc, name + '.pkl'), 'wb') as f:\n",
    "        pickle.dump(obj, f, protocol=4) # for py37,pickle.HIGHEST_PROTOCOL=4\n",
    "\n",
    "def load_obj(dirc, name):\n",
    "    with open(os.path.join(dirc, name + '.pkl'), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def convertBytes(bytestring, dtype='int'):\n",
    "    \"\"\"\n",
    "    convert bytes to a single element\n",
    "    dtype = {int, long, float, double}\n",
    "    struct: https://docs.python.org/3/library/struct.html\n",
    "    \"\"\" \n",
    "    # int from bytes is much faster than struct.unpack\n",
    "    if dtype =='int' or dtype == 'long': \n",
    "        return int.from_bytes(bytestring, byteorder='little', signed=False)\n",
    "    elif dtype == 'float': \n",
    "        return struct.unpack('f', bytestring)[0]\n",
    "    elif dtype == 'double': \n",
    "        return struct.unpack('d', bytestring)[0]\n",
    "    else:\n",
    "        raise ValueError \n",
    "\n",
    "# Wenqi: the fastest way to load a bytestring list is to use *** np.frombuffer ***\n",
    "def convertBytesList(bytestring, dtype='int'):\n",
    "    \"\"\"\n",
    "    Given a byte string, return the value list\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "    if dtype == 'int' or dtype == 'float':\n",
    "        dsize = 4\n",
    "    elif dtype == 'long' or dtype == 'double':\n",
    "        dsize = 8\n",
    "    else:\n",
    "        raise ValueError \n",
    "        \n",
    "    start_pointer = 0\n",
    "    for i in range(len(bytestring) // dsize):\n",
    "        result_list.append(convertBytes(\n",
    "            bytestring[start_pointer: start_pointer + dsize], dtype=dtype))\n",
    "        start_pointer += dsize\n",
    "    return result_list\n",
    "\n",
    "def calculateDist(query_data, db_vec):\n",
    "    \"\"\"\n",
    "    HNSWLib returns L2 square distance, so do we\n",
    "        both inputs are 1-d np array\n",
    "    \"\"\"\n",
    "    # return l2 distance between two points\n",
    "    return np.sum((query_data - db_vec) ** 2)\n",
    "\n",
    "\n",
    "def merge_two_distance_list(list_A, list_B, k):\n",
    "    \"\"\"\n",
    "    merge two lists by selecting the k pairs of the smallest distance\n",
    "    input:\n",
    "        both list has format [(dist, ID), (dist, ID), ...]\n",
    "    return:\n",
    "        a result list, with ascending distance (the first contains the largest distance)\n",
    "    \"\"\"\n",
    "    \n",
    "    results_heap = []\n",
    "    for i in range(len(list_A)):\n",
    "        dist, server_ID, vec_ID = list_A[i]\n",
    "        heapq.heappush(results_heap, (-dist, server_ID, vec_ID))\n",
    "    for i in range(len(list_B)):\n",
    "        dist, server_ID, vec_ID = list_B[i]\n",
    "        heapq.heappush(results_heap, (-dist, server_ID, vec_ID))\n",
    "\n",
    "    while len(results_heap) > k:\n",
    "        heapq.heappop(results_heap)\n",
    "\n",
    "    results = []\n",
    "    while len(results_heap) > 0:\n",
    "        dist, server_ID, vec_ID = results_heap[0]\n",
    "        results.append((-dist, server_ID, vec_ID))\n",
    "        heapq.heappop(results_heap)\n",
    "    results.reverse()\n",
    "            \n",
    "    return results\n",
    "\n",
    "        \n",
    "class HNSW_index():\n",
    "    \n",
    "    \"\"\"\n",
    "    Returned result list always in the format of (dist, server_ID, vec_ID),\n",
    "        in ascending distance order (the first result is the nearest neighbor)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, local_server_ID=0, dim=128):\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.local_server_ID = local_server_ID\n",
    "        \n",
    "        # Meta Info\n",
    "        self.offsetLevel0_ = None\n",
    "        self.max_elements_ = None\n",
    "        self.cur_element_count = None\n",
    "        self.size_data_per_element_ = None\n",
    "        self.label_offset_ = None\n",
    "        self.offsetData_ = None\n",
    "        self.maxlevel_ = None\n",
    "        self.enterpoint_node_ = None\n",
    "        self.maxM_ = None\n",
    "        self.maxM0_ = None\n",
    "        self.M_ = None\n",
    "        self.mult_ = None # the probability that a node is one a higher level\n",
    "        self.ef_construction_ = None\n",
    "        \n",
    "        # Graph partition info\n",
    "        self.centroid_vectors = None # centroids for all sub-graphs\n",
    "        \n",
    "        # ground layer, all with length of cur_element_count\n",
    "        self.links_count_l0 = None # a list of link_count\n",
    "        self.links_l0 = None # a list of links per vector\n",
    "        self.data_l0 = None # a list of vectors\n",
    "        self.label_l0 = None # a list of vector IDs\n",
    "        self.vec_ID_to_local_ID = dict() # mapping from vector ID -> local storage ID (on ground layer)\n",
    "        \n",
    "        # upper layers, all with length of cur_element_count\n",
    "        self.element_levels_ = None # the level per vector\n",
    "        self.links = None # the upper layer link info (link count + links)\n",
    "        \n",
    "        # remote nodes, order according to local ID (not label ID)\n",
    "        #  remote_links: an 2-D array (cur_element_count, k), \n",
    "        #    each element is a tuple: (server_ID, vector_ID)\n",
    "        self.remote_links_count = None\n",
    "        self.remote_links = None\n",
    "        \n",
    "    def load_meta_info(self, index_bin):\n",
    "        \"\"\"\n",
    "        index_bin = hnswlib index binary \n",
    "        \n",
    "        HNSW save index order:\n",
    "            https://github.com/WenqiJiang/hnswlib-eval/blob/master/hnswlib/hnswalg.h#L588-L616\n",
    "        \"\"\"\n",
    "        self.offsetLevel0_ = int.from_bytes(index_bin[0:8], byteorder='little', signed=False)\n",
    "        self.max_elements_ = int.from_bytes(index_bin[8:16], byteorder='little', signed=False)\n",
    "        self.cur_element_count = int.from_bytes(index_bin[16:24], byteorder='little', signed=False)\n",
    "        self.size_data_per_element_ = int.from_bytes(index_bin[24:32], byteorder='little', signed=False)\n",
    "        self.label_offset_ = int.from_bytes(index_bin[32:40], byteorder='little', signed=False)\n",
    "        self.offsetData_ = int.from_bytes(index_bin[40:48], byteorder='little', signed=False)\n",
    "        self.maxlevel_ = int.from_bytes(index_bin[48:52], byteorder='little', signed=False)\n",
    "        self.enterpoint_node_ = int.from_bytes(index_bin[52:56], byteorder='little', signed=False)\n",
    "        self.maxM_ = int.from_bytes(index_bin[56:64], byteorder='little', signed=False)\n",
    "        self.maxM0_ = int.from_bytes(index_bin[64:72], byteorder='little', signed=False)\n",
    "        self.M_ = int.from_bytes(index_bin[72:80], byteorder='little', signed=False)\n",
    "        self.mult_ = struct.unpack('d', index_bin[80:88])[0] # the probability that a node is one a higher level\n",
    "        self.ef_construction_ = int.from_bytes(index_bin[88:96], byteorder='little', signed=False)\n",
    "        \n",
    "\n",
    "        print(\"offsetLevel0_\", self.offsetLevel0_)\n",
    "        print(\"max_elements_\", self.max_elements_)\n",
    "        print(\"cur_element_count\", self.cur_element_count)\n",
    "        print(\"size_data_per_element_\", self.size_data_per_element_)\n",
    "        print(\"label_offset_\", self.label_offset_)\n",
    "        print(\"offsetData_\", self.offsetData_)\n",
    "        print(\"maxlevel_\", self.maxlevel_)\n",
    "        print(\"enterpoint_node_\", self.enterpoint_node_)\n",
    "        print(\"maxM_\", self.maxM_)\n",
    "        print(\"maxM0_\", self.maxM0_)\n",
    "        print(\"M_\", self.M_)\n",
    "        print(\"mult_\", self.mult_)\n",
    "        print(\"ef_construction_\", self.ef_construction_)\n",
    "        \n",
    "    def set_centroid_vectors(self, centroid_vectors):\n",
    "        \"\"\"\n",
    "        Given n subgraphs, there will be n centroid vectors,\n",
    "            the centroid vector of the current sub-graph is \n",
    "            self.centroid_vectors[self.local_server_ID]\n",
    "        Input:\n",
    "            cluster centroids: (n, dim)\n",
    "        \"\"\"\n",
    "        assert centroid_vectors.shape[1] == self.dim\n",
    "        self.centroid_vectors = centroid_vectors\n",
    "        \n",
    "    def load_ground_layer(self, index_bin):\n",
    "        \"\"\"\n",
    "        Get the ground layer vector ID, vectors, and links:\n",
    "            links_count_l0: vec_num\n",
    "            links_l0: maxM0_ * vec_num \n",
    "            data_l0: (dim, vec_num)\n",
    "            label_l0: vec_num\n",
    "        \"\"\"\n",
    "        \n",
    "        # Layer 0 data \n",
    "        start_byte_pointer = 96\n",
    "        delta = self.cur_element_count * self.size_data_per_element_\n",
    "        data_level0 = index_bin[start_byte_pointer: start_byte_pointer + delta]\n",
    "        \n",
    "        size = len(data_level0)\n",
    "        self.links_count_l0 = []\n",
    "        self.links_l0 = np.zeros((self.cur_element_count, self.maxM0_), dtype=int)\n",
    "        self.data_l0 = np.zeros((self.cur_element_count, self.dim))\n",
    "        self.label_l0 = []\n",
    "        self.vec_ID_to_local_ID = dict()\n",
    "\n",
    "        data_l0_list = []\n",
    "        \n",
    "        assert len(data_level0) == self.size_data_per_element_ * self.cur_element_count\n",
    "        \n",
    "        size_link_count = 4\n",
    "        size_links = self.maxM0_ * 4\n",
    "        size_vectors = self.dim * 4\n",
    "        size_label = 8\n",
    "        \n",
    "        assert self.size_data_per_element_ == \\\n",
    "            size_link_count + size_links + size_vectors + size_label\n",
    "            \n",
    "        for i in range(self.cur_element_count):\n",
    "            # per ground layer node: (link_count (int), links (int array of len=maxM0_), \n",
    "            #    vector (float array of len=dim, vector ID (long)))\n",
    "            \n",
    "            addr_link_count = i * self.size_data_per_element_ \n",
    "            addr_links = addr_link_count + size_link_count\n",
    "            addr_vectors = addr_links + size_links\n",
    "            addr_label = addr_vectors + size_vectors\n",
    "            \n",
    "            tmp_bytes = data_level0[addr_link_count: addr_link_count + size_link_count]\n",
    "            self.links_count_l0.append(convertBytes(tmp_bytes, dtype='int'))\n",
    "        \n",
    "            tmp_bytes = data_level0[addr_links: addr_links + size_links]\n",
    "            self.links_l0[i] = np.frombuffer(tmp_bytes, dtype=np.int32)\n",
    "            \n",
    "            tmp_bytes = data_level0[addr_vectors: addr_vectors + size_vectors]\n",
    "            self.data_l0[i] = np.frombuffer(tmp_bytes, dtype=np.float32)\n",
    "            \n",
    "            tmp_bytes = data_level0[addr_label: addr_label + size_label]\n",
    "            vec_ID = convertBytes(tmp_bytes, dtype='long')\n",
    "            self.label_l0.append(vec_ID)\n",
    "            self.vec_ID_to_local_ID[vec_ID] = i\n",
    "\n",
    "\n",
    "    def load_upper_layers(self, index_bin):\n",
    "        \"\"\"\n",
    "        Get the upper layer info:\n",
    "            element_levels_: the levels of each vector\n",
    "            links: list of upper links\n",
    "        \"\"\"\n",
    "        \n",
    "        # meta + ground data\n",
    "        start_byte_pointer = 96 + self.max_elements_ * self.size_data_per_element_\n",
    "        \n",
    "        # Upper layers\n",
    "        links_count = 0\n",
    "        size_links_per_element_ = self.maxM_ * 4 + 4\n",
    "        self.element_levels_ = []\n",
    "        self.links = []\n",
    "\n",
    "        for i in range(self.cur_element_count):\n",
    "            tmp_bytes = index_bin[start_byte_pointer:start_byte_pointer+4]\n",
    "            linkListSize = convertBytes(tmp_bytes, dtype='int')\n",
    "            start_byte_pointer += 4\n",
    "            \n",
    "            # if an element is only on ground layer, it has no links on upper layers at all\n",
    "            if linkListSize == 0:\n",
    "                self.element_levels_.append(0)\n",
    "                self.links.append([])\n",
    "            else:\n",
    "                level = int(linkListSize / size_links_per_element_)\n",
    "                self.element_levels_.append(level)\n",
    "                tmp_bytes = index_bin[start_byte_pointer:start_byte_pointer+linkListSize]\n",
    "                links_tmp = list(np.frombuffer(tmp_bytes, dtype=np.int32))\n",
    "                start_byte_pointer += linkListSize\n",
    "                links_count += linkListSize / 4;\n",
    "                self.links.append(links_tmp)\n",
    "\n",
    "        assert start_byte_pointer == len(index_bin) # 6606296\n",
    "\n",
    "    def insertRemote(self, remote_hnswlib_indexes, remote_server_IDs, ef=128):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "            remote_hnswlib_indexes: a list of remote_hnswlib_index\n",
    "                remote_hnswlib_index: index loaded by remote memory (hnswlib object)\n",
    "            remote_server_IDs: a list of remote index IDs respective to remote_hnswlib_indexes\n",
    "                e.g., this is server 1, and there are four servers in totol,\n",
    "                    then the remote index IDs should be [0, 2, 3]\n",
    "        \"\"\"\n",
    "        self.remote_links_count = []\n",
    "        self.remote_links = []\n",
    "        \n",
    "        remote_I_list = []\n",
    "        remote_D_list = []\n",
    "        remote_server_ID_list = []\n",
    "        \n",
    "        k = self.maxM0_\n",
    "        \n",
    "        assert len(remote_hnswlib_indexes) == len(remote_server_IDs)\n",
    "        \n",
    "        # query all servers\n",
    "        for i in range(len(remote_server_IDs)):\n",
    "            remote_hnswlib_index = remote_hnswlib_indexes[i]\n",
    "            remote_server_ID = remote_server_IDs[i]\n",
    "        \n",
    "            query = self.data_l0\n",
    "            remote_hnswlib_index.set_ef(ef)\n",
    "            I, D = remote_hnswlib_index.knn_query(query, k=k)\n",
    "            remote_I_list.append(I)\n",
    "            remote_D_list.append(D)\n",
    "            remote_server_ID_list.append(\n",
    "                np.ones((I.shape[0], I.shape[1]), dtype=np.int32) * int(remote_server_ID))\n",
    "    \n",
    "        # merge results per server\n",
    "        remote_I = np.concatenate(remote_I_list, axis=1)\n",
    "        remote_D = np.concatenate(remote_D_list, axis=1)\n",
    "        remote_server_ID = np.concatenate(remote_server_ID_list, axis=1)\n",
    "        \n",
    "        D_server_ID_I_list = [[] for i in range(remote_I.shape[0])]\n",
    "        server_ID_I_list= [[] for i in range(remote_I.shape[0])]\n",
    "        for i in range(remote_I.shape[0]):\n",
    "            for j in range(remote_I.shape[1]):\n",
    "                D_server_ID_I_list[i].append((remote_D[i][j], remote_server_ID[i][j], remote_I[i][j]))\n",
    "            D_server_ID_I_list[i].sort()\n",
    "            D_server_ID_I_list[i] = D_server_ID_I_list[i][:k]\n",
    "            server_ID_I_list[i] = [(s, i) for d, s, i in D_server_ID_I_list[i]]\n",
    "                \n",
    "        \n",
    "        self.remote_links_count = [k for i in range(self.cur_element_count)]\n",
    "        #  remote_links: an 2-D array (cur_element_count x k), \n",
    "        #    each element is a tuple: (server_ID, vector_ID)\n",
    "        self.remote_links = server_ID_I_list\n",
    "        \n",
    "    def searchKnnGroundLayer(self, q_data, k, ef, ep_local_id, existing_results=None):\n",
    "        \"\"\"\n",
    "        The ground layer searching process.\n",
    "        Input:\n",
    "            query vector\n",
    "            topk\n",
    "            ef\n",
    "            ep_local_id: entry point of the ground layer (local ID, not real vec ID)\n",
    "            existing_results: a list of search results on previous servers\n",
    "        Output:\n",
    "            topK results: list of (dist, serverID, vecID) in ascending distance order\n",
    "            search_path_local_ID_gnd: ground layer search path (local node ID) \n",
    "            search_path_vec_ID_gnd: ground layer search path (vector ID)\n",
    "        \"\"\"\n",
    "        \n",
    "        num_elements = self.cur_element_count\n",
    "        links_count_l0 = self.links_count_l0\n",
    "        links_l0 = self.links_l0\n",
    "        data_l0 = self.data_l0\n",
    "        label_l0 = self.label_l0\n",
    "        dim = self.dim\n",
    "        \n",
    "        search_path_local_ID_gnd = set()\n",
    "        search_path_vec_ID_gnd = set()\n",
    "        \n",
    "        # dynamic list (result candidates): (-dist, server_ID, vec_ID)\n",
    "        top_candidates = [] \n",
    "        # candidate list: (dist, local_ID)\n",
    "        candidate_set = []\n",
    "        # local visisted vectors\n",
    "        visited_array = set() \n",
    "        \n",
    "        \n",
    "        ep_dist = calculateDist(q_data, data_l0[ep_local_id])\n",
    "        distUpperBound = ep_dist \n",
    "        # By default heap queue is a min heap: https://docs.python.org/3/library/heapq.html\n",
    "        # candidate_set = candidate list, min heap\n",
    "        # top_candidates = dynamic list (potential results), max heap\n",
    "        # compare min(candidate_set) vs max(top_candidates)\n",
    "        if not existing_results:\n",
    "            vec_ID = label_l0[ep_local_id]\n",
    "            heapq.heappush(top_candidates, (-ep_dist, self.local_server_ID, vec_ID))\n",
    "            heapq.heappush(candidate_set, (ep_dist, ep_local_id))\n",
    "            visited_array.add(ep_local_id) \n",
    "        else:\n",
    "            for dist, server_ID, vec_ID in existing_results:\n",
    "                heapq.heappush(top_candidates, (-dist, server_ID, vec_ID))\n",
    "                if server_ID == self.local_server_ID: \n",
    "                    local_ID = self.vec_ID_to_local_ID[vec_ID]\n",
    "                    heapq.heappush(candidate_set, (dist, local_ID))\n",
    "                    visited_array.add(local_ID)\n",
    "                \n",
    "                \n",
    "        while len(candidate_set)!=0:\n",
    "            current_node_dist, current_node_id = candidate_set[0]\n",
    "            if ((current_node_dist > distUpperBound)):\n",
    "                break\n",
    "            heapq.heappop(candidate_set)\n",
    "            search_path_local_ID_gnd.add(current_node_id)\n",
    "            size = links_count_l0[current_node_id]\n",
    "            \n",
    "            for i in range(size):\n",
    "                candidate_id = links_l0[current_node_id][i]\n",
    "                if (candidate_id not in visited_array):\n",
    "                    visited_array.add(candidate_id)\n",
    "                    currVec = data_l0[candidate_id]\n",
    "                    dist = calculateDist(q_data, currVec)\n",
    "                    \n",
    "                    if (len(top_candidates) < ef or distUpperBound > dist):\n",
    "                        heapq.heappush(candidate_set, (dist, candidate_id))\n",
    "                        vec_ID = label_l0[candidate_id]\n",
    "                        heapq.heappush(top_candidates, (-dist, self.local_server_ID, vec_ID))\n",
    "                    if (len(top_candidates) > ef):\n",
    "                        heapq.heappop(top_candidates)\n",
    "                    if (len(top_candidates)!=0):\n",
    "                        distUpperBound = -top_candidates[0][0] \n",
    "                     \n",
    "\n",
    "        while len(top_candidates) > k:\n",
    "            heapq.heappop(top_candidates)\n",
    "\n",
    "        result = []\n",
    "        while len(top_candidates) > 0:\n",
    "            minus_dist, server_ID, vec_ID = top_candidates[0]\n",
    "            result.append([-minus_dist, server_ID, vec_ID])\n",
    "            heapq.heappop(top_candidates)\n",
    "        result.reverse()\n",
    "            \n",
    "        for local_ID in search_path_local_ID_gnd:\n",
    "            search_path_vec_ID_gnd.add(label_l0[local_ID])\n",
    "\n",
    "        return result, search_path_local_ID_gnd, search_path_vec_ID_gnd\n",
    "        \n",
    "    def searchKnn(self, q_data, k, ef):\n",
    "        \"\"\"\n",
    "        The HNSW way to search knn, from top layer all the way down to the bottom.\n",
    "        result a list of (distance, vec_ID) in ascending distance\n",
    "        \"\"\"\n",
    "        \n",
    "        ep_node = self.enterpoint_node_\n",
    "        max_level = self.maxlevel_\n",
    "        links = self.links\n",
    "        data_l0 = self.data_l0\n",
    "        label_l0 = self.label_l0\n",
    "        dim = self.dim\n",
    "        \n",
    "        currObj = ep_node\n",
    "        currVec = data_l0[currObj]\n",
    "        curdist = calculateDist(q_data, currVec)\n",
    "        \n",
    "        search_path_local_ID_upper = set()\n",
    "        search_path_vec_ID_upper = set()\n",
    "        \n",
    "        # search upper layers\n",
    "        for level in reversed(range(1, max_level+1)):\n",
    "            \n",
    "            changed = True\n",
    "            while changed:\n",
    "            \n",
    "                search_path_local_ID_upper.add(currObj)\n",
    "                changed = False\n",
    "                if (len(links[currObj])==0):\n",
    "                    break\n",
    "                else:\n",
    "                    start_index = (level-1) * 17\n",
    "                    size = links[currObj][start_index]\n",
    "                    neighbors = links[currObj][(start_index+1):(start_index+17)]\n",
    "                    \n",
    "                    for i in range(size):\n",
    "                        cand = neighbors[i]\n",
    "                        currVec = data_l0[cand]\n",
    "                        dist = calculateDist(q_data, currVec)\n",
    "                        if (dist < curdist):\n",
    "                            curdist = dist\n",
    "                            currObj = cand\n",
    "                            changed = True\n",
    "                            \n",
    "        for local_ID in search_path_local_ID_upper:\n",
    "            search_path_vec_ID_upper.add(label_l0[local_ID])\n",
    "            \n",
    "        # search in ground layer\n",
    "        result, search_path_local_ID_gnd, search_path_vec_ID_gnd = \\\n",
    "            self.searchKnnGroundLayer(q_data, k, ef, ep_local_id=currObj)\n",
    "        \n",
    "        return result, search_path_local_ID_upper, search_path_vec_ID_upper, search_path_local_ID_gnd, search_path_vec_ID_gnd\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98da7448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offsetLevel0_ 0\n",
      "max_elements_ 100000\n",
      "cur_element_count 100000\n",
      "size_data_per_element_ 652\n",
      "label_offset_ 644\n",
      "offsetData_ 132\n",
      "maxlevel_ 4\n",
      "enterpoint_node_ 81624\n",
      "maxM_ 16\n",
      "maxM0_ 32\n",
      "M_ 16\n",
      "mult_ 0.36067376022224085\n",
      "ef_construction_ 128\n",
      "load ground layer...\n",
      "time consumption: 0.53 sec\n",
      "load upper layer...\n",
      "time consumption: 0.12 sec\n"
     ]
    }
   ],
   "source": [
    "index_path = '../indexes/SIFT100K_index.bin'\n",
    "index = Path(index_path).read_bytes()\n",
    "dim = 128\n",
    "hnsw_index = HNSW_index(local_server_ID=0, dim=dim)\n",
    "hnsw_index.load_meta_info(index)\n",
    "\n",
    "print('load ground layer...')\n",
    "t0 = time.time()\n",
    "hnsw_index.load_ground_layer(index)\n",
    "t1 = time.time()\n",
    "print(\"time consumption: {:.2f} sec\".format(t1 - t0))\n",
    "\n",
    "print('load upper layer...')\n",
    "t0 = time.time()\n",
    "hnsw_index.load_upper_layers(index)\n",
    "t1 = time.time()\n",
    "print(\"time consumption: {:.2f} sec\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c0075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "ef = hnsw_index.ef_construction_\n",
    "print(ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd40f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmap_fvecs(fname):\n",
    "    x = np.memmap(fname, dtype='int32', mode='r')\n",
    "    d = x[0]\n",
    "    return x.view('float32').reshape(-1, d + 1)[:, 1:]\n",
    "\n",
    "def mmap_bvecs(fname):\n",
    "    x = np.memmap(fname, dtype='uint8', mode='r')\n",
    "    d = x[:4].view('int32')[0]\n",
    "    return x.reshape(-1, d + 4)[:, 4:]\n",
    "\n",
    "def ivecs_read(fname):\n",
    "    a = np.fromfile(fname, dtype='int32')\n",
    "    d = a[0]\n",
    "    # Wenqi: Format of ground truth (for 10000 query vectors):\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #        ...     ...\n",
    "    #   1000(topK), [1000 ids]\n",
    "    # 10000 rows in total, 10000 * 1001 elements, 10000 * 1001 * 4 bytes\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy()\n",
    "\n",
    "def fvecs_read(fname):\n",
    "    return ivecs_read(fname).view('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc6a4643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shapes:\n",
      "Base vector xb:  (1000000, 128)\n",
      "Query vector xq:  (10000, 128)\n",
      "Ground truth gt:  (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "dbname = 'SIFT1M'\n",
    "dim=128\n",
    "\n",
    "if dbname.startswith('SIFT'):\n",
    "    # SIFT1M to SIFT1000M\n",
    "    dbsize = int(dbname[4:-1])\n",
    "    xb = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_base.bvecs')\n",
    "    xq = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_query.bvecs')\n",
    "    gt = ivecs_read('/mnt/scratch/wenqi/Faiss_experiments/bigann/gnd/idx_%dM.ivecs' % dbsize)\n",
    "\n",
    "    N_VEC = int(dbsize * 1000 * 1000)\n",
    "\n",
    "    # trim xb to correct size\n",
    "    xb = xb[:dbsize * 1000 * 1000]\n",
    "    xb = xb.astype('float32').copy()\n",
    "\n",
    "    # Wenqi: load xq to main memory and reshape\n",
    "    xq = xq.astype('float32').copy()\n",
    "    xq = np.array(xq, dtype=np.float32)\n",
    "    gt = np.array(gt, dtype=np.int32)\n",
    "\n",
    "    print(\"Vector shapes:\")\n",
    "    print(\"Base vector xb: \", xb.shape)\n",
    "    print(\"Query vector xq: \", xq.shape)\n",
    "    print(\"Ground truth gt: \", gt.shape)\n",
    "else:\n",
    "    print('unknown dataset', dbname, file=sys.stderr)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5127e1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading index from ../indexes/SIFT100K_index.bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index_path='../indexes/{}_index.bin'.format('SIFT100K')\n",
    "p = hnswlib.Index(space='l2', dim=dim)  # the space can be changed - keeps the data, alters the distance function.\n",
    "print(\"\\nLoading index from {}\\n\".format(index_path))\n",
    "p.load_index(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df2699d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 128)\n"
     ]
    }
   ],
   "source": [
    "p.set_ef(128)\n",
    "all_queries = xb.astype('float32')\n",
    "print(all_queries.shape)\n",
    "I, D = p.knn_query(all_queries, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ee7f9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1894519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creat a mapping,\n",
    "    for each vector in the graph, attach a list of close vectors that will be stored on disk\n",
    "\"\"\"\n",
    "I_list = list(I.reshape(-1))\n",
    "\n",
    "node_to_vec_list = dict()\n",
    "for vec_ID in hnsw_index.vec_ID_to_local_ID: # enumerate all vec_ID in the graph\n",
    "    node_to_vec_list[vec_ID] = []\n",
    "    \n",
    "for i, vec_ID in enumerate(I_list):\n",
    "    node_to_vec_list[vec_ID].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45958ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 203483,\n",
       " 226433,\n",
       " 317122,\n",
       " 395686,\n",
       " 414817,\n",
       " 487099,\n",
       " 520895,\n",
       " 570523,\n",
       " 575383,\n",
       " 591816,\n",
       " 601326,\n",
       " 608767,\n",
       " 648378,\n",
       " 835152,\n",
       " 862636,\n",
       " 867816,\n",
       " 989477]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_to_vec_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1d98855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[54514.0, 0, 76346],\n",
       "  [55145.0, 0, 45476],\n",
       "  [56291.0, 0, 13166],\n",
       "  [56684.0, 0, 21706],\n",
       "  [60442.0, 0, 57428],\n",
       "  [61629.0, 0, 69782],\n",
       "  [62416.0, 0, 69873],\n",
       "  [63540.0, 0, 84944],\n",
       "  [63946.0, 0, 58420],\n",
       "  [65414.0, 0, 76705],\n",
       "  [65518.0, 0, 19596],\n",
       "  [66519.0, 0, 45763],\n",
       "  [66641.0, 0, 54589],\n",
       "  [68240.0, 0, 67006],\n",
       "  [69543.0, 0, 98550],\n",
       "  [70368.0, 0, 89653],\n",
       "  [71844.0, 0, 40734],\n",
       "  [73618.0, 0, 26815],\n",
       "  [73676.0, 0, 94977],\n",
       "  [73906.0, 0, 13916],\n",
       "  [74079.0, 0, 98690],\n",
       "  [74373.0, 0, 42889],\n",
       "  [75131.0, 0, 79036],\n",
       "  [75489.0, 0, 77265],\n",
       "  [76520.0, 0, 63517],\n",
       "  [76649.0, 0, 6501],\n",
       "  [76728.0, 0, 77279],\n",
       "  [76828.0, 0, 80666],\n",
       "  [77446.0, 0, 71399],\n",
       "  [77467.0, 0, 70184],\n",
       "  [78006.0, 0, 81792],\n",
       "  [78051.0, 0, 92766],\n",
       "  [78226.0, 0, 75905],\n",
       "  [78361.0, 0, 79491],\n",
       "  [79128.0, 0, 31430],\n",
       "  [79492.0, 0, 63283],\n",
       "  [79699.0, 0, 93064],\n",
       "  [80098.0, 0, 98558],\n",
       "  [80354.0, 0, 74725],\n",
       "  [80371.0, 0, 10516],\n",
       "  [80421.0, 0, 63300],\n",
       "  [80747.0, 0, 64336],\n",
       "  [80834.0, 0, 79612],\n",
       "  [80839.0, 0, 50142],\n",
       "  [80865.0, 0, 6393],\n",
       "  [80875.0, 0, 1733],\n",
       "  [80876.0, 0, 55135],\n",
       "  [80942.0, 0, 79709],\n",
       "  [80944.0, 0, 95084],\n",
       "  [81135.0, 0, 8299],\n",
       "  [81247.0, 0, 40710],\n",
       "  [81566.0, 0, 14789],\n",
       "  [81639.0, 0, 49190],\n",
       "  [81710.0, 0, 80728],\n",
       "  [82026.0, 0, 98565],\n",
       "  [82049.0, 0, 55197],\n",
       "  [82120.0, 0, 64345],\n",
       "  [82137.0, 0, 77285],\n",
       "  [82246.0, 0, 69610],\n",
       "  [82390.0, 0, 67041],\n",
       "  [83091.0, 0, 55951],\n",
       "  [83247.0, 0, 95433],\n",
       "  [83348.0, 0, 98111],\n",
       "  [83521.0, 0, 75989],\n",
       "  [83547.0, 0, 80713],\n",
       "  [83696.0, 0, 55014],\n",
       "  [83880.0, 0, 84377],\n",
       "  [83886.0, 0, 98577],\n",
       "  [84095.0, 0, 44488],\n",
       "  [84159.0, 0, 78126],\n",
       "  [84166.0, 0, 1911],\n",
       "  [84478.0, 0, 11774],\n",
       "  [84740.0, 0, 30967],\n",
       "  [84811.0, 0, 6912],\n",
       "  [84893.0, 0, 50052],\n",
       "  [84991.0, 0, 17717],\n",
       "  [85313.0, 0, 14513],\n",
       "  [85315.0, 0, 90191],\n",
       "  [85401.0, 0, 50334],\n",
       "  [85472.0, 0, 9454],\n",
       "  [85500.0, 0, 36897],\n",
       "  [85542.0, 0, 6318],\n",
       "  [85596.0, 0, 80576],\n",
       "  [85603.0, 0, 80991],\n",
       "  [85827.0, 0, 79386],\n",
       "  [86141.0, 0, 16528],\n",
       "  [86194.0, 0, 24955],\n",
       "  [86271.0, 0, 89867],\n",
       "  [86271.0, 0, 36608],\n",
       "  [86456.0, 0, 63159],\n",
       "  [86480.0, 0, 90230],\n",
       "  [86541.0, 0, 6575],\n",
       "  [86550.0, 0, 13302],\n",
       "  [86582.0, 0, 14762],\n",
       "  [86707.0, 0, 7116],\n",
       "  [86759.0, 0, 18381],\n",
       "  [86853.0, 0, 150],\n",
       "  [86924.0, 0, 74687],\n",
       "  [86978.0, 0, 82668],\n",
       "  [87112.0, 0, 83460]],\n",
       " {80741, 81624, 89703, 98286},\n",
       " {80741, 81624, 89703, 98286},\n",
       " {150,\n",
       "  1733,\n",
       "  1911,\n",
       "  1948,\n",
       "  2357,\n",
       "  2428,\n",
       "  6318,\n",
       "  6393,\n",
       "  6501,\n",
       "  6575,\n",
       "  6912,\n",
       "  7116,\n",
       "  7120,\n",
       "  7151,\n",
       "  8299,\n",
       "  9454,\n",
       "  9768,\n",
       "  10516,\n",
       "  11774,\n",
       "  12543,\n",
       "  13166,\n",
       "  13302,\n",
       "  13346,\n",
       "  13916,\n",
       "  14513,\n",
       "  14762,\n",
       "  14789,\n",
       "  16528,\n",
       "  17011,\n",
       "  17717,\n",
       "  18381,\n",
       "  18707,\n",
       "  19596,\n",
       "  21706,\n",
       "  24955,\n",
       "  26815,\n",
       "  27562,\n",
       "  28139,\n",
       "  30967,\n",
       "  31430,\n",
       "  32774,\n",
       "  33199,\n",
       "  33346,\n",
       "  36608,\n",
       "  36897,\n",
       "  40226,\n",
       "  40710,\n",
       "  40734,\n",
       "  42889,\n",
       "  44488,\n",
       "  45439,\n",
       "  45476,\n",
       "  45763,\n",
       "  49190,\n",
       "  50052,\n",
       "  50142,\n",
       "  50334,\n",
       "  50939,\n",
       "  54589,\n",
       "  55014,\n",
       "  55135,\n",
       "  55197,\n",
       "  55951,\n",
       "  56685,\n",
       "  57428,\n",
       "  58104,\n",
       "  58420,\n",
       "  63159,\n",
       "  63283,\n",
       "  63300,\n",
       "  63517,\n",
       "  64063,\n",
       "  64336,\n",
       "  64345,\n",
       "  66780,\n",
       "  67006,\n",
       "  67041,\n",
       "  69233,\n",
       "  69610,\n",
       "  69782,\n",
       "  69873,\n",
       "  70184,\n",
       "  71399,\n",
       "  73080,\n",
       "  74687,\n",
       "  74725,\n",
       "  75905,\n",
       "  75989,\n",
       "  76346,\n",
       "  76705,\n",
       "  77265,\n",
       "  77279,\n",
       "  77285,\n",
       "  78126,\n",
       "  79036,\n",
       "  79386,\n",
       "  79491,\n",
       "  79612,\n",
       "  79709,\n",
       "  80576,\n",
       "  80666,\n",
       "  80713,\n",
       "  80728,\n",
       "  80991,\n",
       "  81702,\n",
       "  81792,\n",
       "  82668,\n",
       "  82848,\n",
       "  83460,\n",
       "  84377,\n",
       "  84944,\n",
       "  85999,\n",
       "  89653,\n",
       "  89703,\n",
       "  89796,\n",
       "  89867,\n",
       "  90191,\n",
       "  90230,\n",
       "  92766,\n",
       "  93064,\n",
       "  94977,\n",
       "  95084,\n",
       "  95433,\n",
       "  98111,\n",
       "  98550,\n",
       "  98558,\n",
       "  98565,\n",
       "  98577,\n",
       "  98590,\n",
       "  98690},\n",
       " {150,\n",
       "  1733,\n",
       "  1911,\n",
       "  1948,\n",
       "  2357,\n",
       "  2428,\n",
       "  6318,\n",
       "  6393,\n",
       "  6501,\n",
       "  6575,\n",
       "  6912,\n",
       "  7116,\n",
       "  7120,\n",
       "  7151,\n",
       "  8299,\n",
       "  9454,\n",
       "  9768,\n",
       "  10516,\n",
       "  11774,\n",
       "  12543,\n",
       "  13166,\n",
       "  13302,\n",
       "  13346,\n",
       "  13916,\n",
       "  14513,\n",
       "  14762,\n",
       "  14789,\n",
       "  16528,\n",
       "  17011,\n",
       "  17717,\n",
       "  18381,\n",
       "  18707,\n",
       "  19596,\n",
       "  21706,\n",
       "  24955,\n",
       "  26815,\n",
       "  27562,\n",
       "  28139,\n",
       "  30967,\n",
       "  31430,\n",
       "  32774,\n",
       "  33199,\n",
       "  33346,\n",
       "  36608,\n",
       "  36897,\n",
       "  40226,\n",
       "  40710,\n",
       "  40734,\n",
       "  42889,\n",
       "  44488,\n",
       "  45439,\n",
       "  45476,\n",
       "  45763,\n",
       "  49190,\n",
       "  50052,\n",
       "  50142,\n",
       "  50334,\n",
       "  50939,\n",
       "  54589,\n",
       "  55014,\n",
       "  55135,\n",
       "  55197,\n",
       "  55951,\n",
       "  56685,\n",
       "  57428,\n",
       "  58104,\n",
       "  58420,\n",
       "  63159,\n",
       "  63283,\n",
       "  63300,\n",
       "  63517,\n",
       "  64063,\n",
       "  64336,\n",
       "  64345,\n",
       "  66780,\n",
       "  67006,\n",
       "  67041,\n",
       "  69233,\n",
       "  69610,\n",
       "  69782,\n",
       "  69873,\n",
       "  70184,\n",
       "  71399,\n",
       "  73080,\n",
       "  74687,\n",
       "  74725,\n",
       "  75905,\n",
       "  75989,\n",
       "  76346,\n",
       "  76705,\n",
       "  77265,\n",
       "  77279,\n",
       "  77285,\n",
       "  78126,\n",
       "  79036,\n",
       "  79386,\n",
       "  79491,\n",
       "  79612,\n",
       "  79709,\n",
       "  80576,\n",
       "  80666,\n",
       "  80713,\n",
       "  80728,\n",
       "  80991,\n",
       "  81702,\n",
       "  81792,\n",
       "  82668,\n",
       "  82848,\n",
       "  83460,\n",
       "  84377,\n",
       "  84944,\n",
       "  85999,\n",
       "  89653,\n",
       "  89703,\n",
       "  89796,\n",
       "  89867,\n",
       "  90191,\n",
       "  90230,\n",
       "  92766,\n",
       "  93064,\n",
       "  94977,\n",
       "  95084,\n",
       "  95433,\n",
       "  98111,\n",
       "  98550,\n",
       "  98558,\n",
       "  98565,\n",
       "  98577,\n",
       "  98590,\n",
       "  98690})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hnsw_index.searchKnn(xq[0], k=100, ef=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a020e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76575 36011 61812 24740 66855  7397 23998 99134  8143  7408 74033 66759\n",
      "  16367 37956 89589 92776 22199 75481 76562 68396 54622 89330 89590 13898\n",
      "  13576 83451 89339 50381 16404 60687 77702 86641 13590 45919 35317 22487\n",
      "  13549 19746 76583 60434 66925  8852 89375  7185 90275 16259 89072 77724\n",
      "  40908   726 17655  7409  3793 89329 90182 88964 88778  9437 41039 46234\n",
      "  35777 37902 89080 13625 85599 23047 28376 89132 34247 76568 63804  8261\n",
      "  22937 54658 99409 57750 81081 88746 80551 58037   331 81349 88774 29595\n",
      "  77830 65256 88427 47579 76579 22951 22205 47504 28284 71323 84039 35329\n",
      "  75525 75463  9207 76561]]\n"
     ]
    }
   ],
   "source": [
    "I, D = p.knn_query(xq[0], k=100)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50017b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[92329.0, 0, 76575]],\n",
       " {8857, 12541, 60587, 81624, 88525, 88904},\n",
       " {8857, 12541, 60587, 81624, 88525, 88904},\n",
       " {331,\n",
       "  726,\n",
       "  3793,\n",
       "  4497,\n",
       "  7185,\n",
       "  7397,\n",
       "  7408,\n",
       "  7409,\n",
       "  7738,\n",
       "  8143,\n",
       "  8169,\n",
       "  8214,\n",
       "  8233,\n",
       "  8261,\n",
       "  8852,\n",
       "  9207,\n",
       "  9437,\n",
       "  12545,\n",
       "  13549,\n",
       "  13576,\n",
       "  13590,\n",
       "  13625,\n",
       "  13898,\n",
       "  15769,\n",
       "  16259,\n",
       "  16367,\n",
       "  16404,\n",
       "  17655,\n",
       "  19344,\n",
       "  19746,\n",
       "  21659,\n",
       "  22199,\n",
       "  22205,\n",
       "  22487,\n",
       "  22766,\n",
       "  22937,\n",
       "  22951,\n",
       "  23047,\n",
       "  23998,\n",
       "  24740,\n",
       "  28284,\n",
       "  28376,\n",
       "  29595,\n",
       "  30992,\n",
       "  34247,\n",
       "  34414,\n",
       "  35317,\n",
       "  35329,\n",
       "  35777,\n",
       "  35902,\n",
       "  36011,\n",
       "  37902,\n",
       "  37956,\n",
       "  40683,\n",
       "  40684,\n",
       "  40685,\n",
       "  40908,\n",
       "  41039,\n",
       "  43180,\n",
       "  45887,\n",
       "  45919,\n",
       "  46234,\n",
       "  47504,\n",
       "  47579,\n",
       "  50381,\n",
       "  54622,\n",
       "  54658,\n",
       "  57537,\n",
       "  57750,\n",
       "  57951,\n",
       "  57957,\n",
       "  58037,\n",
       "  60434,\n",
       "  60687,\n",
       "  61812,\n",
       "  63804,\n",
       "  65256,\n",
       "  66759,\n",
       "  66855,\n",
       "  66925,\n",
       "  68396,\n",
       "  71323,\n",
       "  74033,\n",
       "  75463,\n",
       "  75481,\n",
       "  75525,\n",
       "  76561,\n",
       "  76562,\n",
       "  76568,\n",
       "  76575,\n",
       "  76579,\n",
       "  76583,\n",
       "  77702,\n",
       "  77724,\n",
       "  77830,\n",
       "  80551,\n",
       "  81081,\n",
       "  81349,\n",
       "  83241,\n",
       "  83451,\n",
       "  84039,\n",
       "  85044,\n",
       "  85599,\n",
       "  86641,\n",
       "  88337,\n",
       "  88427,\n",
       "  88505,\n",
       "  88536,\n",
       "  88746,\n",
       "  88756,\n",
       "  88763,\n",
       "  88766,\n",
       "  88774,\n",
       "  88778,\n",
       "  88801,\n",
       "  88885,\n",
       "  88891,\n",
       "  88900,\n",
       "  88904,\n",
       "  88910,\n",
       "  88938,\n",
       "  88946,\n",
       "  88956,\n",
       "  88964,\n",
       "  88966,\n",
       "  89012,\n",
       "  89024,\n",
       "  89064,\n",
       "  89072,\n",
       "  89073,\n",
       "  89078,\n",
       "  89080,\n",
       "  89132,\n",
       "  89146,\n",
       "  89148,\n",
       "  89152,\n",
       "  89158,\n",
       "  89329,\n",
       "  89330,\n",
       "  89339,\n",
       "  89375,\n",
       "  89443,\n",
       "  89589,\n",
       "  89590,\n",
       "  90182,\n",
       "  90275,\n",
       "  92776,\n",
       "  93896,\n",
       "  99134,\n",
       "  99409},\n",
       " {331,\n",
       "  726,\n",
       "  3793,\n",
       "  4497,\n",
       "  7185,\n",
       "  7397,\n",
       "  7408,\n",
       "  7409,\n",
       "  7738,\n",
       "  8143,\n",
       "  8169,\n",
       "  8214,\n",
       "  8233,\n",
       "  8261,\n",
       "  8852,\n",
       "  9207,\n",
       "  9437,\n",
       "  12545,\n",
       "  13549,\n",
       "  13576,\n",
       "  13590,\n",
       "  13625,\n",
       "  13898,\n",
       "  15769,\n",
       "  16259,\n",
       "  16367,\n",
       "  16404,\n",
       "  17655,\n",
       "  19344,\n",
       "  19746,\n",
       "  21659,\n",
       "  22199,\n",
       "  22205,\n",
       "  22487,\n",
       "  22766,\n",
       "  22937,\n",
       "  22951,\n",
       "  23047,\n",
       "  23998,\n",
       "  24740,\n",
       "  28284,\n",
       "  28376,\n",
       "  29595,\n",
       "  30992,\n",
       "  34247,\n",
       "  34414,\n",
       "  35317,\n",
       "  35329,\n",
       "  35777,\n",
       "  35902,\n",
       "  36011,\n",
       "  37902,\n",
       "  37956,\n",
       "  40683,\n",
       "  40684,\n",
       "  40685,\n",
       "  40908,\n",
       "  41039,\n",
       "  43180,\n",
       "  45887,\n",
       "  45919,\n",
       "  46234,\n",
       "  47504,\n",
       "  47579,\n",
       "  50381,\n",
       "  54622,\n",
       "  54658,\n",
       "  57537,\n",
       "  57750,\n",
       "  57951,\n",
       "  57957,\n",
       "  58037,\n",
       "  60434,\n",
       "  60687,\n",
       "  61812,\n",
       "  63804,\n",
       "  65256,\n",
       "  66759,\n",
       "  66855,\n",
       "  66925,\n",
       "  68396,\n",
       "  71323,\n",
       "  74033,\n",
       "  75463,\n",
       "  75481,\n",
       "  75525,\n",
       "  76561,\n",
       "  76562,\n",
       "  76568,\n",
       "  76575,\n",
       "  76579,\n",
       "  76583,\n",
       "  77702,\n",
       "  77724,\n",
       "  77830,\n",
       "  80551,\n",
       "  81081,\n",
       "  81349,\n",
       "  83241,\n",
       "  83451,\n",
       "  84039,\n",
       "  85044,\n",
       "  85599,\n",
       "  86641,\n",
       "  88337,\n",
       "  88427,\n",
       "  88505,\n",
       "  88536,\n",
       "  88746,\n",
       "  88756,\n",
       "  88763,\n",
       "  88766,\n",
       "  88774,\n",
       "  88778,\n",
       "  88801,\n",
       "  88885,\n",
       "  88891,\n",
       "  88900,\n",
       "  88904,\n",
       "  88910,\n",
       "  88938,\n",
       "  88946,\n",
       "  88956,\n",
       "  88964,\n",
       "  88966,\n",
       "  89012,\n",
       "  89024,\n",
       "  89064,\n",
       "  89072,\n",
       "  89073,\n",
       "  89078,\n",
       "  89080,\n",
       "  89132,\n",
       "  89146,\n",
       "  89148,\n",
       "  89152,\n",
       "  89158,\n",
       "  89329,\n",
       "  89330,\n",
       "  89339,\n",
       "  89375,\n",
       "  89443,\n",
       "  89589,\n",
       "  89590,\n",
       "  90182,\n",
       "  90275,\n",
       "  92776,\n",
       "  93896,\n",
       "  99134,\n",
       "  99409})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hnsw_index.searchKnn(xq[0], k=1, ef=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33c70100",
   "metadata": {},
   "outputs": [],
   "source": [
    "I, D = p.knn_query(xq, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76a958cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_eval(I, gt, k=1, query_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d9cc4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the search path on the ground layer, search all attached list\n",
    "\"\"\"\n",
    "ef = 128\n",
    "k = 100\n",
    "query_num = 1000\n",
    "\n",
    "result_list_graph_nodes = []\n",
    "search_path_list = []\n",
    "\n",
    "for i in range(query_num):\n",
    "    result, search_path_local_ID_upper, search_path_vec_ID_upper, search_path_local_ID_gnd, search_path_vec_ID_gnd = \\\n",
    "        hnsw_index.searchKnn(xq[i], k, ef)\n",
    "    result_list_graph_nodes.append(result)\n",
    "    search_path_list.append(search_path_vec_ID_gnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5c28563",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for ls in result_list_graph_nodes:\n",
    "    tmp_list = []\n",
    "    for dist, server_ID, vec_ID in ls:\n",
    "        tmp_list.append(vec_ID)\n",
    "    result_list.append(tmp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "305caed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Wenqi: basically, the result list is a subset of visited search path \n",
    "        This means without clustering, just using random DB vectors as centroid is not enough\n",
    "\"\"\"\n",
    "# c = 0\n",
    "# for i, vec_ID in enumerate(result_list[-1]):\n",
    "#     if vec_ID in search_path_vec_ID_gnd: c += 1\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c22307b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.117"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_eval(result_list, gt, k=1, query_num=query_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c25a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = []\n",
    "# for i in range(query_num):\n",
    "#     dist_vec_list = []\n",
    "#     print(\"search path\", search_path_list[i], len(search_path_list[i]))\n",
    "#     for node_vec_ID in search_path_list[i]:\n",
    "#         for disk_vec_ID in node_to_vec_list[node_vec_ID]:\n",
    "#             tmp.append(disk_vec_ID)\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b41935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = []\n",
    "\n",
    "for i in range(query_num):\n",
    "    dist_vec_list = []\n",
    "    for node_vec_ID in search_path_list[i]:\n",
    "        for disk_vec_ID in node_to_vec_list[node_vec_ID]:\n",
    "            \n",
    "            vec = xb[disk_vec_ID]\n",
    "            dist = np.sum((vec - xq[i]) ** 2)\n",
    "            dist_vec_list.append((dist, disk_vec_ID))\n",
    "#     print(dist_vec_list)\n",
    "    dist_vec_list.sort()\n",
    "#     dist_vec_list = dist_vec_list[:k]\n",
    "    final_result.append([vec_ID for dist, vec_ID in dist_vec_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b60b2df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.902"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_eval(final_result, gt, k=1, query_num=query_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c13e53",
   "metadata": {},
   "source": [
    "## Try clustering based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98dd34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c713f49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning k-means clusters...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 373. GiB for an array with shape (1000000, 100000) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mn_clusters, n_init\u001b[38;5;241m=\u001b[39mn_seeds) \n\u001b[1;32m      6\u001b[0m xt \u001b[38;5;241m=\u001b[39m xb[:train_size]\n\u001b[0;32m----> 7\u001b[0m \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinish learning k-means clusters...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/hnswlib/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1186\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;66;03m# run a k-means once\u001b[39;00m\n\u001b[0;32m-> 1186\u001b[0m labels, inertia, centers, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenters_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;66;03m# determine if these results are the best so far\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;66;03m# we chose a new run if it has a better inertia and the clustering is\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;66;03m# different from the best so far (it's possible that the inertia is\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;66;03m# slightly better even if the clustering is the same with potentially\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;66;03m# permuted labels, due to rounding errors)\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_inertia \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1203\u001b[0m     inertia \u001b[38;5;241m<\u001b[39m best_inertia\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_same_clustering(labels, best_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters)\n\u001b[1;32m   1205\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/hnswlib/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:457\u001b[0m, in \u001b[0;36m_kmeans_single_elkan\u001b[0;34m(X, sample_weight, centers_init, max_iter, verbose, x_squared_norms, tol, n_threads)\u001b[0m\n\u001b[1;32m    453\u001b[0m distance_next_center \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpartition(\n\u001b[1;32m    454\u001b[0m     np\u001b[38;5;241m.\u001b[39masarray(center_half_distances), kth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    455\u001b[0m )[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    456\u001b[0m upper_bounds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n_samples, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 457\u001b[0m lower_bounds \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m center_shift \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n_clusters, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 373. GiB for an array with shape (1000000, 100000) and data type float32"
     ]
    }
   ],
   "source": [
    "print('learning k-means clusters...')\n",
    "train_size = int(1e6)\n",
    "n_clusters = int(1e5) # 100 K centroids\n",
    "n_seeds = 1 # only run 1 seed\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=n_seeds) \n",
    "xt = xb[:train_size]\n",
    "kmeans.fit(xt)\n",
    "print('finish learning k-means clusters...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d295203",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_vectors = kmeans.cluster_centers_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
