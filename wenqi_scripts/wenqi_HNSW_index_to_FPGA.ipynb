{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np \n",
    "import struct\n",
    "import heapq\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmap_fvecs(fname):\n",
    "    x = np.memmap(fname, dtype='int32', mode='r')\n",
    "    d = x[0]\n",
    "    return x.view('float32').reshape(-1, d + 1)[:, 1:]\n",
    "\n",
    "def mmap_bvecs(fname):\n",
    "    x = np.memmap(fname, dtype='uint8', mode='r')\n",
    "    d = x[:4].view('int32')[0]\n",
    "    return x.reshape(-1, d + 4)[:, 4:]\n",
    "\n",
    "def ivecs_read(fname):\n",
    "    a = np.fromfile(fname, dtype='int32')\n",
    "    d = a[0]\n",
    "    # Wenqi: Format of ground truth (for 10000 query vectors):\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #        ...     ...\n",
    "    #   1000(topK), [1000 ids]\n",
    "    # 10000 rows in total, 10000 * 1001 elements, 10000 * 1001 * 4 bytes\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy()\n",
    "\n",
    "def fvecs_read(fname):\n",
    "    return ivecs_read(fname).view('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shapes:\n",
      "Base vector xb:  (1000000, 128)\n",
      "Query vector xq:  (10000, 128)\n",
      "Ground truth gt:  (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "dbname = 'SIFT1M'\n",
    "index_path='../indexes/{}_index.bin'.format(dbname)\n",
    "dim=128\n",
    "\n",
    "if dbname.startswith('SIFT'):\n",
    "    # SIFT1M to SIFT1000M\n",
    "    dbsize = int(dbname[4:-1])\n",
    "    xb = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_base.bvecs')\n",
    "    xq = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_query.bvecs')\n",
    "    gt = ivecs_read('/mnt/scratch/wenqi/Faiss_experiments/bigann/gnd/idx_%dM.ivecs' % dbsize)\n",
    "\n",
    "    N_VEC = int(dbsize * 1000 * 1000)\n",
    "\n",
    "    # trim xb to correct size\n",
    "    xb = xb[:dbsize * 1000 * 1000]\n",
    "\n",
    "    # Wenqi: load xq to main memory and reshape\n",
    "    xq = xq.astype('float32').copy()\n",
    "    xq = np.array(xq, dtype=np.float32)\n",
    "    gt = np.array(gt, dtype=np.int32)\n",
    "\n",
    "    print(\"Vector shapes:\")\n",
    "    print(\"Base vector xb: \", xb.shape)\n",
    "    print(\"Query vector xq: \", xq.shape)\n",
    "    print(\"Ground truth gt: \", gt.shape)\n",
    "else:\n",
    "    print('unknown dataset', dbname, file=sys.stderr)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core HNSW load and search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertBytes(bytestring, dtype='int'):\n",
    "    \"\"\"\n",
    "    convert bytes to a single element\n",
    "    dtype = {int, long, float, double}\n",
    "    struct: https://docs.python.org/3/library/struct.html\n",
    "    \"\"\" \n",
    "    # int from bytes is much faster than struct.unpack\n",
    "    if dtype =='int' or dtype == 'long': \n",
    "        return int.from_bytes(bytestring, byteorder='little', signed=False)\n",
    "    elif dtype == 'float': \n",
    "        return struct.unpack('f', bytestring)[0]\n",
    "    elif dtype == 'double': \n",
    "        return struct.unpack('d', bytestring)[0]\n",
    "    else:\n",
    "        raise ValueError \n",
    "\n",
    "# Wenqi: the fastest way to load a bytestring list is to use *** np.frombuffer ***\n",
    "def convertBytesList(bytestring, dtype='int'):\n",
    "    \"\"\"\n",
    "    Given a byte string, return the value list\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "    if dtype == 'int' or dtype == 'float':\n",
    "        dsize = 4\n",
    "    elif dtype == 'long' or dtype == 'double':\n",
    "        dsize = 8\n",
    "    else:\n",
    "        raise ValueError \n",
    "        \n",
    "    start_pointer = 0\n",
    "    for i in range(len(bytestring) // dsize):\n",
    "        result_list.append(convertBytes(\n",
    "            bytestring[start_pointer: start_pointer + dsize], dtype=dtype))\n",
    "        start_pointer += dsize\n",
    "    return result_list\n",
    "\n",
    "def calculateDist(query_data, db_vec):\n",
    "    \"\"\"\n",
    "    HNSWLib returns L2 square distance, so do we\n",
    "        both inputs are 1-d np array\n",
    "    \"\"\"\n",
    "    # return l2 distance between two points\n",
    "    return np.sum((query_data - db_vec) ** 2)\n",
    "\n",
    "\n",
    "def merge_two_distance_list(list_A, list_B, k):\n",
    "    \"\"\"\n",
    "    merge two lists by selecting the k pairs of the smallest distance\n",
    "    input:\n",
    "        both list has format [(dist, ID), (dist, ID), ...]\n",
    "    return:\n",
    "        a result list, with ascending distance (the first contains the largest distance)\n",
    "    \"\"\"\n",
    "    \n",
    "    results_heap = []\n",
    "    for i in range(len(list_A)):\n",
    "        dist, server_ID, vec_ID = list_A[i]\n",
    "        heapq.heappush(results_heap, (-dist, server_ID, vec_ID))\n",
    "    for i in range(len(list_B)):\n",
    "        dist, server_ID, vec_ID = list_B[i]\n",
    "        heapq.heappush(results_heap, (-dist, server_ID, vec_ID))\n",
    "\n",
    "    while len(results_heap) > k:\n",
    "        heapq.heappop(results_heap)\n",
    "\n",
    "    results = []\n",
    "    while len(results_heap) > 0:\n",
    "        dist, server_ID, vec_ID = results_heap[0]\n",
    "        results.append((-dist, server_ID, vec_ID))\n",
    "        heapq.heappop(results_heap)\n",
    "    results.reverse()\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct \n",
    "\n",
    "        \n",
    "class HNSW_index():\n",
    "    \n",
    "    \"\"\"\n",
    "    Returned result list always in the format of (dist, server_ID, vec_ID),\n",
    "        in ascending distance order (the first result is the nearest neighbor)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, local_server_ID=0, dim=128):\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.local_server_ID = local_server_ID\n",
    "        \n",
    "        # Meta Info\n",
    "        self.offsetLevel0_ = None\n",
    "        self.max_elements_ = None\n",
    "        self.cur_element_count = None\n",
    "        self.size_data_per_element_ = None\n",
    "        self.label_offset_ = None\n",
    "        self.offsetData_ = None\n",
    "        self.maxlevel_ = None\n",
    "        self.enterpoint_node_ = None\n",
    "        self.maxM_ = None\n",
    "        self.maxM0_ = None\n",
    "        self.M_ = None\n",
    "        self.mult_ = None # the probability that a node is one a higher level\n",
    "        self.ef_construction_ = None\n",
    "        \n",
    "        # ground layer, all with length of cur_element_count\n",
    "        self.links_count_l0 = None # a list of link_count\n",
    "        self.links_l0 = None # a list of links per vector\n",
    "        self.data_l0 = None # a list of vectors\n",
    "        label_l0 = None # a list of vector IDs\n",
    "        \n",
    "        # upper layers, all with length of cur_element_count\n",
    "        self.element_levels_ = None # the level per vector\n",
    "        self.links = None # the upper layer link info (link count + links)\n",
    "        \n",
    "        # remote nodes, order according to local ID (not label ID)\n",
    "        #  remote_links: an 2-D array (cur_element_count, k), \n",
    "        #    each element is a tuple: (server_ID, vector_ID)\n",
    "        self.remote_links_count = None\n",
    "        self.remote_links = None\n",
    "        \n",
    "    def load_meta_info(self, index_bin):\n",
    "        \"\"\"\n",
    "        index_bin = hnswlib index binary \n",
    "        \n",
    "        HNSW save index order:\n",
    "            https://github.com/WenqiJiang/hnswlib-eval/blob/master/hnswlib/hnswalg.h#L588-L616\n",
    "        \"\"\"\n",
    "        self.offsetLevel0_ = int.from_bytes(index_bin[0:8], byteorder='little', signed=False)\n",
    "        self.max_elements_ = int.from_bytes(index_bin[8:16], byteorder='little', signed=False)\n",
    "        self.cur_element_count = int.from_bytes(index_bin[16:24], byteorder='little', signed=False)\n",
    "        self.size_data_per_element_ = int.from_bytes(index_bin[24:32], byteorder='little', signed=False)\n",
    "        self.label_offset_ = int.from_bytes(index_bin[32:40], byteorder='little', signed=False)\n",
    "        self.offsetData_ = int.from_bytes(index_bin[40:48], byteorder='little', signed=False)\n",
    "        self.maxlevel_ = int.from_bytes(index_bin[48:52], byteorder='little', signed=False)\n",
    "        self.enterpoint_node_ = int.from_bytes(index_bin[52:56], byteorder='little', signed=False)\n",
    "        self.maxM_ = int.from_bytes(index_bin[56:64], byteorder='little', signed=False)\n",
    "        self.maxM0_ = int.from_bytes(index_bin[64:72], byteorder='little', signed=False)\n",
    "        self.M_ = int.from_bytes(index_bin[72:80], byteorder='little', signed=False)\n",
    "        self.mult_ = struct.unpack('d', index_bin[80:88])[0] # the probability that a node is one a higher level\n",
    "        self.ef_construction_ = int.from_bytes(index_bin[88:96], byteorder='little', signed=False)\n",
    "        \n",
    "\n",
    "        print(\"offsetLevel0_\", self.offsetLevel0_)\n",
    "        print(\"max_elements_\", self.max_elements_)\n",
    "        print(\"cur_element_count\", self.cur_element_count)\n",
    "        print(\"size_data_per_element_\", self.size_data_per_element_)\n",
    "        print(\"label_offset_\", self.label_offset_)\n",
    "        print(\"offsetData_\", self.offsetData_)\n",
    "        print(\"maxlevel_\", self.maxlevel_)\n",
    "        print(\"enterpoint_node_\", self.enterpoint_node_)\n",
    "        print(\"maxM_\", self.maxM_)\n",
    "        print(\"maxM0_\", self.maxM0_)\n",
    "        print(\"M_\", self.M_)\n",
    "        print(\"mult_\", self.mult_)\n",
    "        print(\"ef_construction_\", self.ef_construction_)\n",
    "        \n",
    "    \n",
    "    def load_ground_layer(self, index_bin):\n",
    "        \"\"\"\n",
    "        Get the ground layer vector ID, vectors, and links:\n",
    "            links_count_l0: vec_num\n",
    "            links_l0: maxM0_ * vec_num \n",
    "            data_l0: (dim, vec_num)\n",
    "            label_l0: vec_num\n",
    "        \"\"\"\n",
    "        \n",
    "        # Layer 0 data \n",
    "        start_byte_pointer = 96\n",
    "        delta = self.cur_element_count * self.size_data_per_element_\n",
    "        data_level0 = index_bin[start_byte_pointer: start_byte_pointer + delta]\n",
    "        \n",
    "        size = len(data_level0)\n",
    "        self.links_count_l0 = []\n",
    "        self.links_l0 = np.zeros((self.cur_element_count, self.maxM0_), dtype=int)\n",
    "        self.data_l0 = np.zeros((self.cur_element_count, self.dim))\n",
    "        self.label_l0 = []\n",
    "\n",
    "        data_l0_list = []\n",
    "        \n",
    "        assert len(data_level0) == self.size_data_per_element_ * self.cur_element_count\n",
    "        \n",
    "        size_link_count = 4\n",
    "        size_links = self.maxM0_ * 4\n",
    "        size_vectors = self.dim * 4\n",
    "        size_label = 8\n",
    "        \n",
    "        assert self.size_data_per_element_ == \\\n",
    "            size_link_count + size_links + size_vectors + size_label\n",
    "            \n",
    "        for i in range(self.cur_element_count):\n",
    "            # per ground layer node: (link_count (int), links (int array of len=maxM0_), \n",
    "            #    vector (float array of len=dim, vector ID (long)))\n",
    "            \n",
    "            addr_link_count = i * self.size_data_per_element_ \n",
    "            addr_links = addr_link_count + size_link_count\n",
    "            addr_vectors = addr_links + size_links\n",
    "            addr_label = addr_vectors + size_vectors\n",
    "            \n",
    "            tmp_bytes = data_level0[addr_link_count: addr_link_count + size_link_count]\n",
    "            self.links_count_l0.append(convertBytes(tmp_bytes, dtype='int'))\n",
    "        \n",
    "            tmp_bytes = data_level0[addr_links: addr_links + size_links]\n",
    "            self.links_l0[i] = np.frombuffer(tmp_bytes, dtype=np.int32)\n",
    "            \n",
    "            tmp_bytes = data_level0[addr_vectors: addr_vectors + size_vectors]\n",
    "            self.data_l0[i] = np.frombuffer(tmp_bytes, dtype=np.float32)\n",
    "            \n",
    "            tmp_bytes = data_level0[addr_label: addr_label + size_label]\n",
    "            self.label_l0.append(convertBytes(tmp_bytes, dtype='long'))\n",
    "\n",
    "\n",
    "    def load_upper_layers(self, index_bin):\n",
    "        \"\"\"\n",
    "        Get the upper layer info:\n",
    "            element_levels_: the levels of each vector\n",
    "            links: list of upper links\n",
    "        \"\"\"\n",
    "        \n",
    "        # meta + ground data\n",
    "        start_byte_pointer = 96 + self.max_elements_ * self.size_data_per_element_\n",
    "        \n",
    "        # Upper layers\n",
    "        links_count = 0\n",
    "        size_links_per_element_ = self.maxM_ * 4 + 4\n",
    "        self.element_levels_ = []\n",
    "        self.links = []\n",
    "\n",
    "        for i in range(self.cur_element_count):\n",
    "            tmp_bytes = index_bin[start_byte_pointer:start_byte_pointer+4]\n",
    "            linkListSize = convertBytes(tmp_bytes, dtype='int')\n",
    "            start_byte_pointer += 4\n",
    "            \n",
    "            # if an element is only on ground layer, it has no links on upper layers at all\n",
    "            if linkListSize == 0:\n",
    "                self.element_levels_.append(0)\n",
    "                self.links.append([])\n",
    "            else:\n",
    "                level = int(linkListSize / size_links_per_element_)\n",
    "                self.element_levels_.append(level)\n",
    "                tmp_bytes = index_bin[start_byte_pointer:start_byte_pointer+linkListSize]\n",
    "                links_tmp = list(np.frombuffer(tmp_bytes, dtype=np.int32))\n",
    "                start_byte_pointer += linkListSize\n",
    "                links_count += linkListSize / 4\n",
    "                self.links.append(links_tmp)\n",
    "\n",
    "        assert start_byte_pointer == len(index_bin) # 6606296\n",
    "\n",
    "    def save_as_FPGA_format(self, out_dir):\n",
    "        \"\"\"\n",
    "        Save the data as FPGA format, must make sure `load_ground_layer` and `load_upper_layers` are already invoked.\n",
    "        \"\"\"\n",
    "        # save meta data\n",
    "        byte_array_meta = bytearray() \n",
    "        byte_array_meta += self.cur_element_count.to_bytes(4, byteorder='little', signed=False)\n",
    "        byte_array_meta += self.maxlevel_.to_bytes(4, byteorder='little', signed=False)\n",
    "        byte_array_meta += self.enterpoint_node_.to_bytes(4, byteorder='little', signed=False)\n",
    "        byte_array_meta += self.maxM_.to_bytes(4, byteorder='little', signed=False)\n",
    "        byte_array_meta += self.maxM0_.to_bytes(4, byteorder='little', signed=False)\n",
    "\n",
    "        # save ground level links\n",
    "        # format of each node:\n",
    "        #   [64 B header = num_links (4B int) + 62 byte paddings] + N [64B actual links] + paddings (to 64 B)\n",
    "        byte_array_ground_links = bytearray()\n",
    "        for i in range(self.cur_element_count):\n",
    "            link_count = self.links_count_l0[i]\n",
    "            links = self.links_l0[i]\n",
    "            byte_array_ground_links += int(link_count).to_bytes(4, byteorder='little', signed=False)\n",
    "            byte_array_ground_links += b'\\x00' * 60\n",
    "            for j in range(self.maxM0_):\n",
    "                if j < link_count:\n",
    "                    byte_array_ground_links += int(links[j]).to_bytes(4, byteorder='little', signed=False)\n",
    "                else:\n",
    "                    byte_array_ground_links += b'\\x00' * 4\n",
    "            if len(byte_array_ground_links) % 64 != 0:\n",
    "                byte_array_ground_links += b'\\x00' * (64 - len(byte_array_ground_links) % 64)\n",
    "            \n",
    "        # save ground level vectors\n",
    "        # format of each node:\n",
    "        # [vector (4B float) + padding] + [visited (4B int, init as -1) + padding]\n",
    "        byte_array_ground_vectors = bytearray()\n",
    "        for i in range(self.cur_element_count):\n",
    "            vector = self.data_l0[i]\n",
    "            for j in range(self.dim):\n",
    "                byte_array_ground_vectors += struct.pack('f', vector[j])\n",
    "            if len(byte_array_ground_vectors) % 64 != 0:\n",
    "                byte_array_ground_vectors += b'\\x00' * (64 - len(byte_array_ground_vectors) % 64)\n",
    "            byte_array_ground_vectors += b'\\xff\\xff\\xff\\xff' # -1 in int32\n",
    "            byte_array_ground_vectors += b'\\x00' * 60\n",
    "\n",
    "        # save upper links, format:\n",
    "        #   [num_links (4B int) + padding] + N [64B actual links] + paddings (to 64 B)\n",
    "        byte_array_upper_links = bytearray()\n",
    "        # save pointers to addresses of upper links, each is 4B int as a pointer to a byte address\n",
    "        byte_array_upper_links_pointers = bytearray()\n",
    "        for i in range(self.cur_element_count):\n",
    "            levels = self.element_levels_[i] \n",
    "            pointer_addr = len(byte_array_upper_links) \n",
    "            byte_array_upper_links_pointers += pointer_addr.to_bytes(4, byteorder='little', signed=False)\n",
    "            if levels != 0:\n",
    "                links = self.links[i]\n",
    "                for j in range(levels):\n",
    "                    link_count = links[j * (1 + self.M_)]\n",
    "                    byte_array_upper_links += int(link_count).to_bytes(4, byteorder='little', signed=False)\n",
    "                    byte_array_upper_links += b'\\x00' * 60\n",
    "                    for k in range(self.M_):\n",
    "                        byte_array_upper_links += int(links[j * (1 + self.M_) + 1 + k]).to_bytes(4, byteorder='little', signed=False)\n",
    "                    if len(byte_array_upper_links) % 64 != 0:\n",
    "                        byte_array_upper_links += b'\\x00' * (64 - len(byte_array_upper_links) % 64)\n",
    "        \n",
    "        # save to files\n",
    "        with open(os.path.join(out_dir, 'meta.bin'), 'wb') as f:\n",
    "            f.write(byte_array_meta)\n",
    "        with open(os.path.join(out_dir, 'ground_links.bin'), 'wb') as f:\n",
    "            f.write(byte_array_ground_links)\n",
    "        with open(os.path.join(out_dir, 'ground_vectors.bin'), 'wb') as f:\n",
    "            f.write(byte_array_ground_vectors)\n",
    "        with open(os.path.join(out_dir, 'upper_links.bin'), 'wb') as f:\n",
    "            f.write(byte_array_upper_links)\n",
    "        with open(os.path.join(out_dir, 'upper_links_pointers.bin'), 'wb') as f:\n",
    "            f.write(byte_array_upper_links_pointers)\n",
    "\n",
    "    def searchKnn(self, q_data, k, ef, debug=False):\n",
    "        \"\"\"\n",
    "        result a list of (distance, vec_ID) in ascending distance\n",
    "        \"\"\"\n",
    "        \n",
    "        ep_node = self.enterpoint_node_\n",
    "        num_elements = self.cur_element_count\n",
    "        max_level = self.maxlevel_\n",
    "        links_count_l0 = self.links_count_l0\n",
    "        links_l0 = self.links_l0\n",
    "        data_l0 = self.data_l0\n",
    "        links = self.links\n",
    "        label_l0 = self.label_l0\n",
    "        dim = self.dim\n",
    "        \n",
    "        currObj = ep_node\n",
    "        currVec = data_l0[currObj]\n",
    "        curdist = calculateDist(q_data, currVec)\n",
    "        \n",
    "        search_path_local_ID = set()\n",
    "        search_path_vec_ID = set()\n",
    "        \n",
    "        # search upper layers\n",
    "        for level in reversed(range(1, max_level+1)):\n",
    "            if debug:\n",
    "                print(\"\")\n",
    "                print(\"level: \", level)\n",
    "            changed = True\n",
    "            while changed:\n",
    "                if debug:\n",
    "                    print(\"current object: \", currObj, \", current distance: \", curdist)\n",
    "                search_path_local_ID.add(currObj)\n",
    "                changed = False\n",
    "                ### Wenqi: here, assuming Node ID can be used to retrieve upper links (which is not true for indexes with ID starting from non-0)\n",
    "                if (len(links[currObj])==0):\n",
    "                    break\n",
    "                else:\n",
    "                    start_index = (level-1) * (1 + self.M_)\n",
    "                    size = links[currObj][start_index]\n",
    "                    if debug:\n",
    "                        print(\"size of neighbors: \", size) \n",
    "                    neighbors = links[currObj][(start_index+1):(start_index+(1 + self.M_))]\n",
    "                    for i in range(size):\n",
    "                        cand = neighbors[i]\n",
    "                        currVec = data_l0[cand]\n",
    "                        dist = calculateDist(q_data, currVec)\n",
    "                        if debug:\n",
    "                            print(\"cand: \", cand, \", dist: \", dist)\n",
    "                        if (dist < curdist):\n",
    "                            curdist = dist\n",
    "                            currObj = cand\n",
    "                            changed = True\n",
    "                            if debug:\n",
    "                                print(\"changed\")\n",
    "                    if debug:\n",
    "                        print(\"one node finish\")\n",
    "                        print(\"\")\n",
    "\n",
    "        # search in ground layer\n",
    "        if debug:\n",
    "            print(\"\")\n",
    "            print(\"level: 0\")\n",
    "        visited_array = set() # default 0\n",
    "        top_candidates = []\n",
    "        candidate_set = []\n",
    "        lowerBound = curdist \n",
    "        # By default heap queue is a min heap: https://docs.python.org/3/library/heapq.html\n",
    "        # candidate_set = candidate list, min heap\n",
    "        # top_candidates = dynamic list (potential results), max heap\n",
    "        # compare min(candidate_set) vs max(top_candidates)\n",
    "        heapq.heappush(top_candidates, (-curdist, currObj))\n",
    "        heapq.heappush(candidate_set,(curdist, currObj))\n",
    "        visited_array.add(currObj) \n",
    "\n",
    "        while len(candidate_set)!=0:\n",
    "            current_node_pair = candidate_set[0]\n",
    "            if ((current_node_pair[0] > lowerBound)):\n",
    "                break\n",
    "            heapq.heappop(candidate_set)\n",
    "            current_node_id = current_node_pair[1]\n",
    "            search_path_local_ID.add(current_node_id)\n",
    "            size = links_count_l0[current_node_id]\n",
    "            if debug:\n",
    "                print(\"current object: \", current_node_id)\n",
    "                print(\"size of neighbors: \", size)\n",
    "            for i in range(size):\n",
    "                candidate_id = links_l0[current_node_id][i]\n",
    "                if (candidate_id not in visited_array):\n",
    "                    visited_array.add(candidate_id)\n",
    "                    currVec = data_l0[candidate_id]\n",
    "                    dist = calculateDist(q_data, currVec)\n",
    "                    if debug:\n",
    "                        print(\"current object: \", candidate_id, \", current distance: \", dist, \", lowerBound: \", lowerBound)\n",
    "                    if (len(top_candidates) < ef or lowerBound > dist):\n",
    "                        if debug:\n",
    "                            print(\"added\")\n",
    "                        heapq.heappush(candidate_set, (dist, candidate_id))\n",
    "                        heapq.heappush(top_candidates, (-dist, candidate_id))\n",
    "                    if (len(top_candidates) > ef):\n",
    "                        heapq.heappop(top_candidates)\n",
    "                    if (len(top_candidates)!=0):\n",
    "                        lowerBound = -top_candidates[0][0]\n",
    "                else :\n",
    "                    if debug:\n",
    "                        print(\"current object: \", candidate_id, \", visited already\")\n",
    "            if debug:\n",
    "                print(\"one node finishes\")\n",
    "                print(\"\")\n",
    "\n",
    "        while len(top_candidates) > k:\n",
    "            heapq.heappop(top_candidates)\n",
    "\n",
    "        result = []\n",
    "        while len(top_candidates) > 0:\n",
    "            candidate_pair = top_candidates[0]\n",
    "            # Wenqi: here, replace the local candidate ID by real node ID, great!\n",
    "            result.append([-candidate_pair[0], self.local_server_ID, label_l0[candidate_pair[1]]])\n",
    "            heapq.heappop(top_candidates)\n",
    "        result.reverse()\n",
    "            \n",
    "        for local_ID in search_path_local_ID:\n",
    "            search_path_vec_ID.add(label_l0[local_ID])\n",
    "\n",
    "        return result, search_path_local_ID, search_path_vec_ID\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links format \n",
    "\n",
    "bytestrings: 0 0 0 ... \n",
    "\n",
    "0 -> no upper layer\n",
    "N -> N bytes for the following string, this is m x 4 x (1 + M), here M = 16 -> \n",
    "\n",
    "0 0 0 68 {string contents} 0 0 136 {string contents}\n",
    "\n",
    "string contents: first element: valid edge number; rest: vector IDs\n",
    "\n",
    "e.g.,\n",
    "[\n",
    "// first 1 + 16 elements\n",
    "  10,\n",
    "  \n",
    "  1561,\n",
    "  3999,\n",
    "  4373,\n",
    "  4213,\n",
    "  178,\n",
    "  6898,\n",
    "  7020,\n",
    "  7380,\n",
    "  8454,\n",
    "  8779,\n",
    "  3498,\n",
    "  3755,\n",
    "  3999,\n",
    "  4213,\n",
    "  4373,\n",
    "  4374,\n",
    "  \n",
    "// second 1 + 16 elements\n",
    "  15,\n",
    "  \n",
    "  1191,\n",
    "  1298,\n",
    "  1311,\n",
    "  1781,\n",
    "  1930,\n",
    "  2086,\n",
    "  2598,\n",
    "  2925,\n",
    "  2936,\n",
    "  3262,\n",
    "  4374,\n",
    "  4390,\n",
    "  5441,\n",
    "  5546,\n",
    "  5607,\n",
    "  0],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "788325340"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "index_path='../indexes/SIFT1M_index_M_32.bin'#.format(dbname)\n",
    "index = Path(index_path).read_bytes()\n",
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnsw_index = HNSW_index(local_server_ID=0, dim=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Output:\n",
    "\n",
    "\n",
    "The parameters in the index header, stored in small endian\n",
    "uint64_t offsetLevel0_; // 0:8\n",
    "uint64_t max_elements_; // 8:16\n",
    "uint64_t cur_element_count; // 16:24\n",
    "uint64_t size_data_per_element_; // 24:32\n",
    "uint64_t label_offset_; // 32:40\n",
    "uint64_t offsetData_; // 40:48\n",
    "uint32_t maxlevel_; // 48:52\n",
    "uint32_t enterpoint_node_; // 52:56\n",
    "uint64_t maxM_; // 56:64\n",
    "uint64_t maxM0_; // 64:72\n",
    "uint64_t M_; // 72:80\n",
    "double mult_; // 80:88\n",
    "uint64_t ef_construction_; // 88:96\n",
    "\n",
    "Results I got from C++ on SIFT1M:\n",
    "\n",
    "Index file size: 660564936\n",
    "offsetLevel0_: 0\n",
    "max_elements_: 1000000\n",
    "cur_element_count: 1000000\n",
    "size_data_per_element_: 652\n",
    "label_offset_: 644\n",
    "offsetData_: 132\n",
    "maxlevel_: 5\n",
    "enterpoint_node_: 572337\n",
    "maxM_: 16\n",
    "maxM0_: 32\n",
    "M_: 16\n",
    "mult_: 0.360674\n",
    "ef_construction_: 128\n",
    "\n",
    "size_data_per_element_ 652 = sizeVec (128 * 4) + maxM0_ * link_num (4) size_link_ID (32 * 4) + vec_ID (8) \n",
    "···"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offsetLevel0_ 0\n",
      "max_elements_ 1000000\n",
      "cur_element_count 1000000\n",
      "size_data_per_element_ 780\n",
      "label_offset_ 772\n",
      "offsetData_ 260\n",
      "maxlevel_ 4\n",
      "enterpoint_node_ 572468\n",
      "maxM_ 32\n",
      "maxM0_ 64\n",
      "M_ 32\n",
      "mult_ 0.28853900817779266\n",
      "ef_construction_ 128\n"
     ]
    }
   ],
   "source": [
    "hnsw_index.load_meta_info(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hnsw_index.maxM0_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time consumption: 3.21 sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "HNSW_index.load_ground_layer(hnsw_index, index)\n",
    "t1 = time.time()\n",
    "print(\"time consumption: {:.2f} sec\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time consumption: 1.23 sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "HNSW_index.load_upper_layers(hnsw_index, index)\n",
    "t1 = time.time()\n",
    "print(\"time consumption: {:.2f} sec\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '../FPGA_indexes/SIFT1M_index_M_32'\n",
    "if not os.path.exists(out_dir):\n",
    "\tos.makedirs(out_dir)\n",
    "hnsw_index.save_as_FPGA_format(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time consumption: 0.09 sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "result, search_path_local_ID, search_path_vec_ID = HNSW_index.searchKnn(hnsw_index, xq[0], k=1, ef=128) \n",
    "t1 = time.time()\n",
    "print(\"time consumption: {:.2f} sec\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[61125.0, 0, 504814]]\n",
      "504814\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "print(gt[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 {504835, 582150, 233481, 899594, 500240, 582161, 813078, 847900, 928798, 928802, 582191, 133171, 572468, 335953, 301139, 422483, 539223, 900184, 454231, 240216, 944727, 191064, 318561, 487010, 582765, 328816, 582781, 945798, 831624, 803468, 123023, 294036, 372375, 504471, 543901, 602781, 229024, 90275, 504484, 647847, 504487, 36011, 810157, 583350, 504502, 462014, 746178, 371400, 296136, 596184, 581849, 312027, 581853, 696545, 504547, 7397, 276200, 593131, 504558, 7408, 296178, 601330, 846581, 851705, 582401, 467203, 983300, 893189, 344333, 370459, 504605, 76575, 505120, 278818, 454435, 209187, 504613, 331045, 66855, 274729, 329513, 372014, 941872, 370481, 941875, 260404, 272693, 627002, 428858, 99134, 815428, 283974, 720199, 426311, 296269, 429392, 581976, 183643, 296284, 486749, 366950, 126826, 996719, 421746, 61812, 648063, 576902, 360844, 219536, 885651, 292756, 583065, 344473, 294315, 504753, 100277, 295863, 567738, 23998, 504772, 205768, 682443, 8142, 544207, 233941, 582620, 888802, 582631, 657385, 581100, 504814, 928756}\n"
     ]
    }
   ],
   "source": [
    "print(len(search_path_local_ID), search_path_local_ID) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 2\tsorted_search_path_local_ID = 8142\tsorted_search_path_vec_ID = 8143\n"
     ]
    }
   ],
   "source": [
    "sorted_search_path_vec_ID = sorted(list(search_path_vec_ID)) \n",
    "sorted_search_path_local_ID = sorted(list(search_path_local_ID))\n",
    "\n",
    "for i in range(len(sorted_search_path_local_ID)):\n",
    "    if sorted_search_path_local_ID[i] != sorted_search_path_vec_ID[i]:\n",
    "        print(\"i = {}\\tsorted_search_path_local_ID = {}\\tsorted_search_path_vec_ID = {}\".format(\n",
    "            i, sorted_search_path_local_ID[i], sorted_search_path_vec_ID[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
