{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920a66ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np \n",
    "import struct\n",
    "import heapq\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from distributed_graph_index_construction import HNSW_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2df3ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(dirc, name):\n",
    "    with open(os.path.join(dirc, name + '.pkl'), 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6930ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_eval(result_list, gt, k):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        result list: a 2-dim list\n",
    "            dim 1: query num\n",
    "            dim 2: topK\n",
    "        gt: a ground truth 2-d numpy array\n",
    "            dim 1: query num\n",
    "            dim 2: topK, 1000 for sift dataset\n",
    "        k: topK to be used for recall evaluation,\n",
    "            *** can be anything smaller than the dim2 of result_list ***)\n",
    "    Output:\n",
    "        recall\n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "    for i in range(query_num):\n",
    "        gt_set = set()\n",
    "        for j in range(k):\n",
    "            gt_set.add(gt[i][j])\n",
    "        for j in range(k):\n",
    "            vec_ID = result_list[i][j]\n",
    "            if vec_ID in gt_set:\n",
    "                count += 1\n",
    "    recall = count / (query_num * k)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d400eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmap_fvecs(fname):\n",
    "    x = np.memmap(fname, dtype='int32', mode='r')\n",
    "    d = x[0]\n",
    "    return x.view('float32').reshape(-1, d + 1)[:, 1:]\n",
    "\n",
    "def mmap_bvecs(fname):\n",
    "    x = np.memmap(fname, dtype='uint8', mode='r')\n",
    "    d = x[:4].view('int32')[0]\n",
    "    return x.reshape(-1, d + 4)[:, 4:]\n",
    "\n",
    "def ivecs_read(fname):\n",
    "    a = np.fromfile(fname, dtype='int32')\n",
    "    d = a[0]\n",
    "    # Wenqi: Format of ground truth (for 10000 query vectors):\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #        ...     ...\n",
    "    #   1000(topK), [1000 ids]\n",
    "    # 10000 rows in total, 10000 * 1001 elements, 10000 * 1001 * 4 bytes\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy()\n",
    "\n",
    "def fvecs_read(fname):\n",
    "    return ivecs_read(fname).view('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c57806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shapes:\n",
      "Base vector xb:  (1000000, 128)\n",
      "Query vector xq:  (10000, 128)\n",
      "Ground truth gt:  (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "dbname = 'SIFT1M'\n",
    "index_path='../indexes/{}_index.bin'.format(dbname)\n",
    "dim=128\n",
    "\n",
    "if dbname.startswith('SIFT'):\n",
    "    # SIFT1M to SIFT1000M\n",
    "    dbsize = int(dbname[4:-1])\n",
    "    xb = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_base.bvecs')\n",
    "    xq = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_query.bvecs')\n",
    "    gt = ivecs_read('/mnt/scratch/wenqi/Faiss_experiments/bigann/gnd/idx_%dM.ivecs' % dbsize)\n",
    "\n",
    "    N_VEC = int(dbsize * 1000 * 1000)\n",
    "\n",
    "    # trim xb to correct size\n",
    "    xb = xb[:dbsize * 1000 * 1000]\n",
    "\n",
    "    # Wenqi: load xq to main memory and reshape\n",
    "    xq = xq.astype('float32').copy()\n",
    "    xq = np.array(xq, dtype=np.float32)\n",
    "    gt = np.array(gt, dtype=np.int32)\n",
    "\n",
    "    print(\"Vector shapes:\")\n",
    "    print(\"Base vector xb: \", xb.shape)\n",
    "    print(\"Query vector xq: \", xq.shape)\n",
    "    print(\"Ground truth gt: \", gt.shape)\n",
    "else:\n",
    "    print('unknown dataset', dbname, file=sys.stderr)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28260c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_search(query_vec, kmeans, index_list, k, ef, all_vectors):\n",
    "    \"\"\"\n",
    "    query_vec: a numpy array of a single d-dimensional vector\n",
    "    kmeans: the kmeans object\n",
    "    index_list: a list of loaded python object of hnsw index\n",
    "    \"\"\"\n",
    "    assert ef >= k\n",
    "    \n",
    "    query_kmeans_format = query_vec.reshape(1,-1).astype(np.float64)\n",
    "    partition_id = kmeans.predict(query_kmeans_format)[0]\n",
    "    search_path = []\n",
    "    graph_path_len_list = [] # the graph search path length of each visited servers\n",
    "    entry_point_ID = None\n",
    "    \n",
    "    # While doing distributed search, set k as ef, such that the first server can \n",
    "    #   pass the entire result list to the second server.\n",
    "    # After searching all required servres, we prune the result\n",
    "    while True:\n",
    "        current_index = index_list[partition_id]\n",
    "        search_path.append(partition_id)\n",
    "        \n",
    "        if (len(search_path)) == 1: \n",
    "            # Search from the top layer of HNSW\n",
    "            results, local_results, remote_results, search_remote, remote_partition_id, remote_ep_vec_ID, graph_path_len = \\\n",
    "                current_index.searchKnnPlusRemoteCache(query_vec, k=ef, ef=ef, all_vectors=all_vectors)\n",
    "        else:\n",
    "            # Search from the ground layer given the info passed by the last server\n",
    "            results, local_results, remote_results, search_remote, remote_partition_id, remote_ep_vec_ID, graph_path_len = \\\n",
    "                current_index.searchKnnPlusRemoteCache(query_vec, k=ef, ef=ef, all_vectors=all_vectors, \n",
    "                                                       ep_vec_id=entry_point_ID, existing_results=results)\n",
    "        \n",
    "        entry_point_ID = remote_ep_vec_ID\n",
    "        graph_path_len_list.append(graph_path_len)\n",
    "        \n",
    "        if not search_remote:\n",
    "            break\n",
    "        else:\n",
    "            if remote_partition_id in search_path:\n",
    "                break\n",
    "            else:\n",
    "                partition_id = remote_partition_id\n",
    "    \n",
    "    # merge all results\n",
    "    results = results[:k]\n",
    "    \n",
    "    # this is in descending order for distance\n",
    "    return results, search_path, graph_path_len_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21216989",
   "metadata": {},
   "source": [
    "## Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab8ea146",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SUBGRAPH = 32\n",
    "parent_dir = '../indexes_subgraph_kmeans/SIFT1M_{}_subgraphs'.format(N_SUBGRAPH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828bf2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hnsw_indexes = [load_obj(parent_dir, 'subgraph_{}_with_remote_edges'.format(i)) for i in range(N_SUBGRAPH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b7618e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32085\n",
      "34445\n",
      "32463\n",
      "28116\n",
      "30155\n",
      "27535\n",
      "32671\n",
      "37909\n",
      "40761\n",
      "32794\n",
      "29259\n",
      "29783\n",
      "27649\n",
      "29474\n",
      "28696\n",
      "30636\n",
      "36666\n",
      "29190\n",
      "35601\n",
      "27197\n",
      "26314\n",
      "40255\n",
      "33383\n",
      "27533\n",
      "33262\n",
      "31533\n",
      "32801\n",
      "32683\n",
      "27502\n",
      "29037\n",
      "25241\n",
      "27371\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_SUBGRAPH): \n",
    "    print(len(all_hnsw_indexes[i].remote_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfab0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = load_obj(parent_dir, 'kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04fefe9",
   "metadata": {},
   "source": [
    "## Recall on distributed graph search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1be3dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "search_path_list = []\n",
    "all_graph_path_len_list = []\n",
    "query_num = 10000\n",
    "k = 100\n",
    "\n",
    "for i in range(query_num):\n",
    "    results, search_path, graph_path_len_list = distributed_search(\n",
    "        xq[i], kmeans, index_list=all_hnsw_indexes, k=k, ef=128, all_vectors=xb)\n",
    "    result_list.append(results)\n",
    "    search_path_list.append(search_path)\n",
    "    all_graph_path_len_list.append(graph_path_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "549fca5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@1 = 0.9912\n",
      "R@10 = 0.9861\n",
      "R@100 = 0.962541\n"
     ]
    }
   ],
   "source": [
    "## Get recall for consider up to 1 remote hop\n",
    "## Wenqi comment: for k-means-based method, the recall is really high\n",
    "# First 100 queries -> 1.0 recall\n",
    "# First 1000 queries -> 0.994 recall\n",
    "# First 10000 queries -> 0.9916 recall \n",
    "\n",
    "result_list_I = [[] for _ in range(len(result_list))]\n",
    "for i in range(len(result_list)): \n",
    "    for r in result_list[i]:\n",
    "        result_list_I[i].append(r[2])\n",
    "\n",
    "print(\"R@1 =\", recall_eval(result_list=result_list_I, gt=gt, k=1))\n",
    "print(\"R@10 =\", recall_eval(result_list=result_list_I, gt=gt, k=10))\n",
    "print(\"R@100 =\", recall_eval(result_list=result_list_I, gt=gt, k=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca1f52b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search remote rate: 0.2976 (2976 cases)\n",
      "average path length: 1.3024\n",
      "search path length distribution: {2: 2928, 1: 7024, 3: 48}\n",
      "average graph search path length on k-th searched sub-graph: {0: 133.1323, 1: 33.845766129032256, 2: 20.895833333333332}\n"
     ]
    }
   ],
   "source": [
    "# Count how many searches travel to remote node\n",
    "# First 100 queries -> 31% travels to remote node; average search path length = 1.31\n",
    "# First 100 queries -> 30% travels to remote node; average search path length = 1.304\n",
    "# First 100 queries -> 29.91% travels to remote node; average search path length = 1.3054 （1 case travel to a third server）\n",
    "\n",
    "search_remote_count = 0\n",
    "total_path_length = 0\n",
    "path_len = np.array([len(search_path_list[i]) for i in range(len(search_path_list))])\n",
    "len_count = dict() \n",
    "graph_path_len_server = dict() # key=server_path_len; value = (total_graph_path_len, case_count), e.g., \n",
    "# The first traversed server: total graph path len, number of cases that the 1st server is searched (all)\n",
    "# The second traversed server: total graph path len, number of cases that the 2nd server is searched (e.g., 30%), etc.\n",
    "ave_graph_path_len_server = dict() # average path length of the k-th server searched\n",
    "\n",
    "\n",
    "\n",
    "for i in range(query_num):\n",
    "    # Count server path length\n",
    "    total_path_length += path_len[i]\n",
    "    if path_len[i] in len_count:\n",
    "        len_count[path_len[i]] += 1\n",
    "    else:\n",
    "        len_count[path_len[i]] = 1\n",
    "    if len(search_path_list[i]) > 1: search_remote_count += 1\n",
    "        \n",
    "    # Count graph path length per server\n",
    "    for j, leng in enumerate(all_graph_path_len_list[i]):\n",
    "        # j th searched server's graph path length is leng\n",
    "        if j in graph_path_len_server:\n",
    "            total_graph_path_len, case_count = graph_path_len_server[j]\n",
    "        else:\n",
    "            total_graph_path_len = 0 \n",
    "            case_count = 0\n",
    "        total_graph_path_len += leng\n",
    "        case_count += 1\n",
    "        graph_path_len_server[j] = (total_graph_path_len, case_count)\n",
    "        \n",
    "for i in graph_path_len_server:\n",
    "    total_graph_path_len, case_count = graph_path_len_server[i]\n",
    "    ave_graph_path_len_server[i] = total_graph_path_len / case_count\n",
    "        \n",
    "average_path_length = total_path_length / query_num\n",
    "print(\"search remote rate: {} ({} cases)\".format(search_remote_count/query_num, search_remote_count))\n",
    "print(\"average path length: {}\".format(average_path_length))\n",
    "print(\"search path length distribution: {}\".format(len_count))\n",
    "print(\"average graph search path length on k-th searched sub-graph: {}\".format(ave_graph_path_len_server))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a308aa8d",
   "metadata": {},
   "source": [
    "## Partition-based search\n",
    "\n",
    "Without distributed search. Using K-means to decide m partitions to search. Explore the relationship between m and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load hnswlib index \"\"\"\n",
    "all_server_IDs = np.arange(N_SUBGRAPH)\n",
    "all_hnswlib_indexes = [hnswlib.Index(space='l2', dim=dim) for i in all_server_IDs]\n",
    "parent_dir = '../indexes_subgraph_kmeans/SIFT1M_32_subgraphs'\n",
    "all_index_paths=[os.path.join(parent_dir, 'subgraph_{}.bin'.format(i)) for i in all_server_IDs]\n",
    "for i in all_server_IDs:\n",
    "    print(\"\\nLoading hnswlib index from {}\\n\".format(all_index_paths[i]))\n",
    "    all_hnswlib_indexes[i].load_index(all_index_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15f35f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = kmeans.cluster_centers_\n",
    "print(cluster_centers.shape, cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b44a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroid_distances(cluster_centers, query_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        cluster_centers: 2-d array (num_clusters, dim)\n",
    "        query_vecs: 2-d array (num_queries, dim)\n",
    "    Output:\n",
    "        distance_mat (num_queries, num_clusters),\n",
    "            each element is a distance (L2 square)\n",
    "    \"\"\"\n",
    "    num_clusters, dim = cluster_centers.shape\n",
    "    nq = query_vecs.shape[0]\n",
    "    assert dim == query_vecs.shape[1]\n",
    "    \n",
    "    distance_mat = np.zeros((nq, num_clusters))\n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "        centroid_replications = np.tile(cluster_centers[i], (nq,1))\n",
    "        distance_mat[:, i] = np.sum((query_vecs - centroid_replications) ** 2, axis=1)\n",
    "    \n",
    "    return distance_mat\n",
    "\n",
    "def kmeans_predict_sorted(cluster_centers, query_vecs):\n",
    "    \"\"\"\n",
    "    Compute the cell centroid IDs for each query in a sorted manner \n",
    "        (increasing distance)\n",
    "    \n",
    "    Input:\n",
    "        cluster_centers: 2-d array (num_clusters, dim)\n",
    "        query_vecs: 2-d array (num_queries, dim)\n",
    "    Output:\n",
    "        ID_mat (num_queries, num_clusters),\n",
    "            each element is a centroid ID \n",
    "    \"\"\"\n",
    "    num_clusters, dim = cluster_centers.shape\n",
    "    nq = query_vecs.shape[0]\n",
    "    \n",
    "    distance_mat = compute_centroid_distances(cluster_centers, query_vecs)\n",
    "    ID_mat = np.argsort(distance_mat, axis=1)\n",
    "    \n",
    "    return ID_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate function correctness\n",
    "print(kmeans_predict_sorted(cluster_centers, xq[:10]))\n",
    "# query_kmeans_format = query_vec.reshape(1,-1).astype(np.float64)\n",
    "partition_id = kmeans.predict(xq[:10].astype(np.float64))\n",
    "print(partition_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_partition_IDs = kmeans_predict_sorted(cluster_centers, xq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be999e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_partition_IDs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7267c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Search several partitions per query vector \"\"\"\n",
    "MAX_VISITED_PARTITIONS = 8 # explore at most \n",
    "nq = xq.shape[0]\n",
    "dim = xq.shape[1]\n",
    "k = 100\n",
    "\n",
    "all_I = np.zeros((nq, k * MAX_VISITED_PARTITIONS), dtype=int)\n",
    "all_D = np.zeros((nq, k * MAX_VISITED_PARTITIONS))\n",
    "\n",
    "for index in all_hnswlib_indexes: \n",
    "    index.set_ef(128)\n",
    "\n",
    "for vec_id in range(nq):\n",
    "    if vec_id % 1000 == 0: print(\"query id: \", vec_id)\n",
    "    for j in range(MAX_VISITED_PARTITIONS):\n",
    "        index_id = sorted_partition_IDs[vec_id][j]\n",
    "        all_I[vec_id][j * k: (j + 1) * k], all_D[vec_id, j * k: (j + 1) * k] = \\\n",
    "            all_hnswlib_indexes[index_id].knn_query(xq[vec_id], k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da17b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_I.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf979ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the recall combining {k} x {num_partitions (1~MAX_VISITED_PARTITIONS)}\n",
    "for tmp_k in [1, 10, 100]:\n",
    "    \n",
    "    all_I_k_tmp = np.zeros((nq, tmp_k * MAX_VISITED_PARTITIONS), dtype=int)\n",
    "    all_D_k_tmp = np.zeros((nq, tmp_k * MAX_VISITED_PARTITIONS))\n",
    "\n",
    "    # copy tmp_k results from hnswlib\n",
    "    for vec_id in range(nq):\n",
    "        for j in range(MAX_VISITED_PARTITIONS):\n",
    "            all_I_k_tmp[vec_id][j * tmp_k: (j + 1) * tmp_k] = all_I[vec_id][j * k: j * k + tmp_k]\n",
    "            all_D_k_tmp[vec_id][j * tmp_k: (j + 1) * tmp_k] = all_D[vec_id][j * k: j * k + tmp_k]\n",
    "\n",
    "    # for upto MAX_VISITED_PARTITIONS partition, compute recall\n",
    "    for tmp_partition in range(1, 1 + MAX_VISITED_PARTITIONS):\n",
    "        \n",
    "        D_I_k_tmp = []\n",
    "        for vec_id in range(nq):\n",
    "            D_I_k_tmp.append([])\n",
    "            for j in range(tmp_partition):\n",
    "                for m in range(tmp_k):\n",
    "                    D_I_k_tmp[vec_id].append((all_D_k_tmp[vec_id][j * tmp_k + m], all_I_k_tmp[vec_id][j * tmp_k + m]))\n",
    "\n",
    "        D_k_tmp = []\n",
    "        I_k_tmp = []\n",
    "        for vec_id in range(nq):\n",
    "            D_I_tmp = sorted(D_I_k_tmp[vec_id])[:tmp_k]\n",
    "            D_k_tmp.append([D for D, I in D_I_tmp[:tmp_k]])\n",
    "            I_k_tmp.append([I for D, I in D_I_tmp[:tmp_k]])\n",
    "        \n",
    "        print(\"Num partition = {}\\tR@{} = {}\".format(\n",
    "            tmp_partition, tmp_k, recall_eval(result_list=I_k_tmp, gt=gt, k=tmp_k)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
