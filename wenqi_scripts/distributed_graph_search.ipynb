{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4b4d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np \n",
    "import struct\n",
    "import heapq\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from distributed_graph_index_construction import HNSW_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b317a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(dirc, name):\n",
    "    with open(os.path.join(dirc, name + '.pkl'), 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c429c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_eval(result_list, gt, k):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        result list: a 2-dim list\n",
    "            dim 1: query num\n",
    "            dim 2: topK\n",
    "        gt: a ground truth 2-d numpy array\n",
    "            dim 1: query num\n",
    "            dim 2: topK, 1000 for sift dataset\n",
    "        k: topK to be used for recall evaluation,\n",
    "            *** can be anything smaller than the dim2 of result_list ***)\n",
    "    Output:\n",
    "        recall\n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "    for i in range(query_num):\n",
    "        gt_set = set()\n",
    "        for j in range(k):\n",
    "            gt_set.add(gt[i][j])\n",
    "        for j in range(k):\n",
    "            vec_ID = result_list[i][j]\n",
    "            if vec_ID in gt_set:\n",
    "                count += 1\n",
    "    recall = count / (query_num * k)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcfc441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmap_fvecs(fname):\n",
    "    x = np.memmap(fname, dtype='int32', mode='r')\n",
    "    d = x[0]\n",
    "    return x.view('float32').reshape(-1, d + 1)[:, 1:]\n",
    "\n",
    "def mmap_bvecs(fname):\n",
    "    x = np.memmap(fname, dtype='uint8', mode='r')\n",
    "    d = x[:4].view('int32')[0]\n",
    "    return x.reshape(-1, d + 4)[:, 4:]\n",
    "\n",
    "def ivecs_read(fname):\n",
    "    a = np.fromfile(fname, dtype='int32')\n",
    "    d = a[0]\n",
    "    # Wenqi: Format of ground truth (for 10000 query vectors):\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #   1000(topK), [1000 ids]\n",
    "    #        ...     ...\n",
    "    #   1000(topK), [1000 ids]\n",
    "    # 10000 rows in total, 10000 * 1001 elements, 10000 * 1001 * 4 bytes\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy()\n",
    "\n",
    "def fvecs_read(fname):\n",
    "    return ivecs_read(fname).view('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5deba529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shapes:\n",
      "Base vector xb:  (1000000, 128)\n",
      "Query vector xq:  (10000, 128)\n",
      "Ground truth gt:  (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "dbname = 'SIFT1M'\n",
    "index_path='../indexes/{}_index.bin'.format(dbname)\n",
    "dim=128\n",
    "\n",
    "if dbname.startswith('SIFT'):\n",
    "    # SIFT1M to SIFT1000M\n",
    "    dbsize = int(dbname[4:-1])\n",
    "    xb = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_base.bvecs')\n",
    "    xq = mmap_bvecs('/mnt/scratch/wenqi/Faiss_experiments/bigann/bigann_query.bvecs')\n",
    "    gt = ivecs_read('/mnt/scratch/wenqi/Faiss_experiments/bigann/gnd/idx_%dM.ivecs' % dbsize)\n",
    "\n",
    "    N_VEC = int(dbsize * 1000 * 1000)\n",
    "\n",
    "    # trim xb to correct size\n",
    "    xb = xb[:dbsize * 1000 * 1000]\n",
    "\n",
    "    # Wenqi: load xq to main memory and reshape\n",
    "    xq = xq.astype('float32').copy()\n",
    "    xq = np.array(xq, dtype=np.float32)\n",
    "    gt = np.array(gt, dtype=np.int32)\n",
    "\n",
    "    print(\"Vector shapes:\")\n",
    "    print(\"Base vector xb: \", xb.shape)\n",
    "    print(\"Query vector xq: \", xq.shape)\n",
    "    print(\"Ground truth gt: \", gt.shape)\n",
    "else:\n",
    "    print('unknown dataset', dbname, file=sys.stderr)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4b1925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_search(query_vec, kmeans, index_list, k, ef, all_vectors):\n",
    "    \"\"\"\n",
    "    query_vec: a numpy array of a single d-dimensional vector\n",
    "    kmeans: the kmeans object\n",
    "    index_list: a list of loaded python object of hnsw index\n",
    "    \"\"\"\n",
    "    query_kmeans_format = query_vec.reshape(1,-1).astype(np.float64)\n",
    "    partition_id = kmeans.predict(query_kmeans_format)[0]\n",
    "    search_path = []\n",
    "    all_results = set() # deduplicate results\n",
    "    \n",
    "    while True:\n",
    "        current_index = index_list[partition_id]\n",
    "        search_path.append(partition_id)\n",
    "        \n",
    "        results, local_results, remote_results, search_remote, remote_partition_id = \\\n",
    "            current_index.searchKnnPlusRemoteCache(query_vec, k, ef, all_vectors, debug=False)\n",
    "        for r in results:\n",
    "            all_results.add(r)\n",
    "        if not search_remote:\n",
    "            break\n",
    "        else:\n",
    "            if remote_partition_id in search_path:\n",
    "                break\n",
    "            else:\n",
    "                partition_id = remote_partition_id\n",
    "    \n",
    "    # merge all results\n",
    "    results_heap = []\n",
    "    for dist, server_ID, vec_ID in all_results:\n",
    "        heapq.heappush(results_heap, (-dist, server_ID, vec_ID))\n",
    "    while len(results_heap) > k:\n",
    "        heapq.heappop(results_heap)\n",
    "\n",
    "    results = []\n",
    "    while len(results_heap) > 0:\n",
    "        dist, server_ID, vec_ID = results_heap[0]\n",
    "        results.append((-dist, server_ID, vec_ID))\n",
    "        heapq.heappop(results_heap)\n",
    "    results.reverse()\n",
    "    \n",
    "    # this is in descending order for distance\n",
    "    return results, search_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d0abd",
   "metadata": {},
   "source": [
    "## Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "257e8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SUBGRAPH = 32\n",
    "parent_dir = '../indexes_subgraph_kmeans/SIFT1M_{}_subgraphs'.format(N_SUBGRAPH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce8f87f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hnsw_indexes = [load_obj(parent_dir, 'subgraph_{}_with_remote_edges'.format(i)) for i in range(N_SUBGRAPH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d108435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27778\n",
      "29567\n",
      "33003\n",
      "29067\n",
      "27136\n",
      "32485\n",
      "30647\n",
      "29204\n",
      "28404\n",
      "29919\n",
      "32778\n",
      "32829\n",
      "32178\n",
      "29858\n",
      "34848\n",
      "40082\n",
      "30120\n",
      "26974\n",
      "31261\n",
      "27816\n",
      "31228\n",
      "27589\n",
      "28360\n",
      "26756\n",
      "43264\n",
      "34713\n",
      "33191\n",
      "34977\n",
      "27934\n",
      "31373\n",
      "32349\n",
      "32312\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_SUBGRAPH): \n",
    "    print(len(all_hnsw_indexes[i].remote_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aecfa493",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = load_obj(parent_dir, 'kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c975248",
   "metadata": {},
   "source": [
    "## Recall on distributed graph search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e372e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "search_path_list = []\n",
    "query_num = 10000\n",
    "k = 100\n",
    "\n",
    "for i in range(query_num):\n",
    "    results, search_path = distributed_search(\n",
    "        xq[i], kmeans, index_list=all_hnsw_indexes, k=k, ef=128, all_vectors=xb)\n",
    "    result_list.append(results)\n",
    "    search_path_list.append(search_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5492ca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@1 = 0.9916\n",
      "R@10 = 0.98342\n",
      "R@100 = 0.954067\n"
     ]
    }
   ],
   "source": [
    "## Get recall for consider up to 1 remote hop\n",
    "## Wenqi comment: for k-means-based method, the recall is really high\n",
    "# First 100 queries -> 1.0 recall\n",
    "# First 1000 queries -> 0.994 recall\n",
    "# First 10000 queries -> 0.9916 recall \n",
    "\n",
    "result_list_I = []\n",
    "for i in range(len(result_list[i])):\n",
    "\n",
    "print(\"R@1 =\", recall_eval(result_list=result_list, gt=gt, k=1))\n",
    "print(\"R@10 =\", recall_eval(result_list=result_list, gt=gt, k=10))\n",
    "print(\"R@100 =\", recall_eval(result_list=result_list, gt=gt, k=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e31f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many searches travel to remote node\n",
    "# First 100 queries -> 31% travels to remote node; average search path length = 1.31\n",
    "# First 100 queries -> 30% travels to remote node; average search path length = 1.304\n",
    "# First 100 queries -> 29.91% travels to remote node; average search path length = 1.3054 （1 case travel to a third server）\n",
    "\n",
    "search_remote_count = 0\n",
    "total_path_length = 0\n",
    "path_len = np.array([len(search_path_list[i]) for i in range(len(search_path_list))])\n",
    "len_count = dict() \n",
    "\n",
    "for i in range(query_num):\n",
    "    total_path_length += path_len[i]\n",
    "    if path_len[i] in len_count:\n",
    "        len_count[path_len[i]] += 1\n",
    "    else:\n",
    "        len_count[path_len[i]] = 1\n",
    "    if len(search_path_list[i]) > 1: search_remote_count += 1\n",
    "        \n",
    "average_path_length = total_path_length / query_num\n",
    "print(\"search remote rate: {} ({} cases)\".format(search_remote_count/query_num, search_remote_count))\n",
    "print(\"average path length: {}\".format(average_path_length))\n",
    "print(\"search path length distribution: {}\".format(len_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb072cc",
   "metadata": {},
   "source": [
    "## Partition-based search\n",
    "\n",
    "Without distributed search. Using K-means to decide m partitions to search. Explore the relationship between m and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc52f2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_0.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_1.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_2.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_3.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_4.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_5.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_6.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_7.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_8.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_9.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_10.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_11.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_12.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_13.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_14.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_15.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_16.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_17.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_18.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_19.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_20.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_21.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_22.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_23.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_24.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_25.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_26.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_27.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_28.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_29.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_30.bin\n",
      "\n",
      "\n",
      "Loading hnswlib index from ../indexes_subgraph_kmeans/SIFT1M_32_subgraphs/subgraph_31.bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load hnswlib index \"\"\"\n",
    "all_server_IDs = np.arange(N_SUBGRAPH)\n",
    "all_hnswlib_indexes = [hnswlib.Index(space='l2', dim=dim) for i in all_server_IDs]\n",
    "parent_dir = '../indexes_subgraph_kmeans/SIFT1M_32_subgraphs'\n",
    "all_index_paths=[os.path.join(parent_dir, 'subgraph_{}.bin'.format(i)) for i in all_server_IDs]\n",
    "for i in all_server_IDs:\n",
    "    print(\"\\nLoading hnswlib index from {}\\n\".format(all_index_paths[i]))\n",
    "    all_hnswlib_indexes[i].load_index(all_index_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfac5910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128) [[16.65008455 53.98379369 83.4343292  ... 16.89162909 18.9815389\n",
      "  19.68714769]\n",
      " [10.67950214 10.82730455 12.91391158 ...  5.00518605  5.97445871\n",
      "   9.6548684 ]\n",
      " [12.65382377  9.79548365 12.54801536 ... 12.06006286 13.3032243\n",
      "  15.60807822]\n",
      " ...\n",
      " [57.04531152 19.92309629  7.68596602 ...  6.68609188  6.42706104\n",
      "   8.05739459]\n",
      " [35.69270833 14.90234375  9.1859375  ... 22.42903646  6.80299479\n",
      "   8.53033854]\n",
      " [27.29260682 28.88550168 23.75180029 ...  7.3731397  12.95079213\n",
      "  14.23847816]]\n"
     ]
    }
   ],
   "source": [
    "cluster_centers = kmeans.cluster_centers_\n",
    "print(cluster_centers.shape, cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8659934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroid_distances(cluster_centers, query_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        cluster_centers: 2-d array (num_clusters, dim)\n",
    "        query_vecs: 2-d array (num_queries, dim)\n",
    "    Output:\n",
    "        distance_mat (num_queries, num_clusters),\n",
    "            each element is a distance (L2 square)\n",
    "    \"\"\"\n",
    "    num_clusters, dim = cluster_centers.shape\n",
    "    nq = query_vecs.shape[0]\n",
    "    assert dim == query_vecs.shape[1]\n",
    "    \n",
    "    distance_mat = np.zeros((nq, num_clusters))\n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "        centroid_replications = np.tile(cluster_centers[i], (nq,1))\n",
    "        distance_mat[:, i] = np.sum((query_vecs - centroid_replications) ** 2, axis=1)\n",
    "    \n",
    "    return distance_mat\n",
    "\n",
    "def kmeans_predict_sorted(cluster_centers, query_vecs):\n",
    "    \"\"\"\n",
    "    Compute the cell centroid IDs for each query in a sorted manner \n",
    "        (increasing distance)\n",
    "    \n",
    "    Input:\n",
    "        cluster_centers: 2-d array (num_clusters, dim)\n",
    "        query_vecs: 2-d array (num_queries, dim)\n",
    "    Output:\n",
    "        ID_mat (num_queries, num_clusters),\n",
    "            each element is a centroid ID \n",
    "    \"\"\"\n",
    "    num_clusters, dim = cluster_centers.shape\n",
    "    nq = query_vecs.shape[0]\n",
    "    \n",
    "    distance_mat = compute_centroid_distances(cluster_centers, query_vecs)\n",
    "    ID_mat = np.argsort(distance_mat, axis=1)\n",
    "    \n",
    "    return ID_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd46bfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14 20 24  9  7 12  0 31 25  5 27 23 19  8 15 13 18 21  2 26 11 30  6 17\n",
      "  28  4 29 22  1 16  3 10]\n",
      " [15 22 26  6 28 17 11 18 13  2 21  4  0 20 12 16 30  1 23  7 31  5  3 10\n",
      "   8 14 19 24 27  9 29 25]\n",
      " [29 21 19 25 23  7 27  5  8  9 24 14 31 20  4  0 12 13 10 30  3 18  2 22\n",
      "  28 17 26  1  6 11 15 16]\n",
      " [10 29  4 19  1  3 17  8  6 21 23  7  5 16 22 26 31 18 11 30 13 14 27 20\n",
      "  25  2  9 28 12 15  0 24]\n",
      " [15 13 17 22 26 28  2 11  6 18 21  4 30 16  1 20 12  5  0  7  3 23 27 10\n",
      "  19 31  8 24  9 14 29 25]\n",
      " [29 10  4 19 23  3  8 21 25  7  1 22  6  5 17 13 31 27 14 30  9 18 12 26\n",
      "  11 16  0 20 28 15  2 24]\n",
      " [29 19 10  5 23 27  4  8 25  7  3 21  1 31  9 14 22 13 12 20 24  0 17  6\n",
      "  26 18 30 11  2 16 28 15]\n",
      " [14 31 20  0 12 24 25 19 23  7 27  9 18  5 29  8  6 21 15 22  4  3 28 13\n",
      "  26  1  2 30 11 17 10 16]\n",
      " [26 16 17  8 15 30 20 21 11 28 22  4 10  9  2  7 18 13  1  3  6 19  5  0\n",
      "  29 31 12 23 27 14 24 25]\n",
      " [ 5  8 19 29 20  4  9 27  1 23 10  7 13  2 25 24 31 26 12  3 22 21 17 14\n",
      "   6 11 18  0 28 15 16 30]]\n",
      "[14 15 29 10 15 29 29 14 26  5]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate function correctness\n",
    "print(kmeans_predict_sorted(cluster_centers, xq[:10]))\n",
    "# query_kmeans_format = query_vec.reshape(1,-1).astype(np.float64)\n",
    "partition_id = kmeans.predict(xq[:10].astype(np.float64))\n",
    "print(partition_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff20d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_partition_IDs = kmeans_predict_sorted(cluster_centers, xq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33d63f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_partition_IDs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49f86f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query id:  0\n",
      "query id:  1000\n",
      "query id:  2000\n",
      "query id:  3000\n",
      "query id:  4000\n",
      "query id:  5000\n",
      "query id:  6000\n",
      "query id:  7000\n",
      "query id:  8000\n",
      "query id:  9000\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Search several partitions per query vector \"\"\"\n",
    "MAX_VISITED_PARTITIONS = 8 # explore at most \n",
    "nq = xq.shape[0]\n",
    "dim = xq.shape[1]\n",
    "k = 100\n",
    "\n",
    "all_I = np.zeros((nq, k * MAX_VISITED_PARTITIONS), dtype=int)\n",
    "all_D = np.zeros((nq, k * MAX_VISITED_PARTITIONS))\n",
    "\n",
    "for index in all_hnswlib_indexes: \n",
    "    index.set_ef(128)\n",
    "\n",
    "for vec_id in range(nq):\n",
    "    if vec_id % 1000 == 0: print(\"query id: \", vec_id)\n",
    "    for j in range(MAX_VISITED_PARTITIONS):\n",
    "        index_id = sorted_partition_IDs[vec_id][j]\n",
    "        all_I[vec_id][j * k: (j + 1) * k], all_D[vec_id, j * k: (j + 1) * k] = \\\n",
    "            all_hnswlib_indexes[index_id].knn_query(xq[vec_id], k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "48aecc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 800)\n"
     ]
    }
   ],
   "source": [
    "print(all_I.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a177c9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partition = 1\tR@1 = 0.6942\n",
      "Num partition = 2\tR@1 = 0.8716\n",
      "Num partition = 3\tR@1 = 0.9356\n",
      "Num partition = 4\tR@1 = 0.9659\n",
      "Num partition = 5\tR@1 = 0.9819\n",
      "Num partition = 6\tR@1 = 0.9891\n",
      "Num partition = 7\tR@1 = 0.9928\n",
      "Num partition = 8\tR@1 = 0.9959\n",
      "Num partition = 1\tR@10 = 0.66609\n",
      "Num partition = 2\tR@10 = 0.84712\n",
      "Num partition = 3\tR@10 = 0.91883\n",
      "Num partition = 4\tR@10 = 0.95313\n",
      "Num partition = 5\tR@10 = 0.97177\n",
      "Num partition = 6\tR@10 = 0.98224\n",
      "Num partition = 7\tR@10 = 0.98794\n",
      "Num partition = 8\tR@10 = 0.99173\n",
      "Num partition = 1\tR@100 = 0.612749\n",
      "Num partition = 2\tR@100 = 0.799239\n",
      "Num partition = 3\tR@100 = 0.880868\n",
      "Num partition = 4\tR@100 = 0.922392\n",
      "Num partition = 5\tR@100 = 0.946111\n",
      "Num partition = 6\tR@100 = 0.960622\n",
      "Num partition = 7\tR@100 = 0.969353\n",
      "Num partition = 8\tR@100 = 0.975071\n"
     ]
    }
   ],
   "source": [
    "# Compute the recall combining {k} x {num_partitions (1~MAX_VISITED_PARTITIONS)}\n",
    "for tmp_k in [1, 10, 100]:\n",
    "    \n",
    "    all_I_k_tmp = np.zeros((nq, tmp_k * MAX_VISITED_PARTITIONS), dtype=int)\n",
    "    all_D_k_tmp = np.zeros((nq, tmp_k * MAX_VISITED_PARTITIONS))\n",
    "\n",
    "    # copy tmp_k results from hnswlib\n",
    "    for vec_id in range(nq):\n",
    "        for j in range(MAX_VISITED_PARTITIONS):\n",
    "            all_I_k_tmp[vec_id][j * tmp_k: (j + 1) * tmp_k] = all_I[vec_id][j * k: j * k + tmp_k]\n",
    "            all_D_k_tmp[vec_id][j * tmp_k: (j + 1) * tmp_k] = all_D[vec_id][j * k: j * k + tmp_k]\n",
    "\n",
    "    # for upto MAX_VISITED_PARTITIONS partition, compute recall\n",
    "    for tmp_partition in range(1, 1 + MAX_VISITED_PARTITIONS):\n",
    "        \n",
    "        D_I_k_tmp = []\n",
    "        for vec_id in range(nq):\n",
    "            D_I_k_tmp.append([])\n",
    "            for j in range(tmp_partition):\n",
    "                for m in range(tmp_k):\n",
    "                    D_I_k_tmp[vec_id].append((all_D_k_tmp[vec_id][j * tmp_k + m], all_I_k_tmp[vec_id][j * tmp_k + m]))\n",
    "\n",
    "        D_k_tmp = []\n",
    "        I_k_tmp = []\n",
    "        for vec_id in range(nq):\n",
    "            D_I_tmp = sorted(D_I_k_tmp[vec_id])[:tmp_k]\n",
    "            D_k_tmp.append([D for D, I in D_I_tmp[:tmp_k]])\n",
    "            I_k_tmp.append([I for D, I in D_I_tmp[:tmp_k]])\n",
    "        \n",
    "        print(\"Num partition = {}\\tR@{} = {}\".format(\n",
    "            tmp_partition, tmp_k, recall_eval(result_list=I_k_tmp, gt=gt, k=tmp_k)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
